{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import json\n",
    "import sys\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "__author__= 'KD'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def json_writer(data, fname):\n",
    "    \"\"\"\n",
    "        Write multiple json files\n",
    "        Args:\n",
    "            data: list(dict): list of dictionaries to be written as json\n",
    "            fname: str: output file name\n",
    "    \"\"\"\n",
    "    with open(fname, mode=\"w\") as fp:\n",
    "        for line in data:\n",
    "            json.dump(line, fp)\n",
    "            fp.write(\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def json_reader(fname):\n",
    "    \"\"\"\n",
    "        Read multiple json files\n",
    "        Args:\n",
    "            fname: str: input file\n",
    "        Returns:\n",
    "            generator: iterator over documents \n",
    "    \"\"\"\n",
    "    for line in open(fname, mode=\"r\"):\n",
    "        yield json.loads(line)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def _stem(doc, p_stemmer, en_stop, return_tokens):\n",
    "    tokens = word_tokenize(doc.lower())\n",
    "    stopped_tokens = filter(lambda token: token not in en_stop, tokens)\n",
    "    stemmed_tokens = map(lambda token: p_stemmer.stem(token), stopped_tokens)\n",
    "    if not return_tokens:\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    return list(stemmed_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def getStemmedDocuments(docs, return_tokens=True):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            docs: str/list(str): document or list of documents that need to be processed\n",
    "            return_tokens: bool: return a re-joined string or tokens\n",
    "        Returns:\n",
    "            str/list(str): processed document or list of processed documents\n",
    "        Example: \n",
    "            new_text = \"It is important to by very pythonly while you are pythoning with python. \\\n",
    "                All pythoners have pythoned poorly at least once.\"\n",
    "            print(getStemmedDocuments(new_text))\n",
    "        Reference: https://pythonprogramming.net/stemming-nltk-tutorial/\n",
    "    \"\"\"\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    ps = PorterStemmer()\n",
    "    if isinstance(docs, list):\n",
    "        output_docs = []\n",
    "        for item in docs:\n",
    "            output_docs.append(_stem(item, ps, en_stop, return_tokens))\n",
    "        return output_docs\n",
    "    else:\n",
    "        return _stem(docs, ps, en_stop, return_tokens)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}