{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../'\n",
    "\n",
    "train_path = os.path.join(BASE_DIR, 'data', 'bank_dataset', 'bank_train.csv')\n",
    "test_path  = os.path.join(BASE_DIR, 'data', 'bank_dataset', 'bank_test.csv')\n",
    "val_path   = os.path.join(BASE_DIR, 'data', 'bank_dataset', 'bank_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(one_hot_encoding, numeric_cols, filename, values_dict = {}):\n",
    "    df = pd.read_csv(filename, delimiter = ';')\n",
    "    Y = df['y'].copy()\n",
    "    Y  = Y.to_numpy()\n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i] == 'yes':\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0 #Assigning 0 to nan values\n",
    "    \n",
    "    Y = Y.astype('int64')\n",
    "    df = df.drop(['y'],axis=1)\n",
    "    \n",
    "    if one_hot_encoding == False:\n",
    "        return df, Y\n",
    "    \n",
    "    if one_hot_encoding == True and values_dict == {}:\n",
    "        for col in df.columns:\n",
    "            if col not in numeric_cols:\n",
    "                values = list(set(list(df[col])))\n",
    "                values_dict[col] = values\n",
    "                \n",
    "                for i in range(df.shape[0]):\n",
    "                    df[col][i] = values_dict[col].index(df[col][i])\n",
    "        return df, Y, values_dict\n",
    "    \n",
    "    elif one_hot_encoding == True:\n",
    "        for col in df.columns:\n",
    "            if col not in numeric_cols:\n",
    "                for i in range(df.shape[0]):\n",
    "                    if df[col][i] in values_dict[col]:\n",
    "                        df[col][i] = values_dict[col].index(df[col][i])\n",
    "                    else:\n",
    "                        df[col][i] = -1\n",
    "        return df, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_num(X,Y):\n",
    "    median = np.median(X)\n",
    "    boolean_flag = X > median\n",
    "    # X_left  = X[boolean_flag == False]\n",
    "    # X_right = X[boolean_flag == True]\n",
    "    Y_left  = Y[boolean_flag == False]\n",
    "    Y_right = Y[boolean_flag == True]\n",
    "    \n",
    "    p1,p2 = 0,0\n",
    "    \n",
    "    if Y_left.shape[0] > 0:\n",
    "        s1 = np.sum(Y_left)\n",
    "        s1 = max(s1, Y_left.shape[0] - s1)\n",
    "        p1 = float(s1)/float(Y_left.shape[0])\n",
    "        p1 = - p1 * np.log(p1)\n",
    "        \n",
    "        if Y_left.shape[0] > s1:\n",
    "            p_t = float(Y_left.shape[0]-s1)/float(Y_left.shape[0])\n",
    "            p_t = -p_t * np.log(p_t)        \n",
    "            p1 += p_t\n",
    "        \n",
    "        p1 = p1*(float(Y_left.shape[0]))/float(Y.shape[0])\n",
    "    \n",
    "    if Y_right.shape[0] > 0:\n",
    "        s2 = np.sum(Y_right)\n",
    "        s2 = max(s2, Y_right.shape[0] - s2)\n",
    "        p2 = float(s2)/float(Y_right.shape[0])\n",
    "        p2 = - p2 * np.log(p2)\n",
    "        \n",
    "        if Y_right.shape[0] > s2:\n",
    "            p_t = float(Y_right.shape[0]-s2)/float(Y_right.shape[0])\n",
    "            p_t = -p_t * np.log(p_t)\n",
    "            p2 += p_t\n",
    "        \n",
    "        p2 = p2*(float(Y_right.shape[0]))/float(Y_right.shape[0])\n",
    "    \n",
    "    # print(p1,p2,X,Y)\n",
    "    \n",
    "    return p1+p2\n",
    "\n",
    "def entropy_categorical(X,Y):\n",
    "    val = list(set(list(X)))\n",
    "    val_count = dict.fromkeys(val,[0,0])\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        val_count[X[i]][1] += 1\n",
    "        if Y[i] == 1:\n",
    "            val_count[X[i]][0] += 1\n",
    "    entr = 0\n",
    "\n",
    "    for category,count in val_count.items():\n",
    "        p = 0\n",
    "        val_count[category][0] = max(count[0], count[1] - count[0])\n",
    "        if count[1] > 0:\n",
    "            p = float(val_count[category][0])/float(count[1])\n",
    "            p = -p * np.log(p)\n",
    "\n",
    "            p_t = float(count[1]-val_count[category][0])/float(count[1])\n",
    "            p_t = -p_t * np.log(p_t)\n",
    "\n",
    "            p += p_t\n",
    "            \n",
    "            p = p * (float(count[1]))/float(Y.shape[0])        \n",
    "        entr += p\n",
    "    \n",
    "    return entr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(attribute, one_hot_encoding, numeric_cols, parent_entr, indices, X, Y):\n",
    "    X_new = np.array((X.iloc[indices])[attribute])\n",
    "    Y_new = Y[indices]\n",
    "    entr  = 0\n",
    "    info_parent = parent_entr\n",
    "    \n",
    "    if attribute in numeric_cols:\n",
    "        entr = entropy_num(X_new, Y_new)\n",
    "        # print(\"Info Gain:\",attribute,entr)\n",
    "    else:\n",
    "        if one_hot_encoding == False: # Multi split\n",
    "            entr = entropy_categorical(X_new, Y_new)\n",
    "        else:\n",
    "            entr = entropy_num(X_new, Y_new)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return info_parent - entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_attribute(one_hot_encoding, rem_attr, numeric_cols, parent, indices, X, Y):\n",
    "    best_attr = ''\n",
    "    info_gain = -float('inf')\n",
    "    \n",
    "    parent_entr = 0\n",
    "    # if parent == None:\n",
    "    #     parent_entr = 0\n",
    "    # elif parent.attr in numeric_cols:\n",
    "    #     parent_entr = entropy_num\n",
    "    \n",
    "    for attr in X.columns:\n",
    "        if attr in numeric_cols or attr in rem_attr:            \n",
    "            temp = information_gain(attr, one_hot_encoding, numeric_cols, parent_entr, indices, X, Y)\n",
    "            # print(\"Best_Attr Selection:\",attr,temp)\n",
    "            if temp > info_gain:\n",
    "                info_gain = temp\n",
    "                best_attr = attr\n",
    "    \n",
    "    return best_attr, info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dc_node:\n",
    "\n",
    "    # indices coming at this node\n",
    "    def __init__(self,parent,indices,depth,decision,median=0,value=None,attribute=None):\n",
    "        self.parent = parent\n",
    "        self.indices = indices\n",
    "\n",
    "        self.child = []\n",
    "        self.depth = depth\n",
    "        self.attr  = attribute\n",
    "\n",
    "        self.decision = decision\n",
    "        self.median   = median\n",
    "        self.value    = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_decision_tree(one_hot_encoding, rem_attr, numeric_cols, parent, indices, X, Y, MAX_DEPTH):\n",
    "    Y_new = np.array(Y[indices])\n",
    "    if np.sum(Y_new) > (Y_new.shape[0] - np.sum(Y_new)):\n",
    "        decision = 1\n",
    "    else:\n",
    "        decision = 0\n",
    "    \n",
    "    if indices.shape[0] == 1 or (parent != None and parent.depth >= MAX_DEPTH):\n",
    "        if parent == None:\n",
    "            return dc_node(parent, indices, 1, decision)\n",
    "        else:\n",
    "            return dc_node(parent, indices, parent.depth+1, decision)\n",
    "    else:\n",
    "        attr,gain = best_attribute(one_hot_encoding, rem_attr, numeric_cols, parent, indices, X, Y)\n",
    "        \n",
    "        depth = 0\n",
    "        if parent == None:\n",
    "            depth = 0\n",
    "        else:\n",
    "            depth = parent.depth + 1\n",
    "        node = dc_node(parent,indices,depth,decision,attribute=attr)\n",
    "        \n",
    "        # print(\"Attr:\",attr,\",  Gain: \",gain,\",  Depth:\",depth)\n",
    "        # print(\"Rem:\",rem_attr)\n",
    "        # print(\"Numeric:\",numeric_cols)\n",
    "        # print(\"Indices:\",indices)\n",
    "        \n",
    "        # if gain > 0:\n",
    "        X_new = np.array((X.iloc[indices])[attr])\n",
    "        \n",
    "        if attr in numeric_cols or (one_hot_encoding == True):\n",
    "            median = np.median(X_new)\n",
    "            node.median = median\n",
    "            \n",
    "            boolean_flag = X_new > median\n",
    "            ind_left  = indices[boolean_flag == False]\n",
    "            ind_right = indices[boolean_flag == True]\n",
    "            \n",
    "            if one_hot_encoding == True and attr not in numeric_cols:\n",
    "                boolean_flag = (X_new % 2 == 0)\n",
    "                ind_left  = indices[boolean_flag == False]\n",
    "                ind_right = indices[boolean_flag == True]\n",
    "                rem_attr.remove(attr)\n",
    "                \n",
    "            \n",
    "            # print(\"Left: \",ind_left,ind_left.shape[0])\n",
    "            # print(\"Right: \",ind_right,ind_right.shape[0])\n",
    "            # print(\"Indices: \", indices, indices.shape[0])\n",
    "\n",
    "            if ind_left.shape[0] > 0:\n",
    "                left  = construct_decision_tree(one_hot_encoding, rem_attr.copy(), numeric_cols, node, ind_left, X, Y, MAX_DEPTH)\n",
    "                left.value = 'left'\n",
    "                node.child.append(left)\n",
    "            \n",
    "            if ind_right.shape[0] > 0:\n",
    "                right = construct_decision_tree(one_hot_encoding, rem_attr.copy(), numeric_cols, node, ind_right, X, Y, MAX_DEPTH)\n",
    "                right.value = 'right'\n",
    "                node.child.append(right)\n",
    "                                    \n",
    "        elif one_hot_encoding == False and attr in rem_attr:\n",
    "            rem_attr.remove(attr)\n",
    "            val = list(set(list(X_new)))\n",
    "            for i in val:\n",
    "                ind = indices[X_new == i]\n",
    "                if ind.shape[0] > 0:\n",
    "                    child = construct_decision_tree(one_hot_encoding, rem_attr.copy(), numeric_cols, node, ind, X, Y, MAX_DEPTH)\n",
    "                    child.value = i\n",
    "                    node.child.append(child)\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree(one_hot_encoding, categorical_cols, numeric_cols, X, Y, MAX_DEPTH = 20):\n",
    "    indices = np.arange(0,X.shape[0])\n",
    "    dc_tree = construct_decision_tree(one_hot_encoding, categorical_cols.copy(), numeric_cols, None, indices, X, Y, MAX_DEPTH)\n",
    "    return dc_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_recursive(one_hot_encoding, x, root, numeric_cols):\n",
    "    if len(root.child) == 0:\n",
    "        return root.decision\n",
    "    else:\n",
    "        val = ''\n",
    "        if root.attr in numeric_cols:\n",
    "            if x[root.attr] > root.median:\n",
    "                val = 'right'\n",
    "            else:\n",
    "                val = 'left'\n",
    "        elif one_hot_encoding == True:\n",
    "            index = x[root.attr]\n",
    "            if index % 2 == 0:\n",
    "                val = 'right'\n",
    "            else:\n",
    "                val = 'left'\n",
    "        else:            \n",
    "            val = x[root.attr]\n",
    "        \n",
    "        for i in range(len(root.child)):\n",
    "            if root.child[i].value == val:\n",
    "                return predict_recursive(one_hot_encoding, x, root.child[i], numeric_cols)\n",
    "        return root.decision\n",
    "    \n",
    "def predict(one_hot_encoding, X, root, numeric_cols):\n",
    "    Y_pred = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "    # for i in range(1):\n",
    "        z = predict_recursive(one_hot_encoding, X.iloc[i], root, numeric_cols)\n",
    "        Y_pred[i] = int(z)\n",
    "    Y_pred = Y_pred.astype('int64')\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(root):\n",
    "    if root.parent==None:\n",
    "        print(\"Root Node. Feature Used to Split-> \" + str(root.attr))\n",
    "        print(\"Decision: \" + str(root.decision))\n",
    "        child_list = []\n",
    "        for i in range(len(root.child)):\n",
    "            child_list.append(root.child[i].value)\n",
    "            \n",
    "        print(\"Child Values -> \" + str(child_list))\n",
    "\n",
    "        for c in root.child:\n",
    "            print_tree(c)\n",
    "    else:\n",
    "        print(\"Child splitted on feature -> \" + str(root.parent.attr))\n",
    "        print(\"Decision: \" + str(root.decision))\n",
    "        print(\"Value of child is -> \" + str(root.value))\n",
    "        # print(\"Feature to be used -> \" + str(root.attr))\n",
    "        child_list = []\n",
    "        for i in range(len(root.child)):\n",
    "            child_list.append(root.child[i].value)\n",
    "            \n",
    "        print(\"Child Values -> \" + str(child_list))\n",
    "\n",
    "        for c in root.child:\n",
    "            print_tree(c)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y1,y2):\n",
    "    count = 0.0\n",
    "    for i in range(y1.shape[0]):\n",
    "        if y1[i] == y2[i]:\n",
    "            count+=1.0\n",
    "    return (count)/(y1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(root):\n",
    "    i = 0\n",
    "    node_list = [root]\n",
    "    while i < len(node_list):\n",
    "        top_node = node_list[i]\n",
    "        if len(top_node.child) > 0:\n",
    "            for c in top_node.child:\n",
    "                node_list.append(c)\n",
    "        i+=1\n",
    "    return node_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading test data...\n",
      "Loading val data...\n"
     ]
    }
   ],
   "source": [
    "PART = 'a'\n",
    "one_hot_encoding = False\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "print(\"Loading data...\")\n",
    "Xtrain, Ytrain = load_data(one_hot_encoding, numeric_cols, train_path)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "Xtest, Ytest = load_data(one_hot_encoding, numeric_cols, test_path)\n",
    "\n",
    "print(\"Loading val data...\")\n",
    "Xval, Yval = load_data(one_hot_encoding, numeric_cols, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading test data...\n",
      "Loading val data...\n"
     ]
    }
   ],
   "source": [
    "PART = 'a'\n",
    "one_hot_encoding = True\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "print(\"Loading data...\")\n",
    "Xtrain, Ytrain, values_dict = load_data(one_hot_encoding, numeric_cols, train_path, {})\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "Xtest, Ytest = load_data(one_hot_encoding, numeric_cols, test_path, values_dict)\n",
    "\n",
    "print(\"Loading val data...\")\n",
    "Xval, Yval = load_data(one_hot_encoding, numeric_cols, val_path, values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Depth: 0\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.6%\n",
      "\n",
      "\n",
      "Current Depth: 1\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.6%\n",
      "\n",
      "\n",
      "Current Depth: 2\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.6%\n",
      "\n",
      "\n",
      "Current Depth: 3\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.6%\n",
      "\n",
      "\n",
      "Current Depth: 4\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.6%\n",
      "\n",
      "\n",
      "Current Depth: 5\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.6%\n",
      "\n",
      "\n",
      "Current Depth: 6\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.6%\n",
      "\n",
      "\n",
      "Current Depth: 7\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.4%\n",
      "Test Accuracy: 87.4%\n",
      "Val Accuracy: 88.7%\n",
      "\n",
      "\n",
      "Current Depth: 8\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.6%\n",
      "Test Accuracy: 87.6%\n",
      "Val Accuracy: 88.5%\n",
      "\n",
      "\n",
      "Current Depth: 9\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.7%\n",
      "Test Accuracy: 87.7%\n",
      "Val Accuracy: 88.7%\n",
      "\n",
      "\n",
      "Current Depth: 10\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 88.9%\n",
      "Test Accuracy: 87.7%\n",
      "Val Accuracy: 88.5%\n",
      "\n",
      "\n",
      "Current Depth: 11\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 89.1%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.5%\n",
      "\n",
      "\n",
      "Current Depth: 12\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 89.5%\n",
      "Test Accuracy: 87.6%\n",
      "Val Accuracy: 88.5%\n",
      "\n",
      "\n",
      "Current Depth: 13\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 90.10000000000001%\n",
      "Test Accuracy: 87.5%\n",
      "Val Accuracy: 88.1%\n",
      "\n",
      "\n",
      "Current Depth: 14\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 90.5%\n",
      "Test Accuracy: 87.6%\n",
      "Val Accuracy: 88.2%\n",
      "\n",
      "\n",
      "Current Depth: 15\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 90.9%\n",
      "Test Accuracy: 87.6%\n",
      "Val Accuracy: 88.1%\n",
      "\n",
      "\n",
      "Current Depth: 16\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 91.4%\n",
      "Test Accuracy: 87.3%\n",
      "Val Accuracy: 87.7%\n",
      "\n",
      "\n",
      "Current Depth: 17\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 91.5%\n",
      "Test Accuracy: 87.3%\n",
      "Val Accuracy: 87.6%\n",
      "\n",
      "\n",
      "Current Depth: 18\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 91.60000000000001%\n",
      "Test Accuracy: 87.2%\n",
      "Val Accuracy: 87.5%\n",
      "\n",
      "\n",
      "Current Depth: 19\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 91.7%\n",
      "Test Accuracy: 87.3%\n",
      "Val Accuracy: 87.5%\n",
      "\n",
      "\n",
      "Current Depth: 20\n",
      "Training ...\n",
      "Prediction ...\n",
      "Train Accuracy: 91.7%\n",
      "Test Accuracy: 87.2%\n",
      "Val Accuracy: 87.5%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if PART == 'a':\n",
    "prediction = {}\n",
    "\n",
    "MAX_DEPTH = 20\n",
    "\n",
    "for depth in range(MAX_DEPTH+1):\n",
    "    \n",
    "    print(\"Current Depth:\",depth)\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    print(\"Training ...\")\n",
    "    start = time.time()    \n",
    "    dc_tree = decision_tree(one_hot_encoding, categorical_cols, numeric_cols, Xtrain, Ytrain, depth)    \n",
    "    end = time.time()\n",
    "    \n",
    "    temp.append(round(end-start,5))\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"Prediction ...\")\n",
    "    y_pred = predict(one_hot_encoding, Xtrain, dc_tree, numeric_cols)\n",
    "    acc1 = round(accuracy(y_pred,Ytrain),3)\n",
    "    print(\"Train Accuracy: {}%\".format(100.0*acc1))\n",
    "\n",
    "    y_pred2 = predict(one_hot_encoding, Xtest, dc_tree, numeric_cols)\n",
    "    acc2 = round(accuracy(y_pred2,Ytest),3)\n",
    "    print(\"Test Accuracy: {}%\".format(100.0*acc2))\n",
    "\n",
    "    y_pred3 = predict(one_hot_encoding, Xval, dc_tree, numeric_cols)\n",
    "    acc3 = round(accuracy(y_pred3,Yval),3)\n",
    "    print(\"Val Accuracy: {}%\\n\\n\".format(100.0*acc3))\n",
    "    \n",
    "    end = time.time()\n",
    "    temp.append(round(end-start,5))\n",
    "            \n",
    "    temp.extend([acc1, acc2, acc3])\n",
    "    \n",
    "    node_list = bfs(dc_tree)\n",
    "    temp.insert(0,len(node_list))\n",
    "    \n",
    "    prediction[str(depth)] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [3, 0.31705, 5.33835, 0.884, 0.875, 0.886], '1': [7, 0.43253, 5.2421, 0.884, 0.875, 0.886], '2': [15, 0.66857, 5.47073, 0.884, 0.875, 0.886], '3': [31, 0.87907, 5.73833, 0.884, 0.875, 0.886], '4': [62, 1.05706, 6.15257, 0.884, 0.875, 0.886], '5': [121, 1.42302, 6.93454, 0.884, 0.875, 0.886], '6': [219, 1.65839, 6.87896, 0.884, 0.875, 0.886], '7': [388, 2.06141, 7.22146, 0.884, 0.874, 0.887], '8': [670, 2.76444, 7.63186, 0.886, 0.876, 0.885], '9': [1159, 3.91822, 7.76115, 0.887, 0.877, 0.887], '10': [1905, 5.47081, 8.21427, 0.889, 0.877, 0.885], '11': [3032, 7.96787, 8.42848, 0.891, 0.875, 0.885], '12': [4614, 11.19088, 8.61657, 0.895, 0.876, 0.885], '13': [6660, 15.3253, 12.16928, 0.901, 0.875, 0.881], '14': [9259, 25.74125, 10.94353, 0.905, 0.876, 0.882], '15': [12303, 28.3695, 11.45831, 0.909, 0.876, 0.881], '16': [15379, 44.36459, 11.388, 0.914, 0.873, 0.877], '17': [18313, 49.33252, 12.57808, 0.915, 0.873, 0.876], '18': [21097, 64.35721, 12.74111, 0.916, 0.872, 0.875], '19': [23637, 71.97502, 14.05172, 0.917, 0.873, 0.875], '20': [26061, 83.90648, 13.57939, 0.917, 0.872, 0.875]}\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n",
    "\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "val_acc = []\n",
    "total_nodes = []\n",
    "time_train = []\n",
    "time_pred = []\n",
    "\n",
    "for i in prediction:\n",
    "    total_nodes.append(prediction[i][0])\n",
    "    time_train.append(prediction[i][1])\n",
    "    time_pred.append(prediction[i][2])\n",
    "    train_acc.append(prediction[i][3])\n",
    "    test_acc.append(prediction[i][4])\n",
    "    val_acc.append(prediction[i][5])\n",
    "\n",
    "# {'0': [3, 0.25335, 5.79341, 0.884, 0.875, 0.886], '1': [7, 0.51829, 6.48452, 0.884, 0.875, 0.886], '2': [15, 0.80137, 7.35196, 0.884, 0.875, 0.886], '3': [29, 1.36934, 7.37438, 0.884, 0.875, 0.886], '4': [53, 1.32061, 6.9076, 0.884, 0.875, 0.886], '5': [91, 1.85822, 7.40847, 0.884, 0.875, 0.886], '6': [151, 3.37831, 9.56334, 0.884, 0.875, 0.886], '7': [251, 3.40111, 10.80501, 0.884, 0.875, 0.886], '8': [404, 3.66044, 11.47056, 0.884, 0.875, 0.886], '9': [646, 5.30307, 9.7246, 0.884, 0.874, 0.887], '10': [996, 6.57841, 9.7283, 0.884, 0.874, 0.887], '11': [1475, 9.1379, 10.54124, 0.885, 0.874, 0.886], '12': [2112, 9.83179, 9.59183, 0.886, 0.874, 0.885], '13': [2898, 12.83813, 9.31074, 0.887, 0.874, 0.886], '14': [3797, 19.55251, 11.98687, 0.888, 0.872, 0.885], '15': [4755, 22.48475, 12.16239, 0.89, 0.871, 0.884], '16': [5664, 28.47441, 12.01079, 0.891, 0.872, 0.884], '17': [6477, 30.86752, 10.35908, 0.891, 0.871, 0.884], '18': [7247, 35.12374, 13.57276, 0.891, 0.871, 0.884], '19': [8017, 53.39867, 17.70567, 0.891, 0.871, 0.884], '20': [8787, 52.41782, 17.06274, 0.891, 0.871, 0.884]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def graph_plot(train, test, val, total_nodes, filename):\n",
    "    \n",
    "    # plt.figure()\n",
    "    # plt.figure.set_figwidth(4)\n",
    "    # plt.figure.set_figheight(1)    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    x = [int(i) for i in range(len(train))]\n",
    "    # x = total_nodes\n",
    "    \n",
    "    plt.title(\"Train/Test/Val Accuracies vs Depth/Nodes\")\n",
    "    plt.xlabel(\"Depth\")\n",
    "    plt.ylabel(\"Accuracies\")\n",
    "    plt.plot(x, train,label=\"Train\")\n",
    "    plt.plot(x, test,label=\"Test\")\n",
    "    plt.plot(x, val, label=\"Val\")\n",
    "    # for i in range(len(train)):\n",
    "    #     plt.annotate(\"Nodes: \" + str(total_nodes[i]), xy=(x[i], train[i]+1))\n",
    "    plt.legend()\n",
    "    plt.savefig(filename)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# graph_plot(train_acc, test_acc, val_acc, total_nodes, \"../output/decision_trees_accuracy_vs_nodes.jpg\")\n",
    "graph_plot(train_acc, test_acc, val_acc, total_nodes, \"../output/decision_trees_accuracy_vs_depth_ohe.jpg\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loading test data...\n",
      "Loading val data...\n"
     ]
    }
   ],
   "source": [
    "# PART B (POST PRUNING)\n",
    "\n",
    "PART = 'b'\n",
    "one_hot_encoding = True\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "print(\"Loading data...\")\n",
    "Xtrain, Ytrain, values_dict = load_data(one_hot_encoding, numeric_cols, train_path, {})\n",
    "# Xtrain, Ytrain = load_data(one_hot_encoding, numeric_cols, train_path, {})\n",
    "# values_dict = {}\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "Xtest, Ytest = load_data(one_hot_encoding, numeric_cols, test_path, values_dict)\n",
    "\n",
    "print(\"Loading val data...\")\n",
    "Xval, Yval = load_data(one_hot_encoding, numeric_cols, val_path, values_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_tree = decision_tree(one_hot_encoding, categorical_cols, numeric_cols, Xtrain, Ytrain, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = bfs(dc_tree)\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "val_accuracy = []\n",
    "node_count = [len(node_list)]\n",
    "\n",
    "train_accuracy.append(accuracy_score(predict(one_hot_encoding, Xtrain, dc_tree, numeric_cols), Ytrain))\n",
    "test_accuracy.append(accuracy_score(predict(one_hot_encoding, Xtest, dc_tree, numeric_cols), Ytest))\n",
    "val_accuracy.append(accuracy_score(predict(one_hot_encoding, Xval, dc_tree, numeric_cols), Yval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8869166113691661] [0.8767971687679716] [0.8865546218487395]\n",
      "1159\n"
     ]
    }
   ],
   "source": [
    "print(train_accuracy, test_accuracy, val_accuracy)\n",
    "print(len(node_list))\n",
    "# [0.8930269851802699] [0.8896261888962619] [0.8920831490490934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After:  0.885891198584697 Val Best:  0.8869969040247678 Node number: 10\n",
      "After:  0.8863334807607254 Val Best:  0.8869969040247678 Node number: 20\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 30\n",
      "After:  0.8863334807607254 Val Best:  0.8869969040247678 Node number: 40\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 50\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 60\n",
      "After:  0.8869969040247678 Val Best:  0.8869969040247678 Node number: 70\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 80\n",
      "After:  0.8867757629367536 Val Best:  0.8869969040247678 Node number: 90\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 100\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 110\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 120\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 130\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 140\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 150\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 160\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 170\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 180\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 190\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 200\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 210\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 220\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 230\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 240\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 260\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 270\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 280\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 290\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 310\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 320\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 330\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 340\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 350\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 360\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 370\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 380\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 400\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 410\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 420\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 430\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 440\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 460\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 470\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 480\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 490\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 500\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 510\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 520\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 530\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 540\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 550\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 560\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 570\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 590\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 600\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 610\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 620\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 630\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 640\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 650\n",
      "After:  0.8865546218487395 Val Best:  0.8869969040247678 Node number: 660\n",
      "After:  0.8863334807607254 Val Best:  0.8869969040247678 Node number: 670\n",
      "Best Accuracy:  0.8869969040247678   Previous:  0.8865546218487395\n",
      "Iteration: 1, Val_accuracy: 0.8869969040247678, Node_count: 863\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 10\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 20\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 30\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 40\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 50\n",
      "After:  0.8872180451127819 Val Best:  0.8872180451127819 Node number: 60\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 70\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 80\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 90\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 100\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 110\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 120\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 130\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 140\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 150\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 160\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 170\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 180\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 190\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 210\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 220\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 250\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 260\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 270\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 280\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 290\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 300\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 310\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 320\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 330\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 340\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 350\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 360\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 370\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 380\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 390\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 400\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 410\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 420\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 430\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 440\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 450\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 460\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 470\n",
      "After:  0.8869969040247678 Val Best:  0.8872180451127819 Node number: 480\n",
      "After:  0.8867757629367536 Val Best:  0.8872180451127819 Node number: 490\n",
      "Best Accuracy:  0.8872180451127819   Previous:  0.8869969040247678\n",
      "Iteration: 2, Val_accuracy: 0.8872180451127819, Node_count: 765\n",
      "After:  0.8872180451127819 Val Best:  0.8872180451127819 Node number: 10\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 20\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 30\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 40\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 50\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 60\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 80\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 90\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 100\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 110\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 120\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 130\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 140\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 150\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 170\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 180\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 190\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 200\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 210\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 220\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 230\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 240\n",
      "After:  0.8869969040247678 Val Best:  0.8874391862007961 Node number: 250\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 260\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 270\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 280\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 290\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 300\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 310\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 320\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 330\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 340\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 360\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 370\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 380\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 390\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 400\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 410\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 420\n",
      "After:  0.8872180451127819 Val Best:  0.8874391862007961 Node number: 430\n",
      "Best Accuracy:  0.8874391862007961   Previous:  0.8872180451127819\n",
      "Iteration: 3, Val_accuracy: 0.8874391862007961, Node_count: 663\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 10\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 20\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 30\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 40\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 50\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 70\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 80\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 90\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 100\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 110\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 120\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 130\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 140\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 150\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 160\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 170\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 180\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 200\n",
      "After:  0.8874391862007961 Val Best:  0.8874391862007961 Node number: 210\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 230\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 250\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 260\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 270\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 280\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 290\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 300\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 310\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 320\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 330\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 340\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 350\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 360\n",
      "After:  0.8874391862007961 Val Best:  0.8876603272888103 Node number: 370\n",
      "After:  0.8869969040247678 Val Best:  0.8876603272888103 Node number: 380\n",
      "Best Accuracy:  0.8876603272888103   Previous:  0.8874391862007961\n",
      "Iteration: 4, Val_accuracy: 0.8876603272888103, Node_count: 657\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 10\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 20\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 30\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 40\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 50\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 70\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 80\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 90\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 100\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 110\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 120\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 130\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 140\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 150\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 160\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 170\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 180\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 200\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 210\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 230\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 250\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 260\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 270\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 280\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 290\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 300\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 310\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 320\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 330\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 340\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 350\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 360\n",
      "After:  0.8876603272888103 Val Best:  0.8876603272888103 Node number: 370\n",
      "Best Accuracy:  0.8876603272888103   Previous:  0.8876603272888103\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "val_best_accuracy = val_accuracy[-1]\n",
    "\n",
    "while True:\n",
    "    iteration += 1\n",
    "    previous_accuracy = val_accuracy[-1]\n",
    "    after_accuracy    = None\n",
    "    tree_best_node    = tree\n",
    "    \n",
    "    count = 0\n",
    "    # node_list = node_list[::-1]\n",
    "    for node in node_list:\n",
    "        count = count + 1\n",
    "        if len(node.child) > 0:\n",
    "            child_temp = node.child\n",
    "            node.child = []\n",
    "            after_accuracy = accuracy_score(predict(one_hot_encoding, Xval, dc_tree, numeric_cols), Yval)\n",
    "            \n",
    "            if count%10 == 0:\n",
    "                print(\"After: \",after_accuracy,\"Val Best: \",val_best_accuracy,\"Node number:\",count)\n",
    "                \n",
    "            if after_accuracy > val_best_accuracy:\n",
    "                val_best_accuracy = after_accuracy\n",
    "                tree_best_node = node\n",
    "                # break\n",
    "            node.child = child_temp\n",
    "    \n",
    "    print(\"Best Accuracy: \",val_best_accuracy,\"  Previous: \",previous_accuracy)\n",
    "    if val_best_accuracy > previous_accuracy:\n",
    "        tree_best_node.child = [] # PRUNING\n",
    "    \n",
    "        node_list = bfs(dc_tree)\n",
    "        train_accuracy.append(accuracy_score(predict(one_hot_encoding, Xtrain, dc_tree, numeric_cols), Ytrain))\n",
    "        test_accuracy.append(accuracy_score(predict(one_hot_encoding, Xtest, dc_tree, numeric_cols), Ytest))\n",
    "        val_accuracy.append(accuracy_score(predict(one_hot_encoding, Xval, dc_tree, numeric_cols), Yval))\n",
    "        node_count.append(len(node_list))\n",
    "\n",
    "        print(\"Iteration: {}, Val_accuracy: {}, Node_count: {}\".format(iteration, val_accuracy[-1], node_count[-1]))        \n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1704, 1595, 1294, 1288]\n",
      "Train:  [0.8954877239548772, 0.8947135589471356, 0.8945753151957532, 0.8944370714443707]\n",
      "Test:  [0.8854235788542358, 0.8885202388852024, 0.8885202388852024, 0.8891838088918381]\n",
      "Val:  [0.8911985846970367, 0.8931888544891641, 0.8934099955771783, 0.8936311366651923]\n"
     ]
    }
   ],
   "source": [
    "print(node_count)\n",
    "print(\"Train: \", train_accuracy)\n",
    "print(\"Test: \", test_accuracy)\n",
    "print(\"Val: \", val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C Random Forest\n",
    "def load_data_rf(numeric_cols, filename, values_dict = {}):\n",
    "    df = pd.read_csv(filename, delimiter = ';')\n",
    "    Y = df['y'].copy()\n",
    "    Y  = Y.to_numpy()\n",
    "    for i in range(Y.shape[0]):\n",
    "        if Y[i] == 'yes':\n",
    "            Y[i] = 1\n",
    "        else:\n",
    "            Y[i] = 0 #Assigning 0 to nan values\n",
    "    \n",
    "    Y = Y.astype('int64')\n",
    "    df = df.drop(['y'],axis=1)\n",
    "                \n",
    "    if values_dict == {}:\n",
    "        for col in df.columns:\n",
    "            if col not in numeric_cols:\n",
    "                values = list(set(list(df[col])))\n",
    "                values_dict[col] = values                \n",
    "                for i in range(df.shape[0]):\n",
    "                    temp = df[col][i]\n",
    "                    df[col][i] = np.zeros(len(values))\n",
    "                    df[col][i][values_dict[col].index(temp)] = 1\n",
    "        return df, Y, values_dict\n",
    "    \n",
    "    else:\n",
    "        for col in df.columns:\n",
    "            if col not in numeric_cols:\n",
    "                for i in range(df.shape[0]):\n",
    "                    temp = df[col][i]\n",
    "                    df[col][i] = np.zeros(len(values_dict[col]))\n",
    "                    if temp in values_dict[col]:\n",
    "                        df[col][i][values_dict[col].index(temp)] = 1                        \n",
    "        return df, Y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data...\n",
      "Loading test data...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_data_rf() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26123/3754167611.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading test data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mXtest_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest_rf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_rf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues_dict_rf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading val data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: load_data_rf() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "PART = 'c'\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "print(\"Loading train data...\")\n",
    "Xtrain_rf, Ytrain_rf, values_dict_rf = load_data_rf(numeric_cols, train_path, {})\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "Xtest_rf, Ytest_rf = load_data_rf(numeric_cols, test_path, values_dict_rf)\n",
    "\n",
    "print(\"Loading val data...\")\n",
    "Xval_rf, Yval_rf = load_data_rf(numeric_cols, val_path, values_dict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"num_estimators\": 14,\n",
    "    \"bootstrap\": True,\n",
    "    \"num_features\": 52,\n",
    "    \"accuracy\": -1\n",
    "}\n",
    "\n",
    "n_estimators = [50, 150, 250, 350, 450]\n",
    "max_features = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "min_samples_split = [2, 4, 6, 8, 10]\n",
    "\n",
    "bootstrap_list = [True, False]\n",
    "max_features_list = np.arange(1,Xtrain_rf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in n_estimators:\n",
    "    for bs in bootstrap_list:\n",
    "        for m in max_features_list:\n",
    "            rf = RandomForestClassifier(n_estimators=n, criterion=\"entropy\", bootstrap=bs, max_features=m, min_samples_split=10)\n",
    "            # rf.fit(Xtrain_rf, Ytrain_rf.reshape(-1))\n",
    "            rf.fit(Xtrain, Ytrain.reshape(-1))\n",
    "            # acc = accuracy_score(Yval_rf, np.array(rf.predict(Xval_rf), dtype=int))\n",
    "            acc = accuracy_score(Yval, np.array(rf.predict(Xval), dtype=int))\n",
    "            if acc > best_params['accuracy']:\n",
    "                print(\"Accuracy: \", acc)                \n",
    "                best_params['num_estimators'] = n\n",
    "                best_params['bootstrap'] = bs\n",
    "                best_params['num_features'] = m\n",
    "                best_params['accuracy'] = acc\n",
    "                break\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename, values_dict={}):\n",
    "    df = np.asarray(pd.read_csv(filename, header=None, dtype=int))\n",
    "    if values_dict == {}:\n",
    "        for i in range(df.shape[1]):\n",
    "            length = list(set(list(df[:,i])))\n",
    "            values_dict[i] = length\n",
    "            for j in range(df.shape[0]):\n",
    "                temp = df[j][i]\n",
    "                df[j][i] = np.zeros(len(length))\n",
    "                df[j][i][temp-1] = 1\n",
    "    else:\n",
    "        for i in range(df.shape[1]):\n",
    "            length = values_dict[i]\n",
    "            for j in range(df.shape[0]):\n",
    "                temp = df[j][i]\n",
    "                df[j][i] = np.zeros(len(length))\n",
    "                df[j][i][temp-1] = 1\n",
    "            # df[j] = df[j].ravel()\n",
    "        \n",
    "    return df[:,0:df.shape[1]-1], df[:,df.shape[1]-1]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
