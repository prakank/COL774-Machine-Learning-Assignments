{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtYsjz0VRg5L"
      },
      "source": [
        "1. Encoder: Design a CNN based encoder that handles the variable sized\n",
        "images.\n",
        "2. Decoder: Design a RNN / LSTM based decoder which generates the\n",
        "captions given the encoded image input. Note that you can either design\n",
        "a word-level or a character-level LSTM for generating the caption.\n",
        "3. Training strategy: Use cross-entropy as the loss function and [teacher forcing](https://machinelearningmastery.com/teacher-forcing-for-recurrent-neural-networks/) for training the decoder. Donâ€™t forget to use START and END\n",
        "tokens to allow variable length caption outputs in the decoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ky4NZC_Ih_-",
        "outputId": "848249e7-b951-4c09-d11e-a2add87cc123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F-u7sQ2nKTVX",
        "outputId": "465503c8-b132-4917-a235-49a72706f972"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5Igojb-Zw5Tj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "from skimage import io, transform\n",
        "\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "import re\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiNmxcKi0xTD",
        "outputId": "71a8aba1-992b-467f-a194-aa394395702a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vttv_KYB9lQR"
      },
      "source": [
        "## Image Tranformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "fY7HhVDCw6Ev"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        return img\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # swap color axis because\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1)) # Order of destination axis\n",
        "        # image = image.type(torch.FloatTensor)\n",
        "        # image = image.float\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "IMAGE_RESIZE = (256, 256)\n",
        "\n",
        "# Sequentially compose the transforms\n",
        "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()]) # Applied sequentially\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXgbCtlmtoYY",
        "outputId": "ec7107b4-471d-4415-8fbb-7cc644dc2f97"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRN1vORm9px8"
      },
      "source": [
        "## Caption Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "3yS-KTQ89q75"
      },
      "outputs": [],
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "\n",
        "        # Read raw captions\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "\n",
        "        # Preprocess captions\n",
        "        self.captions_dict = self.process_captions()\n",
        "\n",
        "        # Create vocabulary\n",
        "        self.vocab, self.inv_vocab = self.generate_vocabulary()\n",
        "        \n",
        "    def read_raw_captions(self):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            Dictionary with raw captions list keyed by image ids (integers)\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = {}\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                image_path = \"/content/drive/MyDrive/data/train_data_main/\" + img_captions[0]\n",
        "                \n",
        "                captions_dict[img_captions[0]] = img_captions[1]\n",
        "                # if os.path.exists(image_path):\n",
        "                #     captions_dict[img_captions[0]] = img_captions[1]\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        # Can remove the stopwords and gibberish in the caption\n",
        "        stop_words = stopwords.words('english')\n",
        "        punctuation = list(string.punctuation)\n",
        "\n",
        "        for key, value in raw_captions_dict.items():\n",
        "            cleaned_caption = re.sub('[^A-Za-z0-9]+', ' ', value) #Extra space removal\n",
        "            tokens = word_tokenize(cleaned_caption)\n",
        "            cleaned_tokens = [token for token in tokens if token not in stop_words and token not in punctuation] # Remove stopwords and punctuation\n",
        "            cleaned_caption = \"[START] \" + \" \".join(cleaned_tokens) + \" [END]\"\n",
        "            raw_captions_dict[key] = cleaned_caption        \n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = self.captions_dict\n",
        "        \n",
        "        vocabulary = {}\n",
        "        inv_vocabulary = {}\n",
        "        \n",
        "        max_caption = 0\n",
        "        idx = 1\n",
        "\n",
        "        for key, value in captions_dict.items():\n",
        "            val = value.split()\n",
        "            max_caption = max(max_caption, len(val))\n",
        "\n",
        "            for i in val:\n",
        "                if i not in vocabulary.keys():\n",
        "                    vocabulary[i] = idx\n",
        "                    inv_vocabulary[idx] = i\n",
        "                    idx+=1\n",
        "        self.max_caption = max_caption\n",
        "\n",
        "        # Generate the vocabulary        \n",
        "\n",
        "        return vocabulary, inv_vocabulary\n",
        "    \n",
        "    def get_caption(self, tensor_tokens):\n",
        "        caption = []\n",
        "        for i in tensor_tokens:\n",
        "            if int(i) in self.inv_vocab:\n",
        "                caption.append(self.inv_vocab[int(i)])\n",
        "            else:\n",
        "                caption.append('TOKEN')\n",
        "\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def captions_transform(self, img_caption):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = img_caption.split(\" \")\n",
        "        caption_mapped = np.zeros(self.max_caption)\n",
        "        \n",
        "        # print(img_caption, caption)\n",
        "\n",
        "        for i in range(len(caption)):\n",
        "            try:\n",
        "                caption_mapped[i] = self.vocab[caption[i]]\n",
        "            except:\n",
        "                print(img_caption, caption, i)\n",
        "        \n",
        "        caption = torch.from_numpy(caption_mapped)\n",
        "\n",
        "        # Generate tensors\n",
        "\n",
        "        # return torch.zeros(len(img_caption_list), 10)\n",
        "        # Input: 5\n",
        "        # Vocab: 100\n",
        "        # Output: 5?\n",
        "\n",
        "        # Convert array to tensor\n",
        "        # Use collate function (padding)\n",
        "        # DataLoader\n",
        "\n",
        "        return caption\n",
        "\n",
        "# Set the captions tsv file path\n",
        "BASE_DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'\n",
        "CAPTIONS_FILE_PATH = os.path.join(BASE_DIR, 'data', 'train_text.tsv')\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xelH9BDTeOeJ",
        "outputId": "7c78f41e-3a2c-47f0-8d55-b0f49514f819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7356\n",
            "50000\n",
            "11\n"
          ]
        }
      ],
      "source": [
        "print(len(captions_preprocessing_obj.vocab))\n",
        "print(len(captions_preprocessing_obj.captions_dict))\n",
        "print(captions_preprocessing_obj.max_caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQr28RSD9t4y"
      },
      "source": [
        "## DataSet Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "hPtgrsbq9u_t"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "\n",
        "        sample = {'image': image, 'captions': captions}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestDatasetLoader(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, img_transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the test images.            \n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        \n",
        "        self.image_ids = ['test_data/test' + str(i) + '.jpg' for i in range(1, 5001)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        \n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        \n",
        "        sample = {'image': image}\n",
        "        \n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkcjuZ1t9xhv"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "yezcNlgQreeM"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    \"\"\"\n",
        "       data: is a list of tuples with (example, label, length)\n",
        "             where 'example' is a tensor of arbitrary shape\n",
        "             and label/length are scalars\n",
        "    \"\"\"\n",
        "    for i in zip(*data):\n",
        "        print(i)\n",
        "\n",
        "    labels, lengths = zip(*data)\n",
        "    max_len = max(lengths)\n",
        "    n_ftrs = data[0][0].size(1)\n",
        "    features = torch.zeros((len(data), max_len, n_ftrs))\n",
        "    labels = torch.tensor(labels)\n",
        "    lengths = torch.tensor(lengths)\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        j, k = data[i][0].size(0), data[i][0].size(1)\n",
        "        features[i] = torch.cat([data[i][0], torch.zeros((max_len - j, k))])\n",
        "\n",
        "    return features.float(), labels.long(), lengths.long()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "OKU6wqJW91RW"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, encoded_space_dim, fully_connected_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Given groups=1, weight of size [8, 1, 3, 3], expected input[32, 3, 256, 256] to have 1 channels, but got 3 channels instead\n",
        "\n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0),\n",
        "            nn.ReLU(True)\n",
        "        )        \n",
        "\n",
        "        # print(\"Part 1\")\n",
        "\n",
        "        self.flatten = nn.Flatten(start_dim=1)\n",
        "\n",
        "        # print(\"Part 2\")\n",
        "\n",
        "        self.encoder_lin = nn.Sequential(\n",
        "            nn.Linear(30752, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, encoded_space_dim)\n",
        "        )\n",
        "\n",
        "        # print(\"Part 3\")\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.encoder_cnn(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.encoder_lin(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoded_space_dim,fully_connected_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder_lin = nn.Sequential(\n",
        "            nn.Linear(encoded_space_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 288),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # print(\"Decoder begin ... \")\n",
        "        # print('Part 1')\n",
        "\n",
        "        self.unflatten = nn.Unflatten(dim=1, \n",
        "        unflattened_size=(32, 3, 3))\n",
        "\n",
        "        # print('Part 2')\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 16, 3, \n",
        "            stride=2, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, \n",
        "            padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, \n",
        "            padding=1, output_padding=1)\n",
        "        )\n",
        "\n",
        "# torch.Size([32, 1, 28, 11])\n",
        "\n",
        "        # print('Part 3')\n",
        "\n",
        "        self.decoder_flatten_fin = nn.Flatten(start_dim=1)\n",
        "\n",
        "        self.decoder_linear_fin = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 11)\n",
        "        )\n",
        "\n",
        "        # print('Part 4 ...')\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.decoder_lin(x)\n",
        "        x = self.unflatten(x)\n",
        "        x = self.decoder_conv(x)\n",
        "\n",
        "        # print(\"Inside Block ...\")\n",
        "        # print(x.shape)\n",
        "\n",
        "        x = self.decoder_flatten_fin(x)\n",
        "        x = self.decoder_linear_fin(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "-5ZEiF9iK25l"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, encoder, decoder, captions_preprocessing_obj, loss_function, optimizer, epsilon):\n",
        "        super(ImageCaptionsNet, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.loss_function = loss_function\n",
        "        self.optimizer = optimizer\n",
        "        self.epsilon = epsilon\n",
        "        self.captions_preprocessing_obj = captions_preprocessing_obj\n",
        "\n",
        "\n",
        "    def train(self, device, train_loader):\n",
        "        self.encoder.train()\n",
        "        self.decoder.train()\n",
        "\n",
        "        train_loss = []\n",
        "\n",
        "        for batch_idx, sample in enumerate(train_loader):\n",
        "            net.zero_grad()\n",
        "            image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "            # print(type(image_batch), image_batch.dtype)\n",
        "            # print(type(captions_batch), captions_batch.dtype)\n",
        "            # If GPU training required\n",
        "            if torch.cuda.is_available():\n",
        "                image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "\n",
        "            image_batch = image_batch.float()\n",
        "            captions_batch = captions_batch.float()\n",
        "\n",
        "            encoded_vector = self.encoder(image_batch)\n",
        "            # print(\"Encoded:\",batch_idx)\n",
        "            \n",
        "            output_caption = self.decoder(encoded_vector).float()\n",
        "            # print(\"Decoded:\",batch_idx)\n",
        "            \n",
        "            # print(output_caption.shape)\n",
        "            # print(captions_batch.shape)\n",
        "\n",
        "            print((captions_batch.view(-1).shape))\n",
        "        \n",
        "            loss = self.loss_function(captions_batch.view(-1), output_caption.view(-1)) # 32*11\n",
        "            \n",
        "            # loss = loss_function(output_captions.view(-1,vocab_size), captions.view(-1))\n",
        "            \n",
        "            print(\"Batch:{}, partial train loss (single batch):{}\\n\".format(batch_idx, loss.data), end=\"\")\n",
        "            \n",
        "            # for i in range(captions_batch.shape[0]):\n",
        "                # print(captions_preprocessing_obj.get_caption(captions_batch[i]))\n",
        "                # print(captions_preprocessing_obj.get_caption(output_caption[i]))\n",
        "                # print((captions_batch[i]))\n",
        "                # print((output_caption[i]))\n",
        "                # print()\n",
        "            \n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # print(\"Optimizer Zero grad successful\")\n",
        "\n",
        "            loss.backward(loss)\n",
        "\n",
        "            # print(\"Loss backward successful\")\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # print(\"Optimizer step successful\")\n",
        "                        \n",
        "            train_loss = np.append(train_loss, (loss.detach().cpu().numpy()))            \n",
        "\n",
        "            if len(train_loss) > 1:\n",
        "                if abs(train_loss[-1] - train_loss[-2]) < EPSILON:\n",
        "                    return np.mean(train_loss)\n",
        "        \n",
        "        return np.mean(train_loss)\n",
        "\n",
        "    def predict(self, device, test_loader):\n",
        "        self.encoder.eval()\n",
        "        self.decoder.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            conc_out = []\n",
        "            captions = []\n",
        "            # conc_label = []\n",
        "\n",
        "            for batch_idx, sample in enumerate(test_loader):\n",
        "                # Move tensor to the proper device\n",
        "                image_batch = sample['image'].float()\n",
        "                \n",
        "                image_batch = image_batch.to(device)\n",
        "                # Encode data\n",
        "                encoded_data = self.encoder(image_batch)\n",
        "                # Decode data\n",
        "                decoded_data = self.decoder(encoded_data)  # 32*11\n",
        "\n",
        "                conc_out = decoded_data.cpu().numpy()\n",
        "                                \n",
        "                for i in conc_out:\n",
        "                    captions.append(self.captions_preprocessing_obj.get_caption(i))\n",
        "\n",
        "                print(\"Batch:{}\".format(batch_idx))\n",
        "                \n",
        "                if batch_idx > 3:\n",
        "                    return captions\n",
        "\n",
        "        return captions\n",
        "            # conc_out = torch.cat(conc_out)\n",
        "            # conc_label = torch.cat(conc_label)\n",
        "            # Evaluate global loss\n",
        "            # val_loss = loss_fn(conc_out, conc_label)\n",
        "        # return val_loss.data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjyzweOS95gW"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "EFypAuDkzpPz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([352])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[-1, 7356]' is invalid for input of size 352",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_115834/527311839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUMBER_OF_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration:{} , Loss:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_115834/463149907.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, device, train_loader)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_caption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7356\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 32*11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# loss = loss_function(output_captions.view(-1,vocab_size), captions.view(-1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 7356]' is invalid for input of size 352"
          ]
        }
      ],
      "source": [
        "BASE_DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'\n",
        "\n",
        "# os.chdir('/content/drive/MyDrive/')\n",
        "IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Creating the Dataset\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "\n",
        "# Define your hyperparameters\n",
        "NUMBER_OF_EPOCHS = 3\n",
        "LEARNING_RATE = 1\n",
        "EPSILON = 1e-5\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 1 # Parallel threads for dataloading\n",
        "\n",
        "# Creating the DataLoader for batching purposes\n",
        "os.chdir(os.path.join(BASE_DIR, 'data'))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "# loss_function = torch.nn.MSELoss()\n",
        "\n",
        "encoded_space_dim = 10\n",
        "fully_connected_dim = 128\n",
        "\n",
        "encoder = Encoder(encoded_space_dim, fully_connected_dim)\n",
        "decoder = Decoder(encoded_space_dim, fully_connected_dim)\n",
        "\n",
        "params_to_optimize = [{'params':encoder.parameters()},{'params': decoder.parameters()}]\n",
        "\n",
        "optimizer = optim.SGD(params_to_optimize, lr=LEARNING_RATE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "net = ImageCaptionsNet(encoder, decoder, captions_preprocessing_obj, loss_function, optimizer, EPSILON)\n",
        "# net = net.cuda()\n",
        "\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    loss = net.train(device, train_loader)\n",
        "    print(\"Iteration:{} , Loss:{}\".format(str(epoch + 1), loss))\n",
        "\n",
        "# train_loss =train_epoch(encoder,decoder,device,train_loader,loss_fn,optim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "XhY_MGfpDiEv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch:0\n",
            "Batch:1\n",
            "Batch:2\n",
            "Batch:3\n",
            "Batch:4\n"
          ]
        }
      ],
      "source": [
        "# Predcition\n",
        "\n",
        "TEST_IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "test_img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()]) # Applied sequentially\n",
        "\n",
        "# Creating the Dataset\n",
        "test_dataset = TestDatasetLoader(TEST_IMAGE_DIR, img_transform=test_img_transform)\n",
        "\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "output_caption = net.predict(device, test_loader, captions_preprocessing_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(160,)\n"
          ]
        }
      ],
      "source": [
        "output_caption = np.asarray(output_caption)\n",
        "print(output_caption.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN'\n",
            " 'TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN TOKEN']\n"
          ]
        }
      ],
      "source": [
        "print(output_caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiD_kMyM98uN"
      },
      "outputs": [],
      "source": [
        "# os.chdir('/content/drive/MyDrive/')\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/data/train_data_main/'\n",
        "\n",
        "# # Creating the Dataset\n",
        "# train_dataset = ImageCaptionsDataset(\n",
        "#     IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "#     captions_transform=captions_preprocessing_obj.captions_transform\n",
        "# )\n",
        "\n",
        "# # Define your hyperparameters\n",
        "# NUMBER_OF_EPOCHS = 3\n",
        "# LEARNING_RATE = 1e-1\n",
        "# BATCH_SIZE = 32\n",
        "# NUM_WORKERS = 1 # Parallel threads for dataloading\n",
        "# loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# # print(train_dataset.image_ids, len(train_dataset.image_ids))\n",
        "\n",
        "# # Creating the DataLoader for batching purposes\n",
        "# os.chdir('/content/drive/MyDrive/data/train_data_main/')\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "# # print(train_loader.dataset.captions_dict)\n",
        "\n",
        "# import os\n",
        "# for epoch in range(NUMBER_OF_EPOCHS):\n",
        "#     for batch_idx, sample in enumerate(train_loader):\n",
        "#         net.zero_grad()\n",
        "\n",
        "#         image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "#         # If GPU training required\n",
        "#         print(image_batch.shape)\n",
        "#         print(captions_batch.shape)\n",
        "#         image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "\n",
        "#         output_captions = net((image_batch, captions_batch))\n",
        "        \n",
        "#         # output_captions <- output of decoder\n",
        "#         loss = loss_function(output_captions, captions_batch) # Error computation\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#     print(\"Iteration: \" + str(epoch + 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lqd5ZPI6JPZJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "q1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
