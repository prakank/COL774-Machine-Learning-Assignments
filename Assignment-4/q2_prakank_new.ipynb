{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLCSEegQF4Ny",
        "outputId": "84369ed1-4334-4dd1-ab58-9183838fb867"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_K7nJ9KMGzi3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prakank/.local/lib/python3.8/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
            "  from .collection import imread_collection_wrapper\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "from collections import Counter\n",
        "from skimage import io, transform\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "from time import time\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWNNBt0_KJsW",
        "outputId": "1f4f98e4-b1dd-45b4-d2c6-92d17e82b39e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uri1IDlaKhvD",
        "outputId": "0b4714f4-edeb-4a91-a3fb-c65ff3b113ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psutil in /home/prakank/anaconda3/lib/python3.8/site-packages (5.8.0)\n",
            "No module named 'psutil'\n"
          ]
        }
      ],
      "source": [
        "!pip install psutil\n",
        "try:\n",
        "  from psutil import virtual_memory\n",
        "  ram_gb = virtual_memory().total / 1e9\n",
        "  print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "  if ram_gb < 20:\n",
        "    print('Not using a high-RAM runtime')\n",
        "  else:\n",
        "    print('You are using a high-RAM runtime!')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vDdwWT_sG3JM"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        #print(\"TA RESCALE INPUT\", image.shape)\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "sBJ75E7PGIcJ",
        "outputId": "77e094e8-3173-4a01-9fae-06a37a769283"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ThMQbegvIE5x"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return torch.tensor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KbsfVXxIH59",
        "outputId": "a119f05a-8077-47e3-8e50-d8f4135895ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device set to cpu\n"
          ]
        }
      ],
      "source": [
        "IMAGE_RESIZE = (256, 256)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), transforms.RandomRotation(degrees = (90,90))])\n",
        "\n",
        "# 'train': transforms.Compose([\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.RandomRotation(degrees = (90,90)),\n",
        "#     transforms.RandomRotation(degrees = (180,180)),\n",
        "#     transforms.RandomRotation(degrees = (270,270)),\n",
        "#     transforms.RandomVerticalFlip(p=1),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "print(\"Current device set to {}\".format(device))\n",
        "# DIR = '/content/drive/MyDrive/data/train_data_main/'\n",
        "# DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/'\n",
        "DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-vkxd13g6G7",
        "outputId": "e4480226-191e-4027-b28d-56ba174b7988"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/prakank/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/prakank/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "phase = \"Train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2m4vqxvGIcL",
        "outputId": "038d934d-d9e5-4721-e3b3-0372b3e35593"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vocabulary = 7469\n"
          ]
        }
      ],
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "        self.captions_dict = self.process_captions()\n",
        "        self.vocab = self.generate_vocabulary()\n",
        "    def read_raw_captions(self):\n",
        "        # Dictionary with raw captions list keyed by image ids (integers)\n",
        "        captions_dict = {}\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                image_path = DIR + img_captions[0]\n",
        "                \n",
        "                image_path = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/data/' + img_captions[0]\n",
        "                \n",
        "                if os.path.exists(image_path):\n",
        "                    captions_dict[img_captions[0]] = img_captions[1].lower()\n",
        "                    \n",
        "                # if len(captions_dict) == 5000:\n",
        "                #     break\n",
        "                    \n",
        "                    \n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        # Can remove the stopwords and gibberish in the caption\n",
        "        stop_words = stopwords.words('english')\n",
        "        self.allowedLength = 7\n",
        "        punctuation = list(string.punctuation)\n",
        "\n",
        "        for key, value in raw_captions_dict.items():\n",
        "            cleaned_caption = re.sub('[^A-Za-z0-9]+', ' ', value) #Extra space removal\n",
        "            tokens = word_tokenize(cleaned_caption)\n",
        "            # cleaned_tokens = [token for token in tokens if token not in stop_words and token not in punctuation] # Remove stopwords and punctuation\n",
        "            cleaned_tokens = [token for token in tokens if token not in punctuation] # Remove stopwords and punctuation\n",
        "            \n",
        "            # cleaned_caption = \"[START] \" + \" \".join(cleaned_tokens) + \" [END]\"\n",
        "            # cleaned_caption = \" \".join(cleaned_tokens)\n",
        "            cleaned_caption = \" \".join(cleaned_tokens) + \" [END]\"\n",
        "\n",
        "            raw_captions_dict[key] = cleaned_caption        \n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = self.captions_dict\n",
        "        vocabulary = {}\n",
        "        max_caption = 0\n",
        "        idx = 1\n",
        "        index_to_word = {}\n",
        "        for key, value in captions_dict.items():\n",
        "            val = value.split()\n",
        "            max_caption = max(max_caption, len(val))\n",
        "\n",
        "            for i in val:\n",
        "                if i not in vocabulary.keys():\n",
        "                    vocabulary[i] = idx\n",
        "                    index_to_word[idx] = i\n",
        "                    idx+=1\n",
        "        self.max_caption = max_caption\n",
        "        self.max_caption = (self.allowedLength+2)\n",
        "        \n",
        "        index_to_word[0] = \"NIL\"\n",
        "        self.index_to_word = index_to_word\n",
        "        # Generate the vocabulary\n",
        "        print(\"Size of Vocabulary = {}\".format(len(vocabulary)))\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def get_captions(self, tensor_tokens):\n",
        "        caption = [self.index_to_word[int(x)] for x in tensor_tokens]\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def captions_transform(self, img_caption):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = img_caption.split(\" \")\n",
        "        \n",
        "        \n",
        "        # print(img_caption, caption)\n",
        "\n",
        "        caption_mapped = np.zeros(self.max_caption)\n",
        "        for i in range(len(caption)):\n",
        "            try: caption_mapped[i] = self.vocab[caption[i]]\n",
        "            except: print(img_caption, caption, i)\n",
        "\n",
        "        # caption_mapped = np.zeros((self.max_caption, len(self.vocab)))\n",
        "        # for i in range(len(caption)):\n",
        "        #     val = np.zeros(len(self.vocab))\n",
        "        #     val[self.vocab[caption[i]]] = 1\n",
        "        #     caption_mapped[i,:] = val \n",
        "\n",
        "        #captions_mapped = np.argmax(captions_mapped, axis = 1)\n",
        "        \n",
        "        return torch.LongTensor(caption_mapped)\n",
        "\n",
        "# Set the captions tsv file path\n",
        "\n",
        "# CAPTIONS_FILE_PATH = '/content/drive/MyDrive/data/train_text.tsv'\n",
        "# CAPTIONS_FILE_PATH = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/Train_text.tsv'\n",
        "\n",
        "BASE_DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'\n",
        "CAPTIONS_FILE_PATH = os.path.join(BASE_DIR, 'data', 'train_text.tsv')\n",
        "\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)\n",
        "# embedding_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIOyXozxI420",
        "outputId": "aa318eac-0d3e-442a-d04c-466aa6fb74fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assignment-4.pdf                         q1_new.ipynb\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                                    q2_prakank.ipynb\n",
            "data_extraction.ipynb                    q2_prakank_new.ipynb\n",
            "\u001b[01;34mMachine-Learning-Assignments-master\u001b[0m/     q2_temp.ipynb\n",
            "Machine-Learning-Assignments-master.zip  seq2seq_attention.pdf\n",
            "out1.ipynb                               starter_code.ipynb\n",
            "q1.ipynb\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Hr-f4Q7lE9zy"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "\n",
        "        sample = {'image': image, 'captions': captions}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vAkn6005wm8d"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  res = {}\n",
        "\n",
        "  res['image'] = [sample['image'].unsqueeze(0) for sample in batch] \n",
        "  res['image'] = torch.cat((res['image']), dim=0)\n",
        "\n",
        "  res['captions'] = [sample['captions'] for sample in batch]\n",
        "  res['captions'] = torch.nn.utils.rnn.pad_sequence(res['captions'], batch_first=True)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "95MJggLDwU9O"
      },
      "outputs": [],
      "source": [
        "#ENCODER\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, trainCNN = False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.trainCNN = trainCNN\n",
        "\n",
        "        # if not torch.cuda.is_available():\n",
        "        self.inception = torchvision.models.inception_v3(pretrained=True, aux_logits = False)\n",
        "        # else:\n",
        "        #     self.inception = torchvision.models.inception_v3(pretrained=True, aux_logits = False).cuda()\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(embed_size, momentum=0.01)\n",
        "        \n",
        "        self.inception.fc = nn.Linear(in_features=self.inception.fc.in_features, out_features=embed_dim, bias = True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        # self.inception.fc.weight.data.normal_(0., 0.2)\n",
        "        # self.inception.fc.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"Forward feeding\")\n",
        "        features = self.inception(x)\n",
        "        #print(\"Resnet module op\", x.shape)\n",
        "        for name, param in self.inception.named_parameters():\n",
        "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "               param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = self.trainCNN\n",
        "        return (self.dropout((self.relu(features))))\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, vocab_size, enc_dim=256):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.hidden_lin = nn.Linear(lstm_hidden_size, lstm_hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.img_lin = nn.Linear(embed_dim, lstm_hidden_size)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.concat_lin = nn.Linear(lstm_hidden_size, 1)\n",
        "        # self.alpha_map = nn.Linear()\n",
        "        \n",
        "    #     Attention 0 torch.Size([32, 256]) torch.Size([32, 512])\n",
        "    #     Attention Hidden: torch.Size([32, 1, 512])\n",
        "    #     Attention Img_s: torch.Size([32, 512])\n",
        "    #     Attention att_: torch.Size([32, 32, 512])\n",
        "    #     Attention e_: torch.Size([32, 32])\n",
        "    \n",
        "    def forward(self,image_features, hidden_state):\n",
        "        \n",
        "        hidden_h = self.hidden_lin(hidden_state).unsqueeze(1)\n",
        "        \n",
        "        # print('Attention Hidden:',hidden_h.shape)\n",
        "        \n",
        "        img_s = self.img_lin(image_features)\n",
        "        \n",
        "        # print('Attention Img_s:',img_s.shape)\n",
        "        \n",
        "        att_ = self.tanh(img_s + hidden_h)\n",
        "        \n",
        "        # print('Attention att_:',att_.shape)\n",
        "        \n",
        "        e_ = self.concat_lin(att_).squeeze(2)\n",
        "        \n",
        "        # print('Attention e_:',e_.shape)\n",
        "        \n",
        "        alpha = self.softmax(e_)\n",
        "        context_vec = (image_features * alpha.unsqueeze(2)).sum(1)\n",
        "        \n",
        "        # print('Attention alpha: ', alpha.shape)\n",
        "        return context_vec, alpha\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, vocab_size, wordEmbeddingFilename=None, lstm_layers=1, enc_dim=256):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
        "                            num_layers = lstm_layers, batch_first = True)\n",
        "        \n",
        "        self.lstmCell = nn.LSTMCell(embed_dim+embed_dim, lstm_hidden_size)\n",
        "        \n",
        "        self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "        \n",
        "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "\n",
        "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "        \n",
        "        self.act= nn.Tanh()\n",
        "        \n",
        "        self.h = nn.Linear(embed_dim, lstm_hidden_size)\n",
        "        self.c = nn.Linear(embed_dim, lstm_hidden_size)\n",
        "\n",
        "        # self.embed = self.load_pre_trained(wordEmbeddingFilename)\n",
        "        # self.embed = nn.Embedding.from_pretrained(self.vocab_size,embed_dim,padding_idx=0)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.gate = nn.Linear(lstm_hidden_size, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.bias.data.fill_(0)\n",
        "        \n",
        "        self.out = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "\n",
        "    def load_pre_trained(self, filename):\n",
        "        # import gensim\n",
        "        # from gensim.models.wrappers import FastText\n",
        "        # model = FastText.load_fasttext_format('wiki.simple')\n",
        "        # nn.Embedding.from_pretrained()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, img_feat, captions):\n",
        "        # print('h started ...',img_feat.shape, img_feat.mean(dim=0).shape)\n",
        "        \n",
        "        # print(captions.shape)\n",
        "        \n",
        "        # h = self.act(self.h(img_feat.mean(dim=0)))\n",
        "        h = self.act(self.h(img_feat))\n",
        "        \n",
        "        # print('h computed')\n",
        "        \n",
        "        # c = self.act(self.c(img_feat.mean(dim=0)))\n",
        "        c = self.act(self.c(img_feat))\n",
        "        \n",
        "        \n",
        "        # print('hc computed',h.shape,c.shape)\n",
        "        \n",
        "        max_len = captions_preprocessing_obj.max_caption\n",
        "        embedding = self.embed(captions)\n",
        "        \n",
        "        # print('embedding generated', embedding.shape)\n",
        "        \n",
        "        out_matrix = torch.zeros(img_feat.shape[0], max_len ,self.vocab_size)\n",
        "        alpha_matrix = torch.zeros(img_feat.shape[0], max_len ,img_feat.shape[1])\n",
        "        \n",
        "        # print('Out Matrix:', out_matrix.shape)\n",
        "        # print('Alpha matrix:', alpha_matrix.shape)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            out_matrix = out_matrix.cuda()\n",
        "            alpha_matrix = alpha_matrix.cuda()\n",
        "        \n",
        "        for i in range(max_len):\n",
        "            # print('Attention',i, img_feat.shape, h.shape)\n",
        "            context, alpha = self.attention(img_feat, h)\n",
        "            \n",
        "            # print('Context:{}, Alpha:{}'.format(context.shape, alpha.shape))\n",
        "            gate_out = self.sigmoid(self.gate(h))\n",
        "            \n",
        "            # print('Gate:',gate_out.shape)\n",
        "            \n",
        "            context_gate = context * gate_out \n",
        "            in_ = torch.cat([embedding[:,i], context_gate],dim=1)\n",
        "            \n",
        "            # print('Context gate:',context_gate.shape)\n",
        "            # print('in_shape:',in_.shape)\n",
        "            \n",
        "            # in_ -> 32*512 i.e. batch_szie * lstm_hidden_size\n",
        "            # h,c -> batch_szie * lstm_hidden_size\n",
        "            \n",
        "            \n",
        "            # h started ... torch.Size([32, 300]) torch.Size([300])\n",
        "            # h computed\n",
        "            # hc computed torch.Size([32, 512]) torch.Size([32, 512])\n",
        "            # embedding generated torch.Size([32, 9, 300])\n",
        "            # Out Matrix: torch.Size([32, 9, 2603])\n",
        "            # Alpha matrix: torch.Size([32, 9, 300])\n",
        "            # Attention 0 torch.Size([32, 300]) torch.Size([32, 512])\n",
        "            # Context:torch.Size([32, 300]), Alpha:torch.Size([32, 32])\n",
        "            # Gate: torch.Size([32, 300])\n",
        "            # Context gate: torch.Size([32, 300])\n",
        "            # in_shape: torch.Size([32, 600])\n",
        "            # input has inconsistent input_size: got 600 expected 556\n",
        "            \n",
        "            \n",
        "            # Lstm done\n",
        "            # Dropout done\n",
        "            # Out: torch.Size([32, 2511])\n",
        "            # Alpha: torch.Size([32, 32])\n",
        "            # The expanded size of the tensor (256) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [32, 256].  Tensor sizes: [32, 32]\n",
        "            \n",
        "            \n",
        "            \n",
        "            h,c = self.lstmCell(in_, (h,c))\n",
        "            \n",
        "            # print('Lstm done, h:',h.shape,'c',c.shape)\n",
        "            \n",
        "            h = self.dropout(h)\n",
        "            \n",
        "            # print('Dropout done, h', h.shape)\n",
        "            \n",
        "            #h,c = self.lstm(in_, (h,c))\n",
        "            out = self.out(h)\n",
        "            \n",
        "            # print('Out:',out.shape)\n",
        "            # print('Alpha:',alpha.shape)\n",
        "            \n",
        "            out_matrix[:,i,:]=out\n",
        "            # alpha_matrix[:,i,:]=alpha\n",
        "            \n",
        "        return out_matrix\n",
        "        \n",
        "    # def forward(self, image_features, image_captions):\n",
        "    #     image_features = image_features.unsqueeze(1)\n",
        "    #     embeddings = self.dropout(self.embed(image_captions))\n",
        "    #     #print(embeddings.shape, image_features.shape)\n",
        "    #     embeddings = torch.cat((image_features, embeddings[:,:-1]), dim = 1)\n",
        "    #     #embeddings = torch.cat((image_features, embeddings), dim = 1)\n",
        "    #     hiddens, _ = self.lstm(embeddings)\n",
        "    #     outputs = self.linear(hiddens)\n",
        "        \n",
        "    #     return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1HhVbh_iwsVM"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(ImageCaptionsNet, self).__init__()              \n",
        "        self.Encoder = Encoder(embed_dim = embed_size)\n",
        "        self.Decoder = Decoder(embed_size, hidden_size, vocab_size, num_layers)    \n",
        "        \n",
        "\n",
        "    def forward(self, img_batch, cap_batch):\n",
        "        x = self.Encoder(img_batch)\n",
        "        \n",
        "        #x = x.long().numpy()\n",
        "        # print(x.shape, \"hihi\")\n",
        "\n",
        "        out = self.Decoder(x, cap_batch)\n",
        "        return out\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311,
          "referenced_widgets": [
            "4aea81f101994c8c9153cc16211e7fe8",
            "10fbb9abc20044ffb420da345c1bbe16",
            "aa2ee3ec3f954340851a6b0fafabdb2b",
            "56fd3f2664974e338fe9c875b569ceac",
            "cb13b10a880542428aafcb99245f14e0",
            "001a2531af9a442fbce65f100cac0c5e",
            "30639c5f29614621b3e77d64ce701c46",
            "efee17ebe88642249e6c63de039b4e58",
            "e776d5ae7a674dc794a273fe3ef493bf",
            "53dd9d0ee1a24e22a2950b16e2b33bd3",
            "0a4a42da5fc8495681f8e093aa0ff5b3"
          ]
        },
        "id": "g9oL9H7e_kmW",
        "outputId": "9dedad98-9148-4b7e-c56d-3e52816e2c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOCAB SIZE =  7469\n",
            "Train Dataset loaded\n",
            "Optimizer loaded\n",
            "Train Loader loaded\n"
          ]
        }
      ],
      "source": [
        "# Pratyush\n",
        "# IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "\n",
        "\n",
        "# For google colab\n",
        "# BASE_DIR = '/content/drive/MyDrive/'\n",
        "# os.chdir(os.path.join(BASE_DIR,'data','train_data_main'))\n",
        "\n",
        "\n",
        "# Prakhar\n",
        "os.chdir(os.path.join(BASE_DIR, 'data'))\n",
        "IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "# import gensim\n",
        "\n",
        "\n",
        "\n",
        "embed_size = 300\n",
        "hidden_size = 512\n",
        "num_layers = 9\n",
        "\n",
        "vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "net = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    net = net.to(torch.device(\"cuda:0\"))\n",
        "else:\n",
        "    net = net.to(torch.device(\"cpu\"))\n",
        "\n",
        "\n",
        "# Creating the Dataset\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "print(\"Train Dataset loaded\")\n",
        "# Define your hyperparameters\n",
        "NUMBER_OF_EPOCHS = 1\n",
        "LEARNING_RATE_D = 5e-2\n",
        "LEARNING_RATE_E = 5e-2\n",
        "BATCH_SIZE = 100\n",
        "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer_decoder = optim.Adam(list(net.Decoder.parameters()), lr=LEARNING_RATE_D, betas=(0.9, 0.999))\n",
        "optimizer_encoder = optim.Adam(list(net.Encoder.parameters()), lr=LEARNING_RATE_E, betas=(0.9, 0.999))\n",
        "print(\"Optimizer loaded\")\n",
        "# Creating the DataLoader for batching purposes\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
        "print(\"Train Loader loaded\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "start = time()\n",
        "loss_list = []\n",
        "\n",
        "# encoder_arch.load_state_dict(torch.load('/content/drive/My Drive/ML_Assignment4/encode-state1-scratch.pkl'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Iteration: 1, Loss: 8.91, TimeElapsed: 1.31Min\n",
            "Iteration: 2, Loss: 7.17, TimeElapsed: 1.61Min\n",
            "Iteration: 3, Loss: 13.72, TimeElapsed: 1.98Min\n",
            "Iteration: 4, Loss: 18.41, TimeElapsed: 2.35Min\n",
            "Iteration: 5, Loss: 21.06, TimeElapsed: 2.76Min\n",
            "Iteration: 6, Loss: 24.01, TimeElapsed: 3.08Min\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    iteration = 0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        net.Encoder.zero_grad()\n",
        "        net.Decoder.zero_grad()\n",
        "        optimizer_decoder.zero_grad()\n",
        "        optimizer_encoder.zero_grad()\n",
        "        image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "        #If GPU training required\n",
        "        image_batch = image_batch.float()\n",
        "        #captions_batch = captions_batch.float()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "        \n",
        "        # if (iteration == 2):\n",
        "        #     break\n",
        "        # output_captions = net(image_batch, captions_batch)\n",
        "\n",
        "        try:\n",
        "            output_captions = net(image_batch, captions_batch)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(\"---Error {}\".format(batch_idx))\n",
        "            break\n",
        "\n",
        "        #print(output_captions.shape, captions_batch.shape)\n",
        "        if not torch.cuda.is_available():\n",
        "            loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "        else:\n",
        "            loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer_encoder.step()\n",
        "        optimizer_decoder.step()\n",
        "        \n",
        "        print(\"Iteration: {}, Loss: {}, TimeElapsed: {}Min\".format(iteration+1, round(loss.item(), 2), round((time()-start)/60,2), ))\n",
        "        iteration+=1\n",
        "\n",
        "        if (iteration%5 == 0):\n",
        "            torch.save(net.state_dict(), './encode-state{}_{}-scratch.pkl'.format(str(epoch+1),str(iteration)))\n",
        "            # torch.save(decode_arch.state_dict(), './decode-state{}-scratch.pkl'.format(str(epoch+1) + str(iteration)))\n",
        "        if iteration == 50:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Um_hg6S-etbC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ImageCaptionsNet(\n",
              "  (Encoder): Encoder(\n",
              "    (inception): Inception3(\n",
              "      (Conv2d_1a_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (Conv2d_2a_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (Conv2d_2b_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (Conv2d_3b_1x1): BasicConv2d(\n",
              "        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (Conv2d_4a_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (Mixed_5b): InceptionA(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_2): BasicConv2d(\n",
              "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_5c): InceptionA(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_1): BasicConv2d(\n",
              "          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_2): BasicConv2d(\n",
              "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_5d): InceptionA(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_2): BasicConv2d(\n",
              "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6a): InceptionB(\n",
              "        (branch3x3): BasicConv2d(\n",
              "          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6b): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6c): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6d): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6e): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (AuxLogits): None\n",
              "      (Mixed_7a): InceptionD(\n",
              "        (branch3x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_3): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_4): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_7b): InceptionE(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_7c): InceptionE(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "      (fc): Linear(in_features=2048, out_features=300, bias=True)\n",
              "    )\n",
              "    (bn): BatchNorm2d(300, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU()\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (Decoder): Decoder(\n",
              "    (lstm): LSTM(300, 512, batch_first=True)\n",
              "    (lstmCell): LSTMCell(600, 512)\n",
              "    (attention): AttentionBlock(\n",
              "      (hidden_lin): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (tanh): Tanh()\n",
              "      (img_lin): Linear(in_features=300, out_features=512, bias=True)\n",
              "      (softmax): Softmax(dim=1)\n",
              "      (concat_lin): Linear(in_features=512, out_features=1, bias=True)\n",
              "    )\n",
              "    (linear): Linear(in_features=512, out_features=7469, bias=True)\n",
              "    (embed): Embedding(7469, 300)\n",
              "    (act): Tanh()\n",
              "    (h): Linear(in_features=300, out_features=512, bias=True)\n",
              "    (c): Linear(in_features=300, out_features=512, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "    (gate): Linear(in_features=512, out_features=300, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (out): Linear(in_features=512, out_features=7469, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# net.load_state_dict(torch.load('/content/drive/MyDrive/data/train_data_main/encode-state1-scratch.pkl'))\n",
        "net.load_state_dict(torch.load(os.path.join(BASE_DIR, 'data', 'encode-state1_6-scratch.pkl')))\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-T3HdZtGIcR"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def caption_image(net, image, cap_obj, max_length = 10):\n",
        "    result_cap = []\n",
        "    #print(\"initial img \", image.shape)\n",
        "    with torch.no_grad():\n",
        "        x = net.Encoder(image)\n",
        "        x = x.unsqueeze(0)\n",
        "        #x = self.Encoder(image)\n",
        "        states = None\n",
        "        \n",
        "        h = net.Decoder.act(net.Decoder.h(x))\n",
        "        c = net.Decoder.act(net.Decoder.c(x))\n",
        "\n",
        "        h = h.squeeze(0)\n",
        "        c = c.squeeze(0)\n",
        "        \n",
        "        prev_word = 'start'\n",
        "        \n",
        "        freq = {}\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            \n",
        "            # print(\"X new = \", x.shape) # 1, 1, 300\n",
        "            \n",
        "            prev_word_idx = 0\n",
        "            if prev_word in cap_obj.vocab:\n",
        "                prev_word_idx = cap_obj.vocab[prev_word]\n",
        "            \n",
        "            # print(self.Decoder.embed( torch.tensor(np.asarray(prev_word_idx))).shape )\n",
        "            \n",
        "            if torch.cuda.is_available():\n",
        "                embedding_pre_word = ((net.Decoder.embed( torch.tensor(np.asarray(prev_word_idx)).cuda() )).unsqueeze(0)).unsqueeze(0)\n",
        "            else:\n",
        "                embedding_pre_word = ((net.Decoder.embed( torch.tensor(np.asarray(prev_word_idx)) )).unsqueeze(0)).unsqueeze(0)\n",
        "            \n",
        "            # print(embedding_pre_word.shape)\n",
        "            \n",
        "            x = torch.cat([x, embedding_pre_word],dim=2)\n",
        "            \n",
        "            x = x.squeeze(0)\n",
        "            \n",
        "            # hiddens, states = self.Decoder.lstm(x, states)\n",
        "            \n",
        "            \n",
        "            # print('Before LSTMcell:',x.shape,h.shape,c.shape)\n",
        "            \n",
        "            h, s = net.Decoder.lstmCell(x, (h,c))\n",
        "            \n",
        "            # print(\"Hiddens = \", h.shape)\n",
        "            # print('States:', s.shape)\n",
        "            \n",
        "            output = net.Decoder.linear(h)\n",
        "\n",
        "            # print('Output:',output.shape)\n",
        "            # output_temp = sorted(output.squeeze(0), reverse=True)\n",
        "\n",
        "            predicted_ = (output.squeeze(0)).argsort()\n",
        "            # print(predicted_)\n",
        "            #predicted_ = np.argsort(np.max(output, axis = 0))\n",
        "\n",
        "            word_index = -1\n",
        "            while (cap_obj.index_to_word[int(predicted_[word_index])] == '[START]'):\n",
        "            #or (cap_obj.index_to_word[int(predicted_[word_index])] == '[END]'):\n",
        "                word_index-=1\n",
        "            predicted = predicted_[word_index]\n",
        "            \n",
        "            prev_word = cap_obj.index_to_word[int(predicted)]\n",
        "            \n",
        "            # print(predicted_[-3:])\n",
        "            #print(\"OT, PD\", output.shape, predicted.shape)\n",
        "\n",
        "            # 1 * 7356\n",
        "            result_cap.append(int(predicted))\n",
        "            x = net.Decoder.embed(predicted).unsqueeze(0)\n",
        "            x = x.unsqueeze(0)\n",
        "            \n",
        "            if prev_word in freq:\n",
        "                freq[prev_word] += 1\n",
        "            else:\n",
        "                freq[prev_word] = 1\n",
        "\n",
        "            if (prev_word == '[END]') or freq[prev_word] == 3:\n",
        "                break\n",
        "            \n",
        "            if len(result_cap) > 2 and result_cap[-1] == result_cap[-2] and result_cap[-3] == result_cap[-2]:\n",
        "                result_cap.pop()\n",
        "                break\n",
        "            \n",
        "    return [cap_obj.index_to_word[int(idx)] for idx in result_cap]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "MNcPbwSdGIcT"
      },
      "outputs": [],
      "source": [
        "class TestDatasetLoader(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, img_transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the test images.            \n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        \n",
        "        self.image_ids = ['test_data/test' + str(i) + '.jpg' for i in range(1, 5001)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        \n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        angle_in_degrees = 45\n",
        "\n",
        "        #output = torch.from_numpy(ndimage.rotate(alpha, angle_in_degrees, reshape=False))\n",
        "        # sample = {\n",
        "        #     'top': image,\n",
        "        #     'left': torch.from_numpy(ndimage.rotate(image, 90, reshape=False)),\n",
        "        #     'bottom': torch.from_numpy(ndimage.rotate(image, 180, reshape=False)),\n",
        "        #     'right': torch.from_numpy(ndimage.rotate(image, 270, reshape=False))\n",
        "        #     }\n",
        "        sample = {}\n",
        "        sample['image'] = image # 3* 256 * 256\n",
        "        sample['image_id'] = img_name\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "exekrzwGGIcT",
        "outputId": "8a77277b-dff5-4d5c-fb13-ed151d97badd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/data'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# a = np.zeros(1)\n",
        "# a = a.astype('int')\n",
        "# net.Decoder.embed(torch.tensor(a)).shape\n",
        "BASE_DIR\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "H0hloE5yGIcT",
        "outputId": "2a90dc80-e2f6-4f23-ea80-be9715c615c6"
      },
      "outputs": [],
      "source": [
        "# TEST_IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "# TEST_IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "# Google colab\n",
        "os.chdir(os.path.join(BASE_DIR,'data'))\n",
        "TEST_IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "test_img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) # Applied sequentially\n",
        "\n",
        "# Creating the Dataset\n",
        "test_dataset = TestDatasetLoader(TEST_IMAGE_DIR, img_transform=test_img_transform)\n",
        "\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "#output_caption = net.predict(device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "eyUMp6MLGIcU",
        "outputId": "55774478-3a6a-41bd-a414-2658cc781157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image_idx  0 :  driving south chair carrier chair led chair\n",
            "Image_idx  1 :  driving south chair carrier chair led chair\n",
            "Image_idx  2 :  driving south chair led chair led chair\n",
            "Image_idx  3 :  driving south carrier chair led chair led chair\n",
            "Image_idx  4 :  authority twenty carrier chair led chair led chair\n",
            "Image_idx  5 :  cartoon south carrier chair led chair led chair\n",
            "Image_idx  6 :  electrical south chair carrier chair led chair\n",
            "Image_idx  7 :  driving south carrier chair led chair led chair\n",
            "Image_idx  8 :  authority south carrier chair led chair led chair\n",
            "Image_idx  9 :  driving south capes chair led chair led chair\n",
            "Image_idx  10 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  11 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  12 :  driving south carrier chair led chair led chair\n",
            "Image_idx  13 :  authority south carrier chair led chair led chair\n",
            "Image_idx  14 :  driving south chair carrier chair led chair\n",
            "Image_idx  15 :  driving south chair led chair led chair\n",
            "Image_idx  16 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  17 :  driving south carrier chair led chair led chair\n",
            "Image_idx  18 :  driving south carrier chair led chair led chair\n",
            "Image_idx  19 :  electrical south chair carrier chair led chair\n",
            "Image_idx  20 :  electrical south chair carrier chair led chair\n",
            "Image_idx  21 :  driving south chair carrier chair led chair\n",
            "Image_idx  22 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  23 :  driving south carrier chair led chair led chair\n",
            "Image_idx  24 :  driving south carrier chair led chair led chair\n",
            "Image_idx  25 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  26 :  driving south chair carrier chair led chair\n",
            "Image_idx  27 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  28 :  driving south chair carrier chair led chair\n",
            "Image_idx  29 :  driving south chair carrier chair led chair\n",
            "Image_idx  30 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  31 :  electrical south capes chair led chair led chair\n",
            "Image_idx  32 :  driving south carrier chair led chair led chair\n",
            "Image_idx  33 :  driving south carrier chair led chair led chair\n",
            "Image_idx  34 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  35 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  36 :  driving south carrier chair led chair led chair\n",
            "Image_idx  37 :  cartoon south carrier chair led chair led chair\n",
            "Image_idx  38 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  39 :  driving south carrier chair led chair led chair\n",
            "Image_idx  40 :  electrical south capes chair led chair led chair\n",
            "Image_idx  41 :  driving south chair carrier chair led chair\n",
            "Image_idx  42 :  driving south carrier chair led chair led chair\n",
            "Image_idx  43 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  44 :  driving longchamp carrier chair led chair led chair\n",
            "Image_idx  45 :  driving longchamp carrier chair led chair led chair\n",
            "Image_idx  46 :  electrical south chair carrier chair led chair\n",
            "Image_idx  47 :  driving south chair led chair led chair\n",
            "Image_idx  48 :  electrical south chair carrier chair led chair\n",
            "Image_idx  49 :  sized south chair carrier chair led chair\n",
            "Image_idx  50 :  cartoon longchamp carrier chair led chair led chair\n",
            "Image_idx  51 :  driving south chair carrier chair led chair\n",
            "Image_idx  52 :  driving south carrier chair led chair led chair\n",
            "Image_idx  53 :  driving south chair carrier chair led chair\n",
            "Image_idx  54 :  driving south chair carrier chair led chair\n",
            "Image_idx  55 :  driving south carrier chair led chair led chair\n",
            "Image_idx  56 :  driving south carrier chair led chair led chair\n",
            "Image_idx  57 :  cartoon south capes chair led chair led chair\n",
            "Image_idx  58 :  driving south carrier chair led chair led chair\n",
            "Image_idx  59 :  cartoon longchamp carrier chair led chair led chair\n",
            "Image_idx  60 :  driving south carrier chair led chair led chair\n",
            "Image_idx  61 :  authority south chair carrier chair led chair\n",
            "Image_idx  62 :  driving south chair carrier chair led chair\n",
            "Image_idx  63 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  64 :  driving south chair carrier chair led chair\n",
            "Image_idx  65 :  electrical south chair carrier chair led chair\n",
            "Image_idx  66 :  cartoon twenty carrier chair led chair led chair\n",
            "Image_idx  67 :  authority south chair led chair led chair\n",
            "Image_idx  68 :  airplane south chair carrier chair led chair\n",
            "Image_idx  69 :  driving south chair carrier chair led chair\n",
            "Image_idx  70 :  authority longchamp carrier chair led chair led chair\n",
            "Image_idx  71 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  72 :  driving south carrier chair led chair led chair\n",
            "Image_idx  73 :  driving south carrier chair led chair led chair\n",
            "Image_idx  74 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  75 :  driving south carrier chair led chair led chair\n",
            "Image_idx  76 :  authority south chair carrier chair led chair\n",
            "Image_idx  77 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  78 :  driving south carrier chair led chair led chair\n",
            "Image_idx  79 :  electrical south chair carrier chair led chair\n",
            "Image_idx  80 :  driving south chair led chair led chair\n",
            "Image_idx  81 :  driving south carrier chair led chair led chair\n",
            "Image_idx  82 :  parading south chair carrier chair led chair\n",
            "Image_idx  83 :  driving south chair carrier chair led chair\n",
            "Image_idx  84 :  cartoon south capes chair led chair led chair\n",
            "Image_idx  85 :  driving south carrier chair led chair led chair\n",
            "Image_idx  86 :  driving south carrier chair led chair led chair\n",
            "Image_idx  87 :  driving south chair carrier chair led chair\n",
            "Image_idx  88 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  89 :  driving south chair carrier chair led chair\n",
            "Image_idx  90 :  driving south capes chair led chair led chair\n",
            "Image_idx  91 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  92 :  driving south carrier chair led chair led chair\n",
            "Image_idx  93 :  authority south chair carrier chair led chair\n",
            "Image_idx  94 :  driving south chair carrier chair led chair\n",
            "Image_idx  95 :  driving south chair carrier chair led chair\n",
            "Image_idx  96 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  97 :  electrical south chair carrier chair led chair\n",
            "Image_idx  98 :  driving south carrier chair led chair led chair\n",
            "Image_idx  99 :  authority south chair carrier chair led chair\n",
            "Image_idx  100 :  driving south chair carrier chair led chair\n",
            "Image_idx  101 :  driving longchamp carrier chair led chair led chair\n",
            "Image_idx  102 :  driving longchamp carrier chair led chair led chair\n",
            "Image_idx  103 :  driving south chair carrier chair led chair\n",
            "Image_idx  104 :  driving longchamp carrier chair led chair led chair\n",
            "Image_idx  105 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  106 :  authority south chair carrier chair led chair\n",
            "Image_idx  107 :  authority south chair carrier chair led chair\n",
            "Image_idx  108 :  electrical south chair carrier chair led chair\n",
            "Image_idx  109 :  cartoon south carrier chair led chair led chair\n",
            "Image_idx  110 :  driving south carrier chair led chair led chair\n",
            "Image_idx  111 :  driving south chair led chair led chair\n",
            "Image_idx  112 :  driving south chair carrier chair led chair\n",
            "Image_idx  113 :  authority south carrier chair led chair led chair\n",
            "Image_idx  114 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  115 :  driving south chair carrier chair led chair\n",
            "Image_idx  116 :  driving south chair carrier chair led chair\n",
            "Image_idx  117 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  118 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  119 :  electrical south chair carrier chair led chair\n",
            "Image_idx  120 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  121 :  cartoon longchamp carrier chair led chair led chair\n",
            "Image_idx  122 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  123 :  driving longchamp carrier chair led chair led chair\n",
            "Image_idx  124 :  driving south carrier chair led chair led chair\n",
            "Image_idx  125 :  driving south carrier chair led chair led chair\n",
            "Image_idx  126 :  authority south chair carrier chair led chair\n",
            "Image_idx  127 :  authority south chair carrier chair led chair\n",
            "Image_idx  128 :  driving south chair led chair led chair\n",
            "Image_idx  129 :  airplane south chair carrier chair led chair\n",
            "Image_idx  130 :  driving being chair led chair led chair\n",
            "Image_idx  131 :  driving south capes chair led chair led chair\n",
            "Image_idx  132 :  electrical south chair carrier chair led chair\n",
            "Image_idx  133 :  authority south chair carrier chair led chair\n",
            "Image_idx  134 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  135 :  driving south carrier chair led chair led chair\n",
            "Image_idx  136 :  driving south carrier chair led chair led chair\n",
            "Image_idx  137 :  driving south chair carrier chair led chair\n",
            "Image_idx  138 :  authority south chair carrier chair led chair\n",
            "Image_idx  139 :  driving south carrier chair led chair led chair\n",
            "Image_idx  140 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  141 :  driving south chair carrier chair led chair\n",
            "Image_idx  142 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  143 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  144 :  driving south chair carrier chair led chair\n",
            "Image_idx  145 :  driving south chair carrier chair led chair\n",
            "Image_idx  146 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  147 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  148 :  driving south carrier chair led chair led chair\n",
            "Image_idx  149 :  driving south carrier chair led chair led chair\n",
            "Image_idx  150 :  authority south chair carrier chair led chair\n",
            "Image_idx  151 :  cartoon longchamp carrier chair led chair led chair\n",
            "Image_idx  152 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  153 :  authority south chair led chair led chair\n",
            "Image_idx  154 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  155 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  156 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  157 :  cartoon south chair led chair led chair\n",
            "Image_idx  158 :  driving south carrier chair led chair led chair\n",
            "Image_idx  159 :  driving south carrier chair led chair led chair\n",
            "Image_idx  160 :  authority south chair carrier chair led chair\n",
            "Image_idx  161 :  driving south carrier chair led chair led chair\n",
            "Image_idx  162 :  driving south chair led chair led chair\n",
            "Image_idx  163 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  164 :  driving south chair carrier chair led chair\n",
            "Image_idx  165 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  166 :  authority south chair carrier chair led chair\n",
            "Image_idx  167 :  driving south carrier chair led chair led chair\n",
            "Image_idx  168 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  169 :  authority south chair carrier chair led chair\n",
            "Image_idx  170 :  driving south carrier chair led chair led chair\n",
            "Image_idx  171 :  driving south carrier chair led chair led chair\n",
            "Image_idx  172 :  driving south chair carrier chair led chair\n",
            "Image_idx  173 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  174 :  cartoon longchamp carrier chair led chair led chair\n",
            "Image_idx  175 :  driving south chair carrier chair led chair\n",
            "Image_idx  176 :  authority twenty carrier chair led chair led chair\n",
            "Image_idx  177 :  driving south carrier chair led chair led chair\n",
            "Image_idx  178 :  driving south carrier chair led chair led chair\n",
            "Image_idx  179 :  authority south carrier chair led chair led chair\n",
            "Image_idx  180 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  181 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  182 :  electrical south chair carrier chair led chair\n",
            "Image_idx  183 :  authority longchamp carrier chair led chair led chair\n",
            "Image_idx  184 :  driving south chair carrier chair led chair\n",
            "Image_idx  185 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  186 :  driving south capes chair led chair led chair\n",
            "Image_idx  187 :  driving south carrier chair led chair led chair\n",
            "Image_idx  188 :  driving south carrier chair led chair led chair\n",
            "Image_idx  189 :  driving south carrier chair led chair led chair\n",
            "Image_idx  190 :  driving south chair carrier chair led chair\n",
            "Image_idx  191 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  192 :  authority south chair carrier chair led chair\n",
            "Image_idx  193 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  194 :  cartoon south carrier chair led chair led chair\n",
            "Image_idx  195 :  electrical south capes chair led chair led chair\n",
            "Image_idx  196 :  driving south carrier chair led chair led chair\n",
            "Image_idx  197 :  electrical south carrier chair led chair led chair\n",
            "Image_idx  198 :  driving south chair carrier chair led chair\n",
            "Image_idx  199 :  driving south carrier chair led chair led chair\n",
            "Image_idx  200 :  electrical south carrier chair led chair led chair\n"
          ]
        }
      ],
      "source": [
        "pred_caps = {}\n",
        "for batch_idx, sample in enumerate(test_loader):\n",
        "        #print(, batch_idx)\n",
        "        \n",
        "        # print(sample)\n",
        "        \n",
        "        image = sample['image']\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image = image.cuda()\n",
        "        \n",
        "        image_name = sample['image_id']\n",
        "        #print(\"ld\", image.shape)\n",
        "        image = image.float()\n",
        "        #print(\"Lolxd\" , image.shape)\n",
        "        caption_pred = caption_image(net, image, captions_preprocessing_obj, max_length = 10)\n",
        "        #print(np.asarray(caption_pred).shape)\n",
        "        caption_pred = \" \".join(caption_pred)\n",
        "        cap = caption_pred.replace(\"[START]\",\"\").replace(\"[END]\",\"\")\n",
        "        print(\"Image_idx \", batch_idx,\": \", caption_pred)\n",
        "        # print(\"Image_idx \", batch_idx)\n",
        "        # print(\"Predicted\",batch_idx, pred_cap)\n",
        "\n",
        "        # print(image_name)\n",
        "\n",
        "        # print(type(caption_pred),type(cap))\n",
        "\n",
        "        pred_caps[image_name[0]] = cap\n",
        "        \n",
        "        if batch_idx == 200:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image_idx  0 :  bicyclers redheaded beverages guests\n",
        "# Image_idx  1 :  bicyclers pressure beverages guests\n",
        "# Image_idx  2 :  fedora beverages\n",
        "# Image_idx  3 :  blesses guests beverages\n",
        "# Image_idx  4 :  treads 8 beverages\n",
        "# Image_idx  5 :  torn beverages\n",
        "# Image_idx  6 :  bicyclers beverages\n",
        "# Image_idx  7 :  bicyclers guests beverages\n",
        "# Image_idx  8 :  yawing suits\n",
        "# Image_idx  9 :  game suits guests\n",
        "# Image_idx  10 :  bicyclers beverages\n",
        "# Image_idx  11 :  tubing beverages\n",
        "# Image_idx  12 :  participating beverages\n",
        "# Image_idx  13 :  drawings 8 guests\n",
        "# Image_idx  14 :  bicyclers 8 guests\n",
        "# Image_idx  15 :  blesses beverages\n",
        "# Image_idx  16 :  soaking purplish beverages\n",
        "# Image_idx  17 :  motor 8 beverages\n",
        "# Image_idx  18 :  mucky suits beverages\n",
        "# Image_idx  19 :  toes 8 beverages\n",
        "# Image_idx  20 :  blesses guests beverages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ese_EIHSGIcU"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "with open('test_text.tsv', 'w') as tsvfile:\n",
        "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "    for img in sorted(pred_caps, key=lambda x:int( (x[14:])[:len(x[14:])-4]) ):\n",
        "        writer.writerow([img,pred_caps[img]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnuctI2kGIcV",
        "outputId": "26a7af5b-816c-482b-f80a-2d7ce2b354c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 3 1 2 5 4]\n"
          ]
        }
      ],
      "source": [
        "sc = torch.tensor(1221)\n",
        "bc = (sc.detach().numpy())\n",
        "print(bc,type(bc))\n",
        "captions_preprocessing_obj.index_to_word[int(sc)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VGvIs6ZGIcV"
      },
      "outputs": [],
      "source": [
        "a = np.asarray([1, 3, 4, 1, 10, 9])\n",
        "print(a.argsort())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "q2_prakank.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "001a2531af9a442fbce65f100cac0c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a4a42da5fc8495681f8e093aa0ff5b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10fbb9abc20044ffb420da345c1bbe16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30639c5f29614621b3e77d64ce701c46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aea81f101994c8c9153cc16211e7fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aa2ee3ec3f954340851a6b0fafabdb2b",
              "IPY_MODEL_56fd3f2664974e338fe9c875b569ceac",
              "IPY_MODEL_cb13b10a880542428aafcb99245f14e0"
            ],
            "layout": "IPY_MODEL_10fbb9abc20044ffb420da345c1bbe16"
          }
        },
        "53dd9d0ee1a24e22a2950b16e2b33bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56fd3f2664974e338fe9c875b569ceac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e776d5ae7a674dc794a273fe3ef493bf",
            "max": 108949747,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_efee17ebe88642249e6c63de039b4e58",
            "value": 108949747
          }
        },
        "aa2ee3ec3f954340851a6b0fafabdb2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30639c5f29614621b3e77d64ce701c46",
            "placeholder": "​",
            "style": "IPY_MODEL_001a2531af9a442fbce65f100cac0c5e",
            "value": "100%"
          }
        },
        "cb13b10a880542428aafcb99245f14e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a4a42da5fc8495681f8e093aa0ff5b3",
            "placeholder": "​",
            "style": "IPY_MODEL_53dd9d0ee1a24e22a2950b16e2b33bd3",
            "value": " 104M/104M [00:01&lt;00:00, 75.1MB/s]"
          }
        },
        "e776d5ae7a674dc794a273fe3ef493bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efee17ebe88642249e6c63de039b4e58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
