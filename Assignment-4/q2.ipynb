{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLCSEegQF4Ny",
        "outputId": "5767021b-4134-439e-bd4e-12f8a9ee854a"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (147025888.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/fh/pldcy6r96fsbw54zstw8yy840000gn/T/ipykernel_42572/147025888.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pip install torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_K7nJ9KMGzi3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prakank/.local/lib/python3.8/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
            "  from .collection import imread_collection_wrapper\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "from collections import Counter\n",
        "from skimage import io, transform\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "from time import time\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vDdwWT_sG3JM"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        #print(\"TA RESCALE INPUT\", image.shape)\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ThMQbegvIE5x"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return torch.tensor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KbsfVXxIH59",
        "outputId": "c0e4e5f3-9c2e-4973-cf42-8c7f0417a79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device set to cpu\n"
          ]
        }
      ],
      "source": [
        "IMAGE_RESIZE = (256, 256)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
        "print(\"Current device set to {}\".format(device))\n",
        "# DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/'\n",
        "DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-vkxd13g6G7",
        "outputId": "5192d087-a091-440a-89d3-8008d2bc0381"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/prakank/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/prakank/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "phase = \"Train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vocabulary = 2511\n"
          ]
        }
      ],
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "        self.captions_dict = self.process_captions()\n",
        "        self.vocab = self.generate_vocabulary()\n",
        "    def read_raw_captions(self):\n",
        "        # Dictionary with raw captions list keyed by image ids (integers)\n",
        "        captions_dict = {}\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                image_path = DIR + img_captions[0]\n",
        "                \n",
        "                image_path = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/data/' + img_captions[0]\n",
        "                \n",
        "                if os.path.exists(image_path):\n",
        "                    captions_dict[img_captions[0]] = img_captions[1]\n",
        "                    \n",
        "                if len(captions_dict) == 5000:\n",
        "                    break\n",
        "                    \n",
        "                    \n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        # Can remove the stopwords and gibberish in the caption\n",
        "        stop_words = stopwords.words('english')\n",
        "        self.allowedLength = 7\n",
        "        punctuation = list(string.punctuation)\n",
        "\n",
        "        for key, value in raw_captions_dict.items():\n",
        "            cleaned_caption = re.sub('[^A-Za-z0-9]+', ' ', value) #Extra space removal\n",
        "            tokens = word_tokenize(cleaned_caption)\n",
        "            # cleaned_tokens = [token for token in tokens if token not in stop_words and token not in punctuation] # Remove stopwords and punctuation\n",
        "            cleaned_tokens = [token for token in tokens if token not in punctuation] # Remove stopwords and punctuation\n",
        "            \n",
        "            # cleaned_caption = \"[START] \" + \" \".join(cleaned_tokens) + \" [END]\"\n",
        "            # cleaned_caption = \" \".join(cleaned_tokens)\n",
        "            cleaned_caption = \" \".join(cleaned_tokens) + \" [END]\"\n",
        "\n",
        "            raw_captions_dict[key] = cleaned_caption        \n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = self.captions_dict\n",
        "        vocabulary = {}\n",
        "        max_caption = 0\n",
        "        idx = 1\n",
        "        index_to_word = {}\n",
        "        for key, value in captions_dict.items():\n",
        "            val = value.split()\n",
        "            max_caption = max(max_caption, len(val))\n",
        "\n",
        "            for i in val:\n",
        "                if i not in vocabulary.keys():\n",
        "                    vocabulary[i] = idx\n",
        "                    index_to_word[idx] = i\n",
        "                    idx+=1\n",
        "        self.max_caption = max_caption\n",
        "        self.max_caption = (self.allowedLength+2)\n",
        "        \n",
        "        index_to_word[0] = \"NIL\"\n",
        "        self.index_to_word = index_to_word\n",
        "        # Generate the vocabulary\n",
        "        print(\"Size of Vocabulary = {}\".format(len(vocabulary)))\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def get_captions(self, tensor_tokens):\n",
        "        caption = [self.index_to_word[int(x)] for x in tensor_tokens]\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def captions_transform(self, img_caption):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = img_caption.split(\" \")\n",
        "        \n",
        "        \n",
        "        # print(img_caption, caption)\n",
        "\n",
        "        caption_mapped = np.zeros(self.max_caption)\n",
        "        for i in range(len(caption)):\n",
        "            try: caption_mapped[i] = self.vocab[caption[i]]\n",
        "            except: print(img_caption, caption, i)\n",
        "\n",
        "        # caption_mapped = np.zeros((self.max_caption, len(self.vocab)))\n",
        "        # for i in range(len(caption)):\n",
        "        #     val = np.zeros(len(self.vocab))\n",
        "        #     val[self.vocab[caption[i]]] = 1\n",
        "        #     caption_mapped[i,:] = val \n",
        "\n",
        "        #captions_mapped = np.argmax(captions_mapped, axis = 1)\n",
        "        \n",
        "        return torch.LongTensor(caption_mapped)\n",
        "\n",
        "# Set the captions tsv file path\n",
        "\n",
        "#CAPTIONS_FILE_PATH = '/content/drive/MyDrive/data/train_text.tsv'\n",
        "# CAPTIONS_FILE_PATH = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/Train_text.tsv'\n",
        "\n",
        "\n",
        "BASE_DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'\n",
        "CAPTIONS_FILE_PATH = os.path.join(BASE_DIR, 'data', 'train_text.tsv')\n",
        "\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)\n",
        "embedding_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIOyXozxI420",
        "outputId": "212382f3-8631-4ef0-a56c-1a23abe848d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assignment-4.pdf  out1.ipynb    q2.ipynb\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/             q1.ipynb      seq2seq_attention.pdf\n",
            "data_extraction   q1_new.ipynb  starter_code.ipynb\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hr-f4Q7lE9zy"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "\n",
        "        sample = {'image': image, 'captions': captions}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vAkn6005wm8d"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  res = {}\n",
        "\n",
        "  res['image'] = [sample['image'].unsqueeze(0) for sample in batch] \n",
        "  res['image'] = torch.cat((res['image']), dim=0)\n",
        "\n",
        "  res['captions'] = [sample['captions'] for sample in batch]\n",
        "  res['captions'] = torch.nn.utils.rnn.pad_sequence(res['captions'], batch_first=True)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "95MJggLDwU9O"
      },
      "outputs": [],
      "source": [
        "#ENCODER\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, trainCNN = False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.trainCNN = trainCNN\n",
        "        self.inception = torchvision.models.inception_v3(pretrained=True, aux_logits = False)\n",
        "        \n",
        "        self.bn = nn.BatchNorm2d(embed_size, momentum=0.01)\n",
        "        \n",
        "        self.inception.fc = nn.Linear(in_features=self.inception.fc.in_features, out_features=embed_dim, bias = True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        \n",
        "        # self.inception.fc.weight.data.normal_(0., 0.2)\n",
        "        # self.inception.fc.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"Forward feeding\")\n",
        "        features = self.inception(x)\n",
        "        #print(\"Resnet module op\", x.shape)\n",
        "        for name, param in self.inception.named_parameters():\n",
        "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "               param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = self.trainCNN\n",
        "        return (self.dropout((self.relu(features))))\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, vocab_size):\n",
        "        super(AttentionBlock, self).__init__()\n",
        "        self.hidden_lin = nn.Linear(lstm_hidden_size, lstm_hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.img_lin = nn.Linear(embed_dim, lstm_hidden_size)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.concat_lin = nn.Linear(lstm_hidden_size, 1)\n",
        "    \n",
        "    def forward(self,image_features, hidden_state):\n",
        "        hidden_h = self.hidden_lin(hidden_state).unsqueeze(1)\n",
        "        #print(hidden_h.shape)\n",
        "        img_s = self.img_lin(image_features)\n",
        "        #print(img_s.shape)\n",
        "        att_ = self.tanh(img_s + hidden_h)\n",
        "        e_ = self.concat_lin(att_).squeeze(2)\n",
        "        alpha = self.softmax(e_)\n",
        "        context_vec = (image_features * alpha.unsqueeze(2)).sum(1)\n",
        "        \n",
        "        print('Attention alpha: ', alpha.shape)\n",
        "        return context_vec, alpha\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, vocab_size, wordEmbeddingFilename=None, lstm_layers=1, enc_dim=256):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = (embed_dim+enc_dim), hidden_size = lstm_hidden_size,\n",
        "                            num_layers = lstm_layers, batch_first = True)\n",
        "        \n",
        "        self.lstmCell = nn.LSTMCell(embed_dim+enc_dim, lstm_hidden_size)\n",
        "        \n",
        "        self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "        \n",
        "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "\n",
        "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "        \n",
        "        self.act= nn.Tanh()\n",
        "        \n",
        "        self.h = nn.Linear(enc_dim, lstm_hidden_size)\n",
        "        self.c = nn.Linear(enc_dim, lstm_hidden_size)\n",
        "\n",
        "        # self.embed = self.load_pre_trained(wordEmbeddingFilename)\n",
        "        # self.embed = nn.Embedding.from_pretrained(self.vocab_size,embed_dim,padding_idx=0)\n",
        "        \n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.gate = nn.Linear(lstm_hidden_size, enc_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.bias.data.fill_(0)\n",
        "        \n",
        "        self.out = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "\n",
        "    def load_pre_trained(self, filename):\n",
        "        # import gensim\n",
        "        # from gensim.models.wrappers import FastText\n",
        "        # model = FastText.load_fasttext_format('wiki.simple')\n",
        "        # nn.Embedding.from_pretrained()\n",
        "        pass\n",
        "    \n",
        "    def forward(self, img_feat, captions):\n",
        "        # print('h started ...',img_feat.shape, img_feat.mean(dim=0).shape)\n",
        "        \n",
        "        # h = self.act(self.h(img_feat.mean(dim=0)))\n",
        "        h = self.act(self.h(img_feat))\n",
        "        \n",
        "        # print('h computed')\n",
        "        \n",
        "        # c = self.act(self.c(img_feat.mean(dim=0)))\n",
        "        c = self.act(self.c(img_feat))\n",
        "        \n",
        "        \n",
        "        print('hc computed',h.shape,c.shape)\n",
        "        \n",
        "        max_len = captions_preprocessing_obj.max_caption\n",
        "        embedding = self.embed(captions)\n",
        "        \n",
        "        # print('embedding generated')\n",
        "        \n",
        "        out_matrix = torch.zeros(img_feat.shape[0], max_len ,self.vocab_size)\n",
        "        alpha_matrix = torch.zeros(img_feat.shape[0], max_len ,img_feat.shape[1])\n",
        "        \n",
        "        print('Out Matrix:', out_matrix.shape)\n",
        "        print('Alpha matrix:', alpha_matrix.shape)\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            out_matrix = out_matrix.cuda()\n",
        "            alpha_matrix = alpha_matrix.cuda()\n",
        "        \n",
        "        for i in range(max_len):\n",
        "            print('Attention',i, img_feat.shape, h.shape)\n",
        "            context, alpha = self.attention(img_feat, h)\n",
        "            \n",
        "            print('Context:{}, Alpha:{}'.format(context.shape, alpha.shape))\n",
        "            gate_out = self.sigmoid(self.gate(h))\n",
        "            \n",
        "            print('Gate:',gate_out.shape)\n",
        "            \n",
        "            context_gate = context * gate_out \n",
        "            in_ = torch.cat([embedding[:,i], context_gate],dim=1)\n",
        "            \n",
        "            print('Context gate:',context_gate.shape)\n",
        "            print('in_shape:',in_.shape) \n",
        "            \n",
        "            # in_ -> 32*512 i.e. batch_szie * lstm_hidden_size\n",
        "            # h,c -> batch_szie * lstm_hidden_size\n",
        "            \n",
        "            \n",
        "            # hc computed torch.Size([32, 512]) torch.Size([32, 512])\n",
        "            # Out Matrix: torch.Size([32, 8, 2511])\n",
        "            # Alpha matrix: torch.Size([32, 8, 256])\n",
        "            # Attention 0 torch.Size([32, 256]) torch.Size([32, 512])\n",
        "            # Context:torch.Size([32, 256]), Alpha:torch.Size([32, 32])\n",
        "            # Gate: torch.Size([32, 256])\n",
        "            # Context gate: torch.Size([32, 256])\n",
        "            # in_shape: torch.Size([32, 512])\n",
        "            # Lstm done\n",
        "            # Dropout done\n",
        "            # Out: torch.Size([32, 2511])\n",
        "            # Alpha: torch.Size([32, 32])\n",
        "            # The expanded size of the tensor (256) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [32, 256].  Tensor sizes: [32, 32]\n",
        "            \n",
        "            \n",
        "            \n",
        "            h,c = self.lstmCell(in_, (h,c))\n",
        "            \n",
        "            print('Lstm done')\n",
        "            \n",
        "            h = self.dropout(h)\n",
        "            \n",
        "            print('Dropout done')\n",
        "            \n",
        "            #h,c = self.lstm(in_, (h,c))\n",
        "            out = self.out(h)\n",
        "            \n",
        "            print('Out:',out.shape)\n",
        "            print('Alpha:',alpha.shape)\n",
        "            \n",
        "            out_matrix[:,i,:]=out\n",
        "            alpha_matrix[:,i,:]=alpha\n",
        "            \n",
        "        return out_matrix\n",
        "        \n",
        "    # def forward(self, image_features, image_captions):\n",
        "    #     image_features = image_features.unsqueeze(1)\n",
        "    #     embeddings = self.dropout(self.embed(image_captions))\n",
        "    #     #print(embeddings.shape, image_features.shape)\n",
        "    #     embeddings = torch.cat((image_features, embeddings[:,:-1]), dim = 1)\n",
        "    #     #embeddings = torch.cat((image_features, embeddings), dim = 1)\n",
        "    #     hiddens, _ = self.lstm(embeddings)\n",
        "    #     outputs = self.linear(hiddens)\n",
        "        \n",
        "    #     return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "1HhVbh_iwsVM"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(ImageCaptionsNet, self).__init__()              \n",
        "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
        "        self.Decoder = Decoder(embedding_dim, hidden_size, vocab_size, num_layers)    \n",
        "        \n",
        "\n",
        "    def forward(self, img_batch, cap_batch):\n",
        "        x = self.Encoder(img_batch)\n",
        "        #x = x.long().numpy()\n",
        "        # print(x.shape, \"hihi\")\n",
        "        out = self.Decoder(x, cap_batch)\n",
        "        return out\n",
        "\n",
        "    def caption_image(self, image, cap_obj, max_length = 10):\n",
        "        result_cap = []\n",
        "        #print(\"initial img \", image.shape)\n",
        "        with torch.no_grad():\n",
        "            x = self.Encoder(image)\n",
        "            x = x.unsqueeze(0)\n",
        "            #x = self.Encoder(image)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                \n",
        "                # print(\"X new = \", x.shape)\n",
        "                \n",
        "                hiddens, states = self.Decoder.lstm(x, states)\n",
        "                \n",
        "                # print(\"Hiddens = \", hiddens.shape)\n",
        "                \n",
        "                output = self.Decoder.linear(hiddens.squeeze(1))\n",
        "\n",
        "                # output_temp = sorted(output.squeeze(0), reverse=True)\n",
        "\n",
        "                predicted_ = (output.squeeze(0)).argsort()\n",
        "                # print(predicted_)\n",
        "                #predicted_ = np.argsort(np.max(output, axis = 0))\n",
        "\n",
        "                word_index = -1\n",
        "                while (cap_obj.index_to_word[int(word_index)] == '[START]') or (cap_obj.index_to_word[int(word_index)] == '[END]'):\n",
        "                    word_index-=1\n",
        "                predicted = predicted_[word_index]\n",
        "                \n",
        "                # print(predicted_[-3:])\n",
        "                #print(\"OT, PD\", output.shape, predicted.shape)\n",
        "\n",
        "                # 1 * 7356\n",
        "                result_cap.append(predicted)\n",
        "                x = self.Decoder.embed(predicted).unsqueeze(0)\n",
        "                x = x.unsqueeze(0)\n",
        "\n",
        "                if (cap_obj.index_to_word[predicted.item()] == '[END]'):\n",
        "                    break\n",
        "                \n",
        "                if len(result_cap) > 1 and result_cap[-1] == result_cap[-2]:\n",
        "                    result_cap.pop()\n",
        "                    break\n",
        "                \n",
        "        return [cap_obj.index_to_word[int(idx)] for idx in result_cap]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "g9oL9H7e_kmW",
        "outputId": "59b06aca-27f4-4e87-8247-4edd5af2cbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOCAB SIZE =  2511\n",
            "Train Dataset loaded\n",
            "Optimizer loaded\n",
            "Train Loader loaded\n",
            "Epoch 1\n",
            "hc computed torch.Size([32, 512]) torch.Size([32, 512])\n",
            "Out Matrix: torch.Size([32, 8, 2511])\n",
            "Alpha matrix: torch.Size([32, 8, 256])\n",
            "Attention 0 torch.Size([32, 256]) torch.Size([32, 512])\n",
            "Attention alpha:  torch.Size([32, 32])\n",
            "Context:torch.Size([32, 256]), Alpha:torch.Size([32, 32])\n",
            "Gate: torch.Size([32, 256])\n",
            "Context gate: torch.Size([32, 256])\n",
            "in_shape: torch.Size([32, 512])\n",
            "Lstm done\n",
            "Dropout done\n",
            "Out: torch.Size([32, 2511])\n",
            "Alpha: torch.Size([32, 32])\n",
            "The expanded size of the tensor (256) must match the existing size (32) at non-singleton dimension 1.  Target sizes: [32, 256].  Tensor sizes: [32, 32]\n",
            "---Error 0\n"
          ]
        }
      ],
      "source": [
        "# os.chdir('/content/drive/MyDrive/data/train_data_main/')\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/data/train_data_main/'\n",
        "\n",
        "# IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "\n",
        "os.chdir(os.path.join(BASE_DIR, 'data'))\n",
        "IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "# import gensim\n",
        "\n",
        "\n",
        "\n",
        "embed_size = 300\n",
        "hidden_size = 512\n",
        "num_layers = 9\n",
        "\n",
        "vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "net = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "#net = net.to(torch.device(\"cuda:0\"))\n",
        "net = net.to(torch.device(\"cpu\"))\n",
        "# Creating the Dataset\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "print(\"Train Dataset loaded\")\n",
        "# Define your hyperparameters\n",
        "NUMBER_OF_EPOCHS = 1\n",
        "LEARNING_RATE_D = 5e-2\n",
        "LEARNING_RATE_E = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer_decoder = optim.Adam(list(net.Decoder.parameters()), lr=LEARNING_RATE_D, betas=(0.9, 0.999))\n",
        "optimizer_encoder = optim.Adam(list(net.Encoder.parameters()), lr=LEARNING_RATE_E, betas=(0.9, 0.999))\n",
        "print(\"Optimizer loaded\")\n",
        "# Creating the DataLoader for batching purposes\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
        "print(\"Train Loader loaded\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "start = time()\n",
        "loss_list = []\n",
        "\n",
        "\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    iteration = 0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        net.Encoder.zero_grad()\n",
        "        net.Decoder.zero_grad()\n",
        "        optimizer_decoder.zero_grad()\n",
        "        optimizer_encoder.zero_grad()\n",
        "        image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "        #If GPU training required\n",
        "        image_batch = image_batch.float()\n",
        "        #captions_batch = captions_batch.float()\n",
        "        #image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "        \n",
        "        # if (iteration == 2):\n",
        "        #     break\n",
        "        \n",
        "        try:\n",
        "            output_captions = net(image_batch, captions_batch)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            print(\"---Error {}\".format(batch_idx))\n",
        "            break\n",
        "\n",
        "        #print(output_captions.shape, captions_batch.shape)\n",
        "        loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer_encoder.step()\n",
        "        optimizer_decoder.step()\n",
        "        print(\"Iteration: {}, Loss: {}, TimeElapsed: {}Min\".format(iteration+1, round(loss.item(), 2), round((time()-start)/60,2), ))\n",
        "        iteration+=1\n",
        "        \n",
        "        if (iteration == 100):\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestDatasetLoader(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, img_transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the test images.            \n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        \n",
        "        self.image_ids = ['test_data/test' + str(i) + '.jpg' for i in range(1, 5001)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        \n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        angle_in_degrees = 45\n",
        "\n",
        "        #output = torch.from_numpy(ndimage.rotate(alpha, angle_in_degrees, reshape=False))\n",
        "        # sample = {\n",
        "        #     'top': image,\n",
        "        #     'left': torch.from_numpy(ndimage.rotate(image, 90, reshape=False)),\n",
        "        #     'bottom': torch.from_numpy(ndimage.rotate(image, 180, reshape=False)),\n",
        "        #     'right': torch.from_numpy(ndimage.rotate(image, 270, reshape=False))\n",
        "        #     }\n",
        "        sample['image'] = image # 3* 256 * 256\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "\n",
        "test_img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) # Applied sequentially\n",
        "\n",
        "# Creating the Dataset\n",
        "test_dataset = TestDatasetLoader(TEST_IMAGE_DIR, img_transform=test_img_transform)\n",
        "\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "#output_caption = net.predict(device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image_idx  0 :  man white shirt stands\n",
            "Image_idx  1 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  2 :  man white shirt stands\n",
            "Image_idx  3 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  4 :  man white shirt stands\n",
            "Image_idx  5 :  man wearing [END]\n",
            "Image_idx  6 :  man white shirt stands\n",
            "Image_idx  7 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  8 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  9 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  10 :  man white shirt stands\n",
            "Image_idx  11 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  12 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  13 :  man wearing [END]\n",
            "Image_idx  14 :  man white shirt stands\n",
            "Image_idx  15 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  16 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  17 :  man wearing [END]\n",
            "Image_idx  18 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  19 :  man white shirt stands\n",
            "Image_idx  20 :  man yellow dog running grass looking around grass playing hair\n",
            "Image_idx  21 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  22 :  man white shirt stands\n",
            "Image_idx  23 :  two women sitting outside walking around grass wearing blue shirt\n",
            "Image_idx  24 :  two women sitting outside walking around grass wearing blue shirt\n",
            "Image_idx  25 :  man wearing [END]\n",
            "Image_idx  26 :  man wearing [END]\n",
            "Image_idx  27 :  man sunglasses playing smiling plays hair dressed around shoulders street\n",
            "Image_idx  28 :  man wearing [END]\n",
            "Image_idx  29 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  30 :  two women sitting outside walking around grass wearing blue shirt\n",
            "Image_idx  31 :  man wearing [END]\n",
            "Image_idx  32 :  man wearing [END]\n",
            "Image_idx  33 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  34 :  dog [END]\n",
            "Image_idx  35 :  man wearing [END]\n",
            "Image_idx  36 :  man wearing [END]\n",
            "Image_idx  37 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  38 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  39 :  man white shirt stands\n",
            "Image_idx  40 :  man wearing [END]\n",
            "Image_idx  41 :  dog [END]\n",
            "Image_idx  42 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  43 :  man white shirt stands\n",
            "Image_idx  44 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  45 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  46 :  dog [END]\n",
            "Image_idx  47 :  man wearing [END]\n",
            "Image_idx  48 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  49 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  50 :  dog running beach near along white shirt walking shirt walking\n",
            "Image_idx  51 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  52 :  dog [END]\n",
            "Image_idx  53 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  54 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  55 :  two people walking sitting outside walking around grass playing hair\n",
            "Image_idx  56 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  57 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  58 :  man white shirt stands\n",
            "Image_idx  59 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  60 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  61 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  62 :  man wearing [END]\n",
            "Image_idx  63 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  64 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  65 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  66 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  67 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  68 :  man white shirt stands\n",
            "Image_idx  69 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  70 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  71 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  72 :  man wearing [END]\n",
            "Image_idx  73 :  man wearing [END]\n",
            "Image_idx  74 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  75 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  76 :  man white shirt stands\n",
            "Image_idx  77 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  78 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  79 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  80 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  81 :  man wearing [END]\n",
            "Image_idx  82 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  83 :  man white shirt stands\n",
            "Image_idx  84 :  dog [END]\n",
            "Image_idx  85 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  86 :  band [END]\n",
            "Image_idx  87 :  man wearing [END]\n",
            "Image_idx  88 :  man wearing [END]\n",
            "Image_idx  89 :  person playing volleyball shirt walking table wearing blue shirt walking\n",
            "Image_idx  90 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  91 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  92 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  93 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  94 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  95 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  96 :  man wearing [END]\n",
            "Image_idx  97 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  98 :  man white shirt stands\n",
            "Image_idx  99 :  man wearing [END]\n",
            "Image_idx  100 :  man white shirt stands\n",
            "Image_idx  101 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  102 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  103 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  104 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  105 :  man wearing [END]\n",
            "Image_idx  106 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  107 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  108 :  dog [END]\n",
            "Image_idx  109 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  110 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  111 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  112 :  man sunglasses using [END]\n",
            "Image_idx  113 :  dog [END]\n",
            "Image_idx  114 :  man wearing [END]\n",
            "Image_idx  115 :  man wearing [END]\n",
            "Image_idx  116 :  dog [END]\n",
            "Image_idx  117 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  118 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  119 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  120 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  121 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  122 :  man wearing [END]\n",
            "Image_idx  123 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  124 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  125 :  man wearing [END]\n",
            "Image_idx  126 :  two women sitting outside walking around grass wearing blue shirt\n",
            "Image_idx  127 :  man white shirt stands\n",
            "Image_idx  128 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  129 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  130 :  man wearing [END]\n",
            "Image_idx  131 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  132 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  133 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  134 :  man white shirt stands\n",
            "Image_idx  135 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  136 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  137 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  138 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  139 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  140 :  man wearing [END]\n",
            "Image_idx  141 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  142 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  143 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  144 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  145 :  man white shirt stands\n",
            "Image_idx  146 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  147 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  148 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  149 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  150 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  151 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  152 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  153 :  man white shirt stands\n",
            "Image_idx  154 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  155 :  band [END]\n",
            "Image_idx  156 :  two women sitting outside walking around grass wearing blue shirt\n",
            "Image_idx  157 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  158 :  man wearing [END]\n",
            "Image_idx  159 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  160 :  man wearing [END]\n",
            "Image_idx  161 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  162 :  dog [END]\n",
            "Image_idx  163 :  man wearing [END]\n",
            "Image_idx  164 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  165 :  man white shirt stands\n",
            "Image_idx  166 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  167 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  168 :  man wearing [END]\n",
            "Image_idx  169 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  170 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  171 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  172 :  man wearing [END]\n",
            "Image_idx  173 :  man white shirt stands\n",
            "Image_idx  174 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  175 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  176 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  177 :  man white shirt stands\n",
            "Image_idx  178 :  man white shirt stands\n",
            "Image_idx  179 :  dog [END]\n",
            "Image_idx  180 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  181 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  182 :  man wearing [END]\n",
            "Image_idx  183 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  184 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  185 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  186 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  187 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  188 :  man wearing [END]\n",
            "Image_idx  189 :  man white shirt stands\n",
            "Image_idx  190 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  191 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  192 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  193 :  man white shirt stands\n",
            "Image_idx  194 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  195 :  dog [END]\n",
            "Image_idx  196 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  197 :  man white dog running grass looking around grass playing hair\n",
            "Image_idx  198 :  man sitting outside walking around grass wearing blue shirt walking\n",
            "Image_idx  199 :  man white shirt stands\n",
            "Image_idx  200 :  man white dog running grass looking around grass playing hair\n"
          ]
        }
      ],
      "source": [
        "pred_caps = {}\n",
        "for batch_idx, sample in enumerate(test_loader):\n",
        "        #print(, batch_idx)\n",
        "        image = sample['image']\n",
        "        #print(\"ld\", image.shape)\n",
        "        image = image.float()\n",
        "        #print(\"Lolxd\" , image.shape)\n",
        "        caption_pred = net.caption_image(image, captions_preprocessing_obj, max_length = 10)\n",
        "        #print(np.asarray(caption_pred).shape)\n",
        "        caption_pred = \" \".join(caption_pred)\n",
        "        cap = caption_pred.replace(\"[START]\",\"\").replace(\"[END]\",\"\")\n",
        "        print(\"Image_idx \", batch_idx,\": \", caption_pred)\n",
        "        #print(\"Predicted\",batch_idx, pred_cap)\n",
        "        \n",
        "        if batch_idx == 200:\n",
        "                break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'blue'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "captions_preprocessing_obj.index_to_word[19]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 3 1 2 5 4]\n"
          ]
        }
      ],
      "source": [
        "a = np.asarray([1, 3, 4, 1, 10, 9])\n",
        "print(a.argsort())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "q2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
