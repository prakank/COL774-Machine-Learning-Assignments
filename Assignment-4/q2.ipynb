{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLCSEegQF4Ny",
        "outputId": "5767021b-4134-439e-bd4e-12f8a9ee854a"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (147025888.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/fh/pldcy6r96fsbw54zstw8yy840000gn/T/ipykernel_42572/147025888.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pip install torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_K7nJ9KMGzi3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "from collections import Counter\n",
        "from skimage import io, transform\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "from time import time\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "import gensim\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vDdwWT_sG3JM"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        #print(\"TA RESCALE INPUT\", image.shape)\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ThMQbegvIE5x"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return torch.tensor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KbsfVXxIH59",
        "outputId": "c0e4e5f3-9c2e-4973-cf42-8c7f0417a79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device set to cpu\n"
          ]
        }
      ],
      "source": [
        "IMAGE_RESIZE = (256, 256)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
        "print(\"Current device set to {}\".format(device))\n",
        "DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-vkxd13g6G7",
        "outputId": "5192d087-a091-440a-89d3-8008d2bc0381"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/pratyushsaini/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/pratyushsaini/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "phase = \"Train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vocabulary = 7356\n"
          ]
        }
      ],
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "        self.captions_dict = self.process_captions()\n",
        "        self.vocab = self.generate_vocabulary()\n",
        "    def read_raw_captions(self):\n",
        "        # Dictionary with raw captions list keyed by image ids (integers)\n",
        "        captions_dict = {}\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                image_path = DIR + img_captions[0]\n",
        "                if os.path.exists(image_path):\n",
        "                    captions_dict[img_captions[0]] = img_captions[1]\n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        # Can remove the stopwords and gibberish in the caption\n",
        "        stop_words = stopwords.words('english')\n",
        "        punctuation = list(string.punctuation)\n",
        "\n",
        "        for key, value in raw_captions_dict.items():\n",
        "            cleaned_caption = re.sub('[^A-Za-z0-9]+', ' ', value) #Extra space removal\n",
        "            tokens = word_tokenize(cleaned_caption)\n",
        "            cleaned_tokens = [token for token in tokens if token not in stop_words and token not in punctuation] # Remove stopwords and punctuation\n",
        "            cleaned_caption = \"[START] \" + \" \".join(cleaned_tokens) + \" [END]\"\n",
        "            raw_captions_dict[key] = cleaned_caption        \n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = self.captions_dict\n",
        "        vocabulary = {}\n",
        "        max_caption = 0\n",
        "        idx = 1\n",
        "        index_to_word = {}\n",
        "        for key, value in captions_dict.items():\n",
        "            val = value.split()\n",
        "            max_caption = max(max_caption, len(val))\n",
        "\n",
        "            for i in val:\n",
        "                if i not in vocabulary.keys():\n",
        "                    vocabulary[i] = idx\n",
        "                    index_to_word[idx] = i\n",
        "                    idx+=1\n",
        "        self.max_caption = max_caption\n",
        "        index_to_word[0] = \"NIL\"\n",
        "        self.index_to_word = index_to_word\n",
        "        # Generate the vocabulary\n",
        "        print(\"Size of Vocabulary = {}\".format(len(vocabulary)))\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def get_captions(self, tensor_tokens):\n",
        "        caption = [self.index_to_word[int(x)] for x in tensor_tokens]\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def captions_transform(self, img_caption):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = img_caption.split(\" \")\n",
        "        \n",
        "        \n",
        "        # print(img_caption, caption)\n",
        "\n",
        "        caption_mapped = np.zeros(self.max_caption)\n",
        "        for i in range(len(caption)):\n",
        "            try: caption_mapped[i] = self.vocab[caption[i]]\n",
        "            except: print(img_caption, caption, i)\n",
        "\n",
        "        # caption_mapped = np.zeros((self.max_caption, len(self.vocab)))\n",
        "        # for i in range(len(caption)):\n",
        "        #     val = np.zeros(len(self.vocab))\n",
        "        #     val[self.vocab[caption[i]]] = 1\n",
        "        #     caption_mapped[i,:] = val \n",
        "\n",
        "        #captions_mapped = np.argmax(captions_mapped, axis = 1)\n",
        "        \n",
        "        return torch.LongTensor(caption_mapped)\n",
        "\n",
        "# Set the captions tsv file path\n",
        "\n",
        "#CAPTIONS_FILE_PATH = '/content/drive/MyDrive/data/train_text.tsv'\n",
        "CAPTIONS_FILE_PATH = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/Train_text.tsv'\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)\n",
        "embedding_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIOyXozxI420",
        "outputId": "212382f3-8631-4ef0-a56c-1a23abe848d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grishkin_Sene.pdf\n",
            "Train_text.tsv\n",
            "Vinyals_Show_and_Tell_2015_CVPR_paper.pdf\n",
            "q1.ipynb\n",
            "q1_new.ipynb\n",
            "q2.ipynb\n",
            "starter_code.ipynb\n",
            "\u001b[34mtest_data\u001b[m\u001b[m/\n",
            "test_data.zip\n",
            "\u001b[34mtrain_data\u001b[m\u001b[m/\n",
            "train_data.zip\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hr-f4Q7lE9zy"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "\n",
        "        sample = {'image': image, 'captions': captions}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vAkn6005wm8d"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  res = {}\n",
        "\n",
        "  res['image'] = [sample['image'].unsqueeze(0) for sample in batch] \n",
        "  res['image'] = torch.cat((res['image']), dim=0)\n",
        "\n",
        "  res['captions'] = [sample['captions'] for sample in batch]\n",
        "  res['captions'] = torch.nn.utils.rnn.pad_sequence(res['captions'], batch_first=True)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "95MJggLDwU9O"
      },
      "outputs": [],
      "source": [
        "#ENCODER\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, trainCNN = False):\n",
        "        super(Encoder, self).__init__()\n",
        "        # resnet50 = models.resnet50(pretrained=True, progress=True)        \n",
        "        # self.resnet50 = resnet50\n",
        "        # for param in self.resnet50.parameters():\n",
        "        #     param.requires_grad = False\n",
        "        # self.fc = nn.Linear(in_features=self.resnet50.fc.in_features, out_features=embed_dim, bias = True)\n",
        "        # layers = list(resnet50.children())[:-1]\n",
        "        # self.resnet50 = nn.Sequential(*layers)\n",
        "        # self.relu = nn.LeakyReLU()\n",
        "        # print(\"resnet50 Loaded Successfully..!\")\n",
        "        self.trainCNN = trainCNN\n",
        "        self.inception = torchvision.models.inception_v3(pretrained=True, aux_logits = False)\n",
        "        self.inception.fc = nn.Linear(in_features=self.inception.fc.in_features, out_features=embed_dim, bias = True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"Forward feeding\")\n",
        "        features = self.inception(x)\n",
        "        #print(\"Resnet module op\", x.shape)\n",
        "        for name, param in self.inception.named_parameters():\n",
        "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "               param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = self.trainCNN\n",
        "        return self.dropout(self.relu(features))\n",
        "\n",
        "        \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, vocab_size, lstm_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
        "                            num_layers = lstm_layers, batch_first = True)\n",
        "        #self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)        \n",
        "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        \n",
        "    def forward(self, image_features, image_captions):\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        embeddings = self.dropout(self.embed(image_captions))\n",
        "        #print(embeddings.shape, image_features.shape)\n",
        "        embeddings = torch.cat((image_features, embeddings[:,:-1]), dim = 1)\n",
        "        #embeddings = torch.cat((image_features, embeddings), dim = 1)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1HhVbh_iwsVM"
      },
      "outputs": [],
      "source": [
        "units = 512\n",
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(ImageCaptionsNet, self).__init__()              \n",
        "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
        "        self.Decoder = Decoder(embedding_dim, hidden_size, vocab_size, num_layers)    \n",
        "        \n",
        "\n",
        "    def forward(self, img_batch, cap_batch):\n",
        "        x = self.Encoder(img_batch)\n",
        "        #x = x.long().numpy()\n",
        "        #print(x.shape, \"hihi\")\n",
        "        out = self.Decoder(x, cap_batch)\n",
        "        return out\n",
        "    def caption_image(self, image, cap_obj, max_length = 10):\n",
        "        result_cap = []\n",
        "        #print(\"initial img \", image.shape)\n",
        "        with torch.no_grad():\n",
        "            x = self.Encoder(image)\n",
        "            print(x.shape)\n",
        "            x = x.unsqueeze(0)\n",
        "            #x = self.Encoder(image)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                #print(\"X new = \", x.shape)\n",
        "                hiddens, states = self.Decoder.lstm(x, states)\n",
        "                #print(\"Hiddens = \", hiddens.shape)\n",
        "                output = self.Decoder.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "                #predicted = output.argmax(3)\n",
        "                #print(\"OT, PD\", output.shape, predicted.shape)\n",
        "                result_cap.append(predicted.item())\n",
        "                x = self.Decoder.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                if (cap_obj.index_to_word[predicted.item()] == '[END]'):\n",
        "                    break\n",
        "        return [cap_obj.index_to_word[int(idx)] for idx in result_cap]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "g9oL9H7e_kmW",
        "outputId": "59b06aca-27f4-4e87-8247-4edd5af2cbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOCAB SIZE =  7356\n",
            "Train Dataset loaded\n",
            "Optimizer loaded\n",
            "Train Loader loaded\n",
            "Epoch 1\n",
            "torch.Size([8, 256]) hihi\n",
            "Iteration: 1, Loss: 8.93, TimeElapsed: 0.11Min\n",
            "torch.Size([8, 256]) hihi\n",
            "Iteration: 2, Loss: 8.93, TimeElapsed: 0.15Min\n"
          ]
        }
      ],
      "source": [
        "# os.chdir('/content/drive/MyDrive/data/train_data_main/')\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/data/train_data_main/'\n",
        "\n",
        "IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "num_layers = 1\n",
        "vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "net = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "#net = net.to(torch.device(\"cuda:0\"))\n",
        "net = net.to(torch.device(\"cpu\"))\n",
        "# Creating the Dataset\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "print(\"Train Dataset loaded\")\n",
        "# Define your hyperparameters\n",
        "NUMBER_OF_EPOCHS = 1\n",
        "LEARNING_RATE = 1e-1\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
        "print\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.SGD(list(net.Decoder.parameters()) + list(net.Encoder.parameters()), lr=LEARNING_RATE)\n",
        "print(\"Optimizer loaded\")\n",
        "# Creating the DataLoader for batching purposes\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
        "print(\"Train Loader loaded\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "start = time()\n",
        "loss_list = []\n",
        "import os\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    iteration = 0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        net.Encoder.zero_grad()\n",
        "        net.Decoder.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "        image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "        #If GPU training required\n",
        "        image_batch = image_batch.float()\n",
        "        #captions_batch = captions_batch.float()\n",
        "        #image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "        try:\n",
        "            output_captions = net(image_batch, captions_batch)\n",
        "        except:\n",
        "            print(\"---Error {}\".format(batch_idx))\n",
        "            continue\n",
        "        #print(output_captions.shape, captions_batch.shape)\n",
        "        loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(\"Iteration: {}, Loss: {}, TimeElapsed: {}Min\".format(iteration+1, round(loss.item(), 2), round((time()-start)/60,2), ))\n",
        "        iteration+=1\n",
        "        if (iteration > 1):\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestDatasetLoader(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, img_transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the test images.            \n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        \n",
        "        self.image_ids = ['test_data/test' + str(i) + '.jpg' for i in range(1, 5001)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        \n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        angle_in_degrees = 45\n",
        "\n",
        "        #output = torch.from_numpy(ndimage.rotate(alpha, angle_in_degrees, reshape=False))\n",
        "        # sample = {\n",
        "        #     'top': image,\n",
        "        #     'left': torch.from_numpy(ndimage.rotate(image, 90, reshape=False)),\n",
        "        #     'bottom': torch.from_numpy(ndimage.rotate(image, 180, reshape=False)),\n",
        "        #     'right': torch.from_numpy(ndimage.rotate(image, 270, reshape=False))\n",
        "        #     }\n",
        "        sample['image'] = image # 3* 256 * 256\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "\n",
        "test_img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) # Applied sequentially\n",
        "\n",
        "# Creating the Dataset\n",
        "test_dataset = TestDatasetLoader(TEST_IMAGE_DIR, img_transform=test_img_transform)\n",
        "\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "#output_caption = net.predict(device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial img  torch.Size([1, 3, 256, 256])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  0 :  inter swimsuit woamn house perched enact trunk vocalist crutches guy\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  1 :  barreled cleavage korean rope cleveland jog wrestling quire dandelions lands\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  2 :  tunnel surgeon arms dam cafeteria seats walmart cup kisses tub\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  3 :  strong crouched lathering gay avoid leaves jar classic takeout uniform\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  4 :  fake leaves surrounded peaceful antiquated davidson suburbs surfing peron peron\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  5 :  snowsuit fluorescent struggles shapes rainy checked travel opening senior bmx\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  6 :  scored lane dispensing bouquets apparel overalls eager jewelry apron barmaid\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  7 :  jomps brings brings fowl playes motorbike playes avoid playes loving\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  8 :  drinking eat photographers fork classmates amazingly straps racers anchored grading\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  9 :  music bears example garden twisting eighteen doctor handrail snowsuit skimpy\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  10 :  twp weapon striking bmx house showered bites emergency discovers glove\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  11 :  hatted english jewelry apron barmaid scale frightened innertube surrounded basking\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  12 :  cafeteria seats lady bakery anchored wide pinning eyes fabric backflips\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  13 :  snowboard clash sewing bounty lands lands grande ballet tub depart\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  14 :  faces grinding glossy hobo desk portrayed lie coaches science competes\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  15 :  playes pictoral lane outfits treated crutch reflectors spend fabric shrine\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  16 :  playes pictoral lane outfits silhouetted golf gestures patrons sleepy couples\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  17 :  hide playpen demonstrating sparkler possibly ilks delicate seats rainstorm fuel\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  18 :  davidson lock satisfy downing house showered 36 collar branches rollskating\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  19 :  unlocking tussle victory gray jacks mows laundromat clarinets north spotted\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  20 :  surrounded peaceful lane rapids huddling physical deleon slopes rainy donned\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  21 :  postman disgruntled dandelions stripy undershirt tag golf gothically meat dogs\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  22 :  scored nesquik surgeons mall barrels pavement peeking straightening pontoon backstage\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  23 :  playes pictoral lane outfits treated crutch reflectors spend fabric shrine\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  24 :  gnawing loving telephot terrier east martial sands telephot terrier wildebeest\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  25 :  meditating tuxedos chasers guns apron slavic faces presents fighters strange\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  26 :  playes pictoral lane outfits silhouetted golf gestures patrons sleepy couples\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  27 :  hindu graze row tempe overhang triangle lays closing architecture smu\n",
            "initial img  torch.Size([1, 3, 256, 256])\n",
            "torch.Size([1, 256])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "X new =  torch.Size([1, 1, 256])\n",
            "OT, PD torch.Size([1, 7356]) torch.Size([1])\n",
            "Image_idx  28 :  playes pictoral lane outfits treated crutch reflectors spend fabric shrine\n",
            "initial img  torch.Size([1, 3, 256, 256])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/fh/pldcy6r96fsbw54zstw8yy840000gn/T/ipykernel_83767/2776171861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#print(\"Lolxd\" , image.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcaption_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaption_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions_preprocessing_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#print(np.asarray(caption_pred).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mcaption_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/fh/pldcy6r96fsbw54zstw8yy840000gn/T/ipykernel_83767/2873099368.py\u001b[0m in \u001b[0;36mcaption_image\u001b[0;34m(self, image, cap_obj, max_length)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"initial img \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/var/folders/fh/pldcy6r96fsbw54zstw8yy840000gn/T/ipykernel_83767/3834609202.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#print(\"Forward feeding\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(\"Resnet module op\", x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInceptionOutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0maux_defined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maux_logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;31m# N x 768 x 17 x 17\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMixed_6c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mbranch7x7dbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch7x7dbl_5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbranch7x7dbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0mbranch_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torchvision/models/inception.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "pred_caps = {}\n",
        "for batch_idx, sample in enumerate(test_loader):\n",
        "        #print(, batch_idx)\n",
        "        image = sample['image']\n",
        "        #print(\"ld\", image.shape)\n",
        "        image = image.float()\n",
        "        #print(\"Lolxd\" , image.shape)\n",
        "        caption_pred = net.caption_image(image, captions_preprocessing_obj, max_length = 10)\n",
        "        #print(np.asarray(caption_pred).shape)\n",
        "        caption_pred = \" \".join(caption_pred)\n",
        "        cap = caption_pred.replace(\"[START]\",\"\").replace(\"[END]\",\"\")\n",
        "        print(\"Image_idx \", batch_idx,\": \", caption_pred)\n",
        "        #print(\"Predicted\",batch_idx, pred_cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "q2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
