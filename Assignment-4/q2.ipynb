{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLCSEegQF4Ny",
        "outputId": "5767021b-4134-439e-bd4e-12f8a9ee854a"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (147025888.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/fh/pldcy6r96fsbw54zstw8yy840000gn/T/ipykernel_42572/147025888.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    pip install torch\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_K7nJ9KMGzi3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "from collections import Counter\n",
        "from skimage import io, transform\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "from time import time\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "import gensim\n",
        "import nltk\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vDdwWT_sG3JM"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        #print(\"TA RESCALE INPUT\", image.shape)\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ThMQbegvIE5x"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return torch.tensor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KbsfVXxIH59",
        "outputId": "c0e4e5f3-9c2e-4973-cf42-8c7f0417a79e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device set to cpu\n"
          ]
        }
      ],
      "source": [
        "IMAGE_RESIZE = (256, 256)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
        "print(\"Current device set to {}\".format(device))\n",
        "DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-vkxd13g6G7",
        "outputId": "5192d087-a091-440a-89d3-8008d2bc0381"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/pratyushsaini/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/pratyushsaini/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "phase = \"Train\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vocabulary = 7356\n"
          ]
        }
      ],
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "        self.captions_dict = self.process_captions()\n",
        "        self.vocab = self.generate_vocabulary()\n",
        "    def read_raw_captions(self):\n",
        "        # Dictionary with raw captions list keyed by image ids (integers)\n",
        "        captions_dict = {}\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                image_path = DIR + img_captions[0]\n",
        "                if os.path.exists(image_path):\n",
        "                    captions_dict[img_captions[0]] = img_captions[1]\n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        # Can remove the stopwords and gibberish in the caption\n",
        "        stop_words = stopwords.words('english')\n",
        "        punctuation = list(string.punctuation)\n",
        "\n",
        "        for key, value in raw_captions_dict.items():\n",
        "            cleaned_caption = re.sub('[^A-Za-z0-9]+', ' ', value) #Extra space removal\n",
        "            tokens = word_tokenize(cleaned_caption)\n",
        "            cleaned_tokens = [token for token in tokens if token not in stop_words and token not in punctuation] # Remove stopwords and punctuation\n",
        "            cleaned_caption = \"[START] \" + \" \".join(cleaned_tokens) + \" [END]\"\n",
        "            #cleaned_caption = \" \".join(cleaned_tokens) + \" [END]\"\n",
        "            raw_captions_dict[key] = cleaned_caption        \n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = self.captions_dict\n",
        "        vocabulary = {}\n",
        "        max_caption = 0\n",
        "        idx = 1\n",
        "        index_to_word = {}\n",
        "        for key, value in captions_dict.items():\n",
        "            val = value.split()\n",
        "            max_caption = max(max_caption, len(val))\n",
        "\n",
        "            for i in val:\n",
        "                if i not in vocabulary.keys():\n",
        "                    vocabulary[i] = idx\n",
        "                    index_to_word[idx] = i\n",
        "                    idx+=1\n",
        "        self.max_caption = max_caption\n",
        "        index_to_word[0] = \"NIL\"\n",
        "        self.index_to_word = index_to_word\n",
        "        # Generate the vocabulary\n",
        "        print(\"Size of Vocabulary = {}\".format(len(vocabulary)))\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def get_captions(self, tensor_tokens):\n",
        "        caption = [self.index_to_word[int(x)] for x in tensor_tokens]\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def captions_transform(self, img_caption):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = img_caption.split(\" \")\n",
        "        \n",
        "        \n",
        "        # print(img_caption, caption)\n",
        "\n",
        "        caption_mapped = np.zeros(self.max_caption)\n",
        "        for i in range(len(caption)):\n",
        "            try: caption_mapped[i] = self.vocab[caption[i]]\n",
        "            except: print(img_caption, caption, i)\n",
        "\n",
        "        # caption_mapped = np.zeros((self.max_caption, len(self.vocab)))\n",
        "        # for i in range(len(caption)):\n",
        "        #     val = np.zeros(len(self.vocab))\n",
        "        #     val[self.vocab[caption[i]]] = 1\n",
        "        #     caption_mapped[i,:] = val \n",
        "\n",
        "        #captions_mapped = np.argmax(captions_mapped, axis = 1)\n",
        "        \n",
        "        return torch.LongTensor(caption_mapped)\n",
        "\n",
        "# Set the captions tsv file path\n",
        "\n",
        "#CAPTIONS_FILE_PATH = '/content/drive/MyDrive/data/train_text.tsv'\n",
        "CAPTIONS_FILE_PATH = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/Train_text.tsv'\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)\n",
        "embedding_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIOyXozxI420",
        "outputId": "212382f3-8631-4ef0-a56c-1a23abe848d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Grishkin_Sene.pdf\n",
            "Train_text.tsv\n",
            "Vinyals_Show_and_Tell_2015_CVPR_paper.pdf\n",
            "q1.ipynb\n",
            "q1_new.ipynb\n",
            "q2.ipynb\n",
            "starter_code.ipynb\n",
            "\u001b[34mtest_data\u001b[m\u001b[m/\n",
            "test_data.zip\n",
            "\u001b[34mtrain_data\u001b[m\u001b[m/\n",
            "train_data.zip\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hr-f4Q7lE9zy"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "\n",
        "        sample = {'image': image, 'captions': captions}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vAkn6005wm8d"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  res = {}\n",
        "\n",
        "  res['image'] = [sample['image'].unsqueeze(0) for sample in batch] \n",
        "  res['image'] = torch.cat((res['image']), dim=0)\n",
        "\n",
        "  res['captions'] = [sample['captions'] for sample in batch]\n",
        "  res['captions'] = torch.nn.utils.rnn.pad_sequence(res['captions'], batch_first=True)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "95MJggLDwU9O"
      },
      "outputs": [],
      "source": [
        "#ENCODER\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim, trainCNN = False):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.trainCNN = trainCNN\n",
        "        self.inception = torchvision.models.inception_v3(pretrained=True, aux_logits = False)\n",
        "        self.inception.fc = nn.Linear(in_features=self.inception.fc.in_features, out_features=embed_dim, bias = True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # self.inception.fc.weight.data.normal_(0., 0.2)\n",
        "        # self.inception.fc.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(\"Forward feeding\")\n",
        "        features = self.inception(x)\n",
        "        #print(\"Resnet module op\", x.shape)\n",
        "        for name, param in self.inception.named_parameters():\n",
        "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "               param.requires_grad = True\n",
        "            else:\n",
        "                param.requires_grad = self.trainCNN\n",
        "        return self.dropout(self.relu(features))\n",
        "\n",
        "        \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, vocab_size, lstm_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "        print(\"VOCAB SIZE = \", self.vocab_size)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
        "                            num_layers = lstm_layers, batch_first = True)\n",
        "        #self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)        \n",
        "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.linear.bias.data.fill_(0)\n",
        "\n",
        "        \n",
        "    def forward(self, image_features, image_captions):\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        embeddings = self.dropout(self.embed(image_captions))\n",
        "        #print(embeddings.shape, image_features.shape)\n",
        "        embeddings = torch.cat((image_features, embeddings[:,:-1]), dim = 1)\n",
        "        #embeddings = torch.cat((image_features, embeddings), dim = 1)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "1HhVbh_iwsVM"
      },
      "outputs": [],
      "source": [
        "units = 512\n",
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(ImageCaptionsNet, self).__init__()              \n",
        "        self.Encoder = Encoder(embed_dim = embedding_dim)\n",
        "        self.Decoder = Decoder(embedding_dim, hidden_size, vocab_size, num_layers)    \n",
        "        \n",
        "\n",
        "    def forward(self, img_batch, cap_batch):\n",
        "        x = self.Encoder(img_batch)\n",
        "        #x = x.long().numpy()\n",
        "        #print(x.shape, \"hihi\")\n",
        "        out = self.Decoder(x, cap_batch)\n",
        "        return out\n",
        "    def caption_image(self, image, cap_obj, max_length = 10):\n",
        "        result_cap = []\n",
        "        #print(\"initial img \", image.shape)\n",
        "        with torch.no_grad():\n",
        "            x = self.Encoder(image)\n",
        "            x = x.unsqueeze(0)\n",
        "            #x = self.Encoder(image)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                #print(\"X new = \", x.shape)\n",
        "                hiddens, states = self.Decoder.lstm(x, states)\n",
        "                #print(\"Hiddens = \", hiddens.shape)\n",
        "                output = self.Decoder.linear(hiddens.squeeze(1))\n",
        "\n",
        "                output_temp = sorted(output.squeeze(0), reverse=True)\n",
        "\n",
        "                predicted_ = (output.squeeze(0)).argsort()\n",
        "                print(predicted_)\n",
        "                #predicted_ = np.argsort(np.max(output, axis = 0))\n",
        "\n",
        "                predicted = output.argmax(1)\n",
        "                print(predicted_[-3:])\n",
        "                #print(\"OT, PD\", output.shape, predicted.shape)\n",
        "\n",
        "                # 1 * 7356\n",
        "                result_cap.append(predicted)\n",
        "                x = self.Decoder.embed(predicted).unsqueeze(1)\n",
        "\n",
        "                if (cap_obj.index_to_word[predicted.item()] == '[END]'):\n",
        "                    break\n",
        "        return [cap_obj.index_to_word[int(idx)] for idx in result_cap]\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "g9oL9H7e_kmW",
        "outputId": "59b06aca-27f4-4e87-8247-4edd5af2cbcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOCAB SIZE =  7356\n",
            "Train Dataset loaded\n",
            "Optimizer loaded\n",
            "Train Loader loaded\n",
            "Epoch 1\n",
            "Iteration: 1, Loss: 8.9, TimeElapsed: 0.44Min\n",
            "Iteration: 2, Loss: 9.01, TimeElapsed: 0.57Min\n"
          ]
        }
      ],
      "source": [
        "# os.chdir('/content/drive/MyDrive/data/train_data_main/')\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/data/train_data_main/'\n",
        "\n",
        "IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "num_layers = 4\n",
        "vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "net = ImageCaptionsNet(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "#net = net.to(torch.device(\"cuda:0\"))\n",
        "net = net.to(torch.device(\"cpu\"))\n",
        "# Creating the Dataset\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "print(\"Train Dataset loaded\")\n",
        "# Define your hyperparameters\n",
        "NUMBER_OF_EPOCHS = 1\n",
        "LEARNING_RATE_D = 5e-2\n",
        "LEARNING_RATE_E = 5e-2\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer_decoder = optim.Adam(list(net.Decoder.parameters()), lr=LEARNING_RATE_D, betas=(0.9, 0.999))\n",
        "optimizer_encoder = optim.Adam(list(net.Encoder.parameters()), lr=LEARNING_RATE_E, betas=(0.9, 0.999))\n",
        "print(\"Optimizer loaded\")\n",
        "# Creating the DataLoader for batching purposes\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
        "print(\"Train Loader loaded\")\n",
        "torch.backends.cudnn.benchmark = True\n",
        "start = time()\n",
        "loss_list = []\n",
        "import os\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    iteration = 0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        net.Encoder.zero_grad()\n",
        "        net.Decoder.zero_grad()\n",
        "        optimizer_decoder.zero_grad()\n",
        "        optimizer_encoder.zero_grad()\n",
        "        image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "        #If GPU training required\n",
        "        image_batch = image_batch.float()\n",
        "        #captions_batch = captions_batch.float()\n",
        "        #image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "        try:\n",
        "            output_captions = net(image_batch, captions_batch)\n",
        "        except:\n",
        "            print(\"---Error {}\".format(batch_idx))\n",
        "            continue\n",
        "        #print(output_captions.shape, captions_batch.shape)\n",
        "        loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "        optimizer_encoder.step()\n",
        "        optimizer_decoder.step()\n",
        "        print(\"Iteration: {}, Loss: {}, TimeElapsed: {}Min\".format(iteration+1, round(loss.item(), 2), round((time()-start)/60,2), ))\n",
        "        iteration+=1\n",
        "        if (iteration > 1):\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TestDatasetLoader(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, img_transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the test images.            \n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        \n",
        "        self.image_ids = ['test_data/test' + str(i) + '.jpg' for i in range(1, 5001)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        \n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        angle_in_degrees = 45\n",
        "\n",
        "        #output = torch.from_numpy(ndimage.rotate(alpha, angle_in_degrees, reshape=False))\n",
        "        # sample = {\n",
        "        #     'top': image,\n",
        "        #     'left': torch.from_numpy(ndimage.rotate(image, 90, reshape=False)),\n",
        "        #     'bottom': torch.from_numpy(ndimage.rotate(image, 180, reshape=False)),\n",
        "        #     'right': torch.from_numpy(ndimage.rotate(image, 270, reshape=False))\n",
        "        #     }\n",
        "        sample['image'] = image # 3* 256 * 256\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "TEST_IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "\n",
        "test_img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) # Applied sequentially\n",
        "\n",
        "# Creating the Dataset\n",
        "test_dataset = TestDatasetLoader(TEST_IMAGE_DIR, img_transform=test_img_transform)\n",
        "\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "#output_caption = net.predict(device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2902, 2780, 5957,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "tensor([5395,  276, 1338,  ...,    1,  137,   43])\n",
            "tensor([  1, 137,  43])\n",
            "tensor([5395, 1338,  276,  ...,  137,    1,   43])\n",
            "tensor([137,   1,  43])\n",
            "tensor([ 276, 5957, 5395,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "tensor([ 276, 5957, 5395,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "tensor([5957,  276, 5395,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "tensor([ 276, 5957, 5395,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "tensor([5957,  276, 5395,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "tensor([ 276, 5957, 5395,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "tensor([5957,  276, 5395,  ...,  267,  137,   43])\n",
            "tensor([267, 137,  43])\n",
            "Image_idx  0 :  three three three three three three three three three three\n"
          ]
        }
      ],
      "source": [
        "pred_caps = {}\n",
        "for batch_idx, sample in enumerate(test_loader):\n",
        "        #print(, batch_idx)\n",
        "        image = sample['image']\n",
        "        #print(\"ld\", image.shape)\n",
        "        image = image.float()\n",
        "        #print(\"Lolxd\" , image.shape)\n",
        "        caption_pred = net.caption_image(image, captions_preprocessing_obj, max_length = 10)\n",
        "        #print(np.asarray(caption_pred).shape)\n",
        "        caption_pred = \" \".join(caption_pred)\n",
        "        cap = caption_pred.replace(\"[START]\",\"\").replace(\"[END]\",\"\")\n",
        "        print(\"Image_idx \", batch_idx,\": \", caption_pred)\n",
        "        #print(\"Predicted\",batch_idx, pred_cap)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'frolicks'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "captions_preprocessing_obj.index_to_word[276]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "q2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
