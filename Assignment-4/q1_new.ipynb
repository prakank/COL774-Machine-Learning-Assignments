{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLCSEegQF4Ny",
        "outputId": "ec9c5f6d-45b8-48e0-e2be-f29148161176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K7nJ9KMGzi3",
        "outputId": "4b63c7a6-1adc-4259-89bd-f89ae78799df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prakank/.local/lib/python3.8/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
            "  from .collection import imread_collection_wrapper\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/prakank/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/prakank/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "from collections import Counter\n",
        "from skimage import io, transform\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "from time import time\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from scipy import ndimage\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vDdwWT_sG3JM"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        #print(\"TA RESCALE INPUT\", image.shape)\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ThMQbegvIE5x"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return torch.tensor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KbsfVXxIH59",
        "outputId": "5b3b1af7-9841-4cd0-b3b7-c461fb5422a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device set to cpu\n"
          ]
        }
      ],
      "source": [
        "IMAGE_RESIZE = (256, 256)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
        "print(\"Current device set to {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLQMHTkKIIn6",
        "outputId": "0291da09-3796-4b47-a417-e5a68e61c68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vocabulary = 2512\n"
          ]
        }
      ],
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "        self.captions_dict = self.process_captions()\n",
        "        self.vocab = self.generate_vocabulary()\n",
        "    def read_raw_captions(self):\n",
        "        # Dictionary with raw captions list keyed by image ids (integers)\n",
        "        captions_dict = {}\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                image_path = '/content/drive/MyDrive/data/train_data_main/' + img_captions[0]\n",
        "                \n",
        "                image_path = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/data/' + img_captions[0]\n",
        "                \n",
        "                if os.path.exists(image_path):\n",
        "                    captions_dict[img_captions[0]] = img_captions[1]\n",
        "                \n",
        "                if len(captions_dict) == 5000:\n",
        "                    break\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        # Can remove the stopwords and gibberish in the caption\n",
        "        stop_words = stopwords.words('english')\n",
        "        self.allowedLength = 7\n",
        "        punctuation = list(string.punctuation)\n",
        "\n",
        "        for key, value in raw_captions_dict.items():\n",
        "            cleaned_caption = re.sub('[^A-Za-z0-9]+', ' ', value) #Extra space removal\n",
        "            tokens = word_tokenize(cleaned_caption)\n",
        "            cleaned_tokens = [token for token in tokens if token not in stop_words and token not in punctuation] # Remove stopwords and punctuation\n",
        "            cleaned_caption = \"[START] \" + \" \".join(cleaned_tokens[:self.allowedLength]) + \" [END]\"\n",
        "            raw_captions_dict[key] = cleaned_caption        \n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = self.captions_dict\n",
        "        vocabulary = {}\n",
        "        max_caption = 0\n",
        "        idx = 1\n",
        "        index_to_word = {}\n",
        "        for key, value in captions_dict.items():\n",
        "            val = value.split()\n",
        "            max_caption = max(max_caption, len(val))\n",
        "\n",
        "            for i in val:\n",
        "                if i not in vocabulary.keys():\n",
        "                    vocabulary[i] = idx\n",
        "                    index_to_word[idx] = i\n",
        "                    idx+=1\n",
        "\n",
        "        self.max_caption = max_caption\n",
        "        self.max_caption = (self.allowedLength+2)\n",
        "\n",
        "        index_to_word[0] = \"NIL\"\n",
        "\n",
        "        self.index_to_word = index_to_word\n",
        "        \n",
        "        # Generate the vocabulary\n",
        "        print(\"Size of Vocabulary = {}\".format(len(vocabulary)))\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def get_captions(self, tensor_tokens):\n",
        "        caption = [self.index_to_word[int(x)] for x in tensor_tokens]\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def captions_transform(self, img_caption):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = img_caption.split(\" \")\n",
        "        \n",
        "        \n",
        "        # print(img_caption, caption)\n",
        "\n",
        "        caption_mapped = np.zeros(self.max_caption)\n",
        "        for i in range(len(caption)):\n",
        "            try: caption_mapped[i] = self.vocab[caption[i]]\n",
        "            except: print(img_caption, caption, i)\n",
        "\n",
        "        # caption_mapped = np.zeros((self.max_caption, len(self.vocab)))\n",
        "        # for i in range(len(caption)):\n",
        "        #     val = np.zeros(len(self.vocab))\n",
        "        #     val[self.vocab[caption[i]]] = 1\n",
        "        #     caption_mapped[i,:] = val \n",
        "\n",
        "        #captions_mapped = np.argmax(captions_mapped, axis = 1)\n",
        "        \n",
        "        return torch.LongTensor(caption_mapped)\n",
        "\n",
        "# Set the captions tsv file path\n",
        "\n",
        "# CAPTIONS_FILE_PATH = '/content/drive/MyDrive/data/train_text.tsv'\n",
        "#CAPTIONS_FILE_PATH = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/Train_text.tsv'\n",
        "\n",
        "BASE_DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'\n",
        "CAPTIONS_FILE_PATH = os.path.join(BASE_DIR, 'data', 'train_text.tsv')\n",
        "\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BZpbuRPXbis",
        "outputId": "374961a0-d26e-4c1e-f6ac-997abfc47af1"
      },
      "outputs": [],
      "source": [
        "# print(captions_preprocessing_obj.index_to_word)\n",
        "# print(captions_preprocessing_obj.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTjaF7F43tkc",
        "outputId": "78cc7b1b-7b94-461d-f953-bb9d00265b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2513\n"
          ]
        }
      ],
      "source": [
        "print(len(captions_preprocessing_obj.index_to_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSGE4kORcRTm"
      },
      "source": [
        "## DataSet Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hr-f4Q7lE9zy"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "\n",
        "        sample = {'image': image, 'captions': captions}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vAkn6005wm8d"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  res = {}\n",
        "#   print('Initial shape: ',np.asarray(batch).shape)\n",
        "\n",
        "  res['image'] = [sample['image'].unsqueeze(0) for sample in batch] \n",
        "\n",
        "#   print('Res image1:',(res['image']))\n",
        "\n",
        "  res['image'] = torch.cat((res['image']), dim=0)\n",
        "  \n",
        "#   print('Res image2:',res['image'].shape)\n",
        "\n",
        "  res['captions'] = [sample['captions'] for sample in batch]\n",
        "  \n",
        "#   print('Res caption1:',res['captions'].shape)\n",
        "\n",
        "  res['captions'] = torch.nn.utils.rnn.pad_sequence(res['captions'], batch_first=True)\n",
        "\n",
        "#   print(res)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nBo0O182cRTn"
      },
      "outputs": [],
      "source": [
        "class TestDatasetLoader(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, img_transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the test images.            \n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        \n",
        "        self.image_ids = ['test_data/test' + str(i) + '.jpg' for i in range(1, 5001)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        \n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        angle_in_degrees = 45\n",
        "\n",
        "        #output = torch.from_numpy(ndimage.rotate(alpha, angle_in_degrees, reshape=False))\n",
        "        # sample = {\n",
        "        #     'top': image,\n",
        "        #     'left': torch.from_numpy(ndimage.rotate(image, 90, reshape=False)),\n",
        "        #     'bottom': torch.from_numpy(ndimage.rotate(image, 180, reshape=False)),\n",
        "        #     'right': torch.from_numpy(ndimage.rotate(image, 270, reshape=False))\n",
        "        #     }\n",
        "        sample['image'] = image\n",
        "        \n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJmX46Q3cRTn"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "95MJggLDwU9O"
      },
      "outputs": [],
      "source": [
        "# the VGG11 architecture\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.in_channels = 3\n",
        "        self.num_classes = embed_dim\n",
        "        # convolutional layers \n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # fully connected linear layers\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=32768, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.Linear(in_features=4096, out_features=embedding_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        # flatten to prepare for the fully connected layers\n",
        "        #rint(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        print(x.shape)\n",
        "        return x\n",
        "        \n",
        "# class Decoder(nn.Module):\n",
        "#     '''\n",
        "#     This class represents the Decoder Module which consists of LSTM layers\n",
        "#     Parameters:\n",
        "#         embed_size : Embedding dimension of words and images\n",
        "#         hidden_size : hidden_state dimension of LSTM\n",
        "#         vocab_size : Length of vocabulary\n",
        "#         num_layers : Number of LSTM layers\n",
        "        \n",
        "#     Input :\n",
        "#         features : Encoded image features\n",
        "#         captions : Tokenized training captions\n",
        "#         lengths: Length of each sequence \n",
        "    \n",
        "#     Output :\n",
        "#         Outputs probability distribution over vocabulary ( dimension : 1 * vocab_size)\n",
        "#     '''\n",
        "#     def __init__(self,embed_dim, lstm_hidden_size, num_layers=1):\n",
        "#         \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "#         super(Decoder, self).__init__()\n",
        "#         self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
        "#         self.lstm = nn.LSTM(embed_dim, lstm_hidden_size, num_layers, batch_first=True)\n",
        "#         self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "#         #self.relu = nn.ReLU(inplace = True)   # Performs worse since LSTM already has sigmoid activation\n",
        "#         self.dropout = nn.Dropout(p=0.5, inplace = False)\n",
        "# #        self.init_weights\n",
        "#         self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "        \n",
        "#     def init_weights(self):\n",
        "#         self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "#         self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "#         self.linear.bias.data.fill_(0)\n",
        "        \n",
        "#     def forward(self, features, captions, lengths):\n",
        "#         \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "#         embeddings = self.embed(captions)   #Embedd tokenized captions into latent space\n",
        "#         embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # Concatenate image enocded features with embedded captions\n",
        "#         #embeddings = self.dropout(embeddings)\n",
        "#         # Dropout after concatenation leads to better Bleu Score\n",
        "#         packed_seq = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted= False)\n",
        "#         hiddens , _ = self.lstm(packed_seq)\n",
        "#         #outputs = self.linear(hiddens_rasha[0])  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "#         outputs = self.linear(self.dropout(hiddens[0]))  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "#         return outputs\n",
        "\n",
        "\n",
        "#     def get_pred(self, features, hidden=None):\n",
        "#         '''Helper function for max_predictions'''\n",
        "#         output, hidden = self.lstm(features, hidden)\n",
        "#         output = self.linear(output.squeeze(1))\n",
        "#         return output, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, lstm_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "        # print(\"VOCAB SIZE = \", self.vocab_size)\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
        "                            num_layers = lstm_layers, batch_first = True)\n",
        "        #self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)        \n",
        "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        \n",
        "    def forward(self, image_features, image_captions):\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        embeddings = self.dropout(self.embed(image_captions))\n",
        "        print(\"Dimension:\",embeddings.shape, image_features.shape)\n",
        "        embeddings = torch.cat((image_features, embeddings[:,:-1]), dim = 1)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, embed_dim, lstm_hidden_size,lstm_layers=1):\n",
        "#         super(Decoder, self).__init__()\n",
        "\n",
        "# #         self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "# #         self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "# #         self.linear.bias.data.fill_(0)\n",
        "\n",
        "#         self.lstm_hidden_size = lstm_hidden_size\n",
        "#         self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "\n",
        "#         self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
        "        \n",
        "#         # self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "\n",
        "#         self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "#         self.dropout = nn.Dropout(p=0.4, inplace = False)\n",
        "\n",
        "#         #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "\n",
        "#         self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "#         self.init_weights()\n",
        "\n",
        "#     def init_weights(self):\n",
        "#         \"\"\"\n",
        "#         Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "#         \"\"\"\n",
        "#         # self.lstm.weight.data.uniform_(-1,1)\n",
        "#         self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "#         # self.fc.bias.data.fill_(0)\n",
        "#         # self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "\n",
        "#     def forward(self, image_features, image_captions):\n",
        "#         #print(\"DECODER INPUT\", image_features)\n",
        "#         # if phase == \"Train\":\n",
        "#         #     #print(image)\n",
        "#         #     image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
        "\n",
        "# #         embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # Concatenate image enocded features with embedded captions\n",
        "# #         #embeddings = self.dropout(embeddings)\n",
        "# #         # Dropout after concatenation leads to better Bleu Score\n",
        "# #         packed_seq = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted= False)\n",
        "# #         hiddens , _ = self.lstm(packed_seq)\n",
        "# #         #outputs = self.linear(hiddens_rasha[0])  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "# #         outputs = self.linear(self.dropout(hiddens[0]))  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "# #         return outputs\n",
        "\n",
        "\n",
        "#         if not torch.cuda.is_available():\n",
        "#             image_features = torch.LongTensor(image_features)            \n",
        "\n",
        "#         image_features = image_features.unsqueeze(1)\n",
        "    \n",
        "#         embedded_captions = self.embed(image_captions)\n",
        "#         embedded_captions = self.dropout(embedded_captions)\n",
        "        \n",
        "#         input_lstm = torch.cat((image_features, embedded_captions[:,:-1]), dim = 1) # Teacher Forcing :)\n",
        "\n",
        "#         #input_lstm = pack_padded_sequence(input_lstm, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "#         lstm_outputs, _ = self.lstm(input_lstm)\n",
        "#         #lstm_outputs = self.linear(lstm_outputs[0]) \n",
        "#         # print(\"lstm_outputs.shape\", lstm_outputs.shape)\n",
        "#         lstm_outputs = self.linear(lstm_outputs) \n",
        "        \n",
        "#         return lstm_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1HhVbh_iwsVM"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, num_of_layers):\n",
        "        super(ImageCaptionsNet, self).__init__()              \n",
        "        self.Encoder = Encoder(embed_size)\n",
        "        self.Decoder = Decoder(embed_size, hidden_size, num_of_layers)\n",
        "\n",
        "    def forward(self, img_batch, cap_batch):\n",
        "        x = self.Encoder(img_batch)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            # x = torch.LongTensor(x)\n",
        "            x = x.cpu().long().numpy()\n",
        "            x = torch.LongTensor(x)\n",
        "            x = x.cuda()\n",
        "        else:\n",
        "            x = x.long().numpy()\n",
        "\n",
        "        try:\n",
        "            print('Working',x.shape, cap_batch.shape)\n",
        "            x = self.Decoder(x, cap_batch)\n",
        "        except:\n",
        "            print('Error',x.shape, cap_batch.shape)\n",
        "        return x\n",
        "    \n",
        "    # def predict(self, device, test_loader):\n",
        "    #     self.Encoder.eval()\n",
        "    #     self.Decoder.eval()\n",
        "\n",
        "    #     with torch.no_grad():\n",
        "    #         conc_out = []\n",
        "    #         captions = []\n",
        "    #         # conc_label = []\n",
        "\n",
        "    #         for batch_idx, sample in enumerate(test_loader):\n",
        "    #             # Move tensor to the proper device\n",
        "    #             image_batch = sample['image'].float()\n",
        "                \n",
        "    #             #image_batch = image_batch.to(device)\n",
        "    #             # Encode data\n",
        "    #             encoded_data = self.Encoder(image_batch)\n",
        "    #             # Decode data\n",
        "    #             decoded_data = self.Decoder(encoded_data)  # 32*11\n",
        "\n",
        "    #             conc_out = decoded_data.cpu().numpy()\n",
        "                                \n",
        "    #             for i in conc_out:\n",
        "    #                 captions.append(self.captions_preprocessing_obj.get_caption(i))\n",
        "\n",
        "    #             print(\"Batch:{}\".format(batch_idx))\n",
        "                \n",
        "    #             if batch_idx > 3:\n",
        "    #                 return captions\n",
        "\n",
        "    #     return captions\n",
        "        \n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "g9oL9H7e_kmW",
        "outputId": "cb47ca5f-3146-4541-bc4d-ff977837bd8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset loaded\n",
            "Optimizer loaded\n",
            "Train Loader loaded\n",
            "Epoch 1\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n"
          ]
        }
      ],
      "source": [
        "# os.chdir('/content/drive/MyDrive/data/train_data_main/')\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/data/train_data_main/'\n",
        "\n",
        "# IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "\n",
        "os.chdir(os.path.join(BASE_DIR, 'data'))\n",
        "IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "embedding_dim = 200\n",
        "units = 512\n",
        "lstm_layers = 4\n",
        "\n",
        "net = ImageCaptionsNet(embedding_dim, units, lstm_layers)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    net = net.to(torch.device(\"cuda:0\"))\n",
        "else:\n",
        "    net = net.to(torch.device(\"cpu\"))\n",
        "# Creating the Dataset\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "print(\"Train Dataset loaded\")\n",
        "# Define your hyperparameters\n",
        "NUMBER_OF_EPOCHS = 5\n",
        "LEARNING_RATE = 1e-1\n",
        "BATCH_SIZE = 10\n",
        "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
        "# optimizer = optim.SGD(list(net.Decoder.parameters()) + list(net.Encoder.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "optimizer_encoder = optim.SGD(list(net.Encoder.parameters()), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "optimizer_decoder = optim.Adam(list(net.Decoder.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Optimizer loaded\")\n",
        "\n",
        "# Creating the DataLoader for batching purposes\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
        "print(\"Train Loader loaded\")\n",
        "\n",
        "start = time()\n",
        "loss_list = []\n",
        "import os\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    iteration = 0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        net.Encoder.zero_grad()\n",
        "        net.Decoder.zero_grad()\n",
        "        \n",
        "        # optimizer.zero_grad()\n",
        "        optimizer_encoder.zero_grad()\n",
        "        optimizer_decoder.zero_grad()\n",
        "\n",
        "        image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "        #If GPU training required\n",
        "        image_batch = image_batch.float()\n",
        "        #captions_batch = captions_batch.float()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "\n",
        "        try:\n",
        "            output_captions = net(image_batch, captions_batch) # Batch_size * max_caption_length * vocab_size\n",
        "            print('Normal',captions_batch.shape, output_captions.shape)    \n",
        "        except:\n",
        "            print('Error---',captions_batch.shape, output_captions.shape)    \n",
        "            \n",
        "        if len(output_captions.shape) < 3:\n",
        "            continue\n",
        "\n",
        "\n",
        "        # print(captions_batch.shape, output_captions.shape)\n",
        "\n",
        "        if batch_idx > 10 or epoch > 0:\n",
        "            for i in range(captions_batch.shape[0]):\n",
        "                if len(captions_batch.shape) > 2:\n",
        "                    val1 = captions_batch[i,::]\n",
        "                    val1 = np.argmax(val1, axis=1).numpy()\n",
        "                else:\n",
        "                    val1 = captions_batch[i,::]\n",
        "                \n",
        "                if torch.cuda.is_available():\n",
        "                    val1 = val1.cpu().detach().numpy()\n",
        "                    val2 = output_captions[i,::]\n",
        "                    val2 = np.argmax((val2.cpu().detach().numpy()), axis=1)\n",
        "                else:\n",
        "                    val1 = val1.detach().numpy()\n",
        "                    val2 = output_captions[i,::]\n",
        "                    val2 = np.argmax((val2.detach().numpy()), axis=1)\n",
        "\n",
        "                print(captions_preprocessing_obj.get_captions(val1))\n",
        "                print(captions_preprocessing_obj.get_captions(val2),'\\n')\n",
        "        \n",
        "        # Analyze the loss_function\n",
        "        # Pass the matrices with same dimension\n",
        "\n",
        "\n",
        "                # val2 = np.argmax((output_captions.detach().numpy())[i,:,:], axis=1)\n",
        "                # print(captions_preprocessing_obj.get_captions(val1.detach().numpy()))\n",
        "                # print(captions_preprocessing_obj.get_captions(val2),'\\n')\n",
        "            # print(list(map(lambda x : captions_preprocessing_obj.index_to_word[int(x)], val1)))\n",
        "            # print(list(map(lambda x : captions_preprocessing_obj.index_to_word[int(x)], val2)))\n",
        "\n",
        "        # for j in captions_batch[0][i].split():\n",
        "        #     print(captions_preprocessing_obj.get_captions(caption_batch[i]))\n",
        "        #     print(captions_preprocessing_obj.get_captions(output_batch[i]))\n",
        "        #     print((caption_batch[i]))\n",
        "        #     print((output_batch[i]))\n",
        "\n",
        "        #print(np.argmax(captions_batch, axis = 2).shape, np.argmax(output_captions.detach().numpy(), axis = 2).shape, image_batch.shape)\n",
        "\n",
        "        # loss = loss_function(output_captions.reshape(output_captions.shape[0], -1), captions_batch.reshape(captions_batch.shape[0], -1)) # 32 * max_caption_length * vocab_size\n",
        "        # loss = loss_function(output_captions.reshape(output_captions.shape[0], -1), captions_batch.reshape(captions_batch.shape[0], -1)) # 32 * max_caption_length * vocab_size\n",
        "        # loss = loss_function(np.argmax(torch.Tensor(np.argmax(output_captions.detach().numpy(), axis = 2))), torch.Tensor(np.argmax(captions_batch, axis = 2))) # 32 * max_caption_length * vocab_size\n",
        "\n",
        "        # Look into the loss function\n",
        "        # Output_captions - Batch_size * max_caption_length * vocab_size\n",
        "        # Captions_batch  - Batch_size * max_caption_length\n",
        "\n",
        "        # loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "        loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizer.step()\n",
        "        optimizer_encoder.step()\n",
        "        optimizer_decoder.step()\n",
        "\n",
        "        print(\"Epoch:{}, Iteration: {}, Loss: {}, TimeElapsed: {}Min\".format(epoch, iteration+1, round(loss.item(), 2), round((time()-start)/60,2), ))\n",
        "        iteration+=1\n",
        "\n",
        "        if (iteration > 15):\n",
        "            break\n",
        "\n",
        "# Encoder Output - Batch_size * "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkBk7uOx4T8D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cad5b-tpraQC"
      },
      "outputs": [],
      "source": [
        "placeholder = np.zeros(15)\n",
        "for i in captions_preprocessing_obj.captions_dict.values():\n",
        "    placeholder[len(i.split(\" \"))] += 1\n",
        "\n",
        "for i in placeholder:\n",
        "    print(round(i,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJvLwS9cRTo"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ0AJsJZcRTo"
      },
      "outputs": [],
      "source": [
        "TEST_IMAGE_DIR = '/content/drive/MyDrive/data/test_data/'\n",
        "\n",
        "test_img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) # Applied sequentially\n",
        "\n",
        "# Creating the Dataset\n",
        "test_dataset = TestDatasetLoader(TEST_IMAGE_DIR, img_transform=test_img_transform)\n",
        "\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "#output_caption = net.predict(device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf6vQFnkcRTo"
      },
      "outputs": [],
      "source": [
        "def caption_image(image_feature, max_words=20):\n",
        "        x = image_feature.unsqueeze(0)\n",
        "        results = []\n",
        "        states = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_words):\n",
        "                \n",
        "                hiddens, states = net.Decoder.lstm(x, states)\n",
        "                decoder_op = net.Decoder.linear(hiddens.squeeze(1))\n",
        "                # predicted_word = decoder_op.argmax(1) # 1, 7356\n",
        "                predicted_word = np.argmax(decoder_op, axis=1) # 1, 7356\n",
        "                decoder_op = decoder_op[0].tolist()\n",
        "                print(max(decoder_op))\n",
        "                prob = max(decoder_op)\n",
        "                x = net.Decoder.embed(predicted_word)\n",
        "                x = x.unsqueeze(0)\n",
        "                word = np.argmax(predicted_word[0])\n",
        "                results.append(word)\n",
        "                if predicted_word == captions_preprocessing_obj.vocab[\"[END]\"]:\n",
        "                    break\n",
        "        print(results)\n",
        "        caption = [captions_preprocessing_obj.index_to_word[int(i)] for i in  results]\n",
        "        cap = ' '.join(caption)\n",
        "        cap = cap.replace(\"[START]\",\"\").replace(\"[END]\",\"\")\n",
        "        return cap\n",
        "def to_device(data, device):\n",
        "        if isinstance(data,(list,tuple)):\n",
        "            return [to_device(x,device) for x in data]\n",
        "        return data.to(device)\n",
        "\n",
        "def max_prediction(encoded_features, decoder_model, max_length=80):\n",
        "    #word_2_ix, ix_2_word = vocab_dict\n",
        "    start_token = captions_preprocessing_obj.vocab['[START]']\n",
        "    end_token = captions_preprocessing_obj.vocab['[END]']\n",
        "    hidden = None # In the beginning the hidden state is None\n",
        "    caption_word_id = []\n",
        "    for i in range(max_length):\n",
        "        encoded_features = encoded_features.unsqueeze(1)\n",
        "        if(hidden == None):\n",
        "            output, hidden = net.Decoder.get_pred(encoded_features)\n",
        "        else:\n",
        "            output, hidden = net.Decoder.get_pred(encoded_features, to_device(hidden, device))\n",
        "    \n",
        "        _ , predicted_id = output.max(1)\n",
        "        caption_word_id.append(predicted_id)\n",
        "        if (predicted_id == end_token):\n",
        "            break\n",
        "        encoded_features = decoder_model.embed(predicted_id)\n",
        "    caption_word_id = torch.stack(caption_word_id, 1)\n",
        "    return caption_word_id.cpu().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj5T__BkcRTp"
      },
      "outputs": [],
      "source": [
        "pred_caps = {}\n",
        "for batch_idx, sample in enumerate(test_loader):\n",
        "        print(\"Image_idx\", batch_idx)\n",
        "        image = sample['image']\n",
        "        image = image.float()\n",
        "        img_features = net.Encoder(image)\n",
        "        print(img_features.shape)\n",
        "        pred_cap = caption_image(img_features, 11)\n",
        "        pred_caps[batch_idx] = pred_cap\n",
        "        print(\"Predicted\",batch_idx, pred_cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE-2K64GcRTp"
      },
      "outputs": [],
      "source": [
        "def test_pred(embed_dim, hidden_dim,num_layer, vocab_len, vocab_dict, train_enc=False):\n",
        "    encoder_model = net.Encoder(embed_dim, train_enc)\n",
        "    decoder_model = net.Decoder(embed_dim, hidden_dim, num_layer)\n",
        "        \n",
        "    #encoder_model = encoder_model.cuda()\n",
        "    encoder_model.eval() # Makes the model ready to be used in evaluation by taking care of batch norm and dropout\n",
        "    #decoder_model = decoder_model.cuda()\n",
        "    decoder_model.eval()\n",
        "    \n",
        "    test_dl = get_test_data()\n",
        "    private_test_dl = get_private_test_data()\n",
        "    bleu_score=[]\n",
        "    hypo_complete = []\n",
        "    ref = [[],[],[],[],[]]\n",
        "    #for batchid, (images, img_id) in enumerate(private_test_dl):\n",
        "    for batch_idx, sample in enumerate(test_loader):\n",
        "        image = sample['image']\n",
        "        images = images.cuda()\n",
        "        encode_feat = encoder_model(images.float(), train_enc)\n",
        "        #output_bs_pred =  beam_search_pred(encode_feat_rasha, decoder_model, vocab_dict, bw)\n",
        "        output = max_prediction(encode_feat, decoder_model,  max_length=80):\n",
        "        output_converted = [captions_preprocessing_obj.index_to_word[int(x)] for x in output]\n",
        "        #hypo = convert_rasha_max_pred(output_bs_pred, ix_2_word)\n",
        "        #private.write(f'{(img_id[0])}\\t {\" \".join(hypo)}\\n') \n",
        "        print(output_converted)\n",
        "\n",
        "    # sacre_bleu = sacrebleu.corpus_bleu(hypo_complete, ref).score\n",
        "    # print(f\"BLEU SCORE on public test data by beam search : {np.mean(np.array(bleu_score))}\")\n",
        "    # print(f\"SACREBLEU SCORE by beam search : {sacre_bleu}\")\n",
        "    # return np.mean(np.array(bleu_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "a =  np.arange(100).reshape(5,20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "q1_new.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
