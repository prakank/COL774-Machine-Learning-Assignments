{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLCSEegQF4Ny",
        "outputId": "ec9c5f6d-45b8-48e0-e2be-f29148161176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K7nJ9KMGzi3",
        "outputId": "4b63c7a6-1adc-4259-89bd-f89ae78799df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/prakank/.local/lib/python3.8/site-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 7.1.0. Several security issues (CVE-2020-11538, CVE-2020-10379, CVE-2020-10994, CVE-2020-10177) have been fixed in pillow 7.1.0 or higher. We recommend to upgrade this library.\n",
            "  from .collection import imread_collection_wrapper\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/prakank/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/prakank/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils, models\n",
        "from collections import Counter\n",
        "from skimage import io, transform\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import matplotlib.pyplot as plt # for plotting\n",
        "import numpy as np\n",
        "from time import time\n",
        "import collections\n",
        "import pickle\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from scipy import ndimage\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vDdwWT_sG3JM"
      },
      "outputs": [],
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        #print(\"TA RESCALE INPUT\", image.shape)\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        img = transform.resize(image, (new_h, new_w))\n",
        "        #print(\"TA RESCALE OUTPUT\", image.shape)\n",
        "        return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ThMQbegvIE5x"
      },
      "outputs": [],
      "source": [
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        # numpy image: H x W x C\n",
        "        # torch image: C X H X W\n",
        "        image = image.transpose((2, 0, 1))\n",
        "        return torch.tensor(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KbsfVXxIH59",
        "outputId": "5b3b1af7-9841-4cd0-b3b7-c461fb5422a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current device set to cpu\n"
          ]
        }
      ],
      "source": [
        "IMAGE_RESIZE = (256, 256)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ])\n",
        "print(\"Current device set to {}\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLQMHTkKIIn6",
        "outputId": "0291da09-3796-4b47-a417-e5a68e61c68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of Vocabulary = 2512\n"
          ]
        }
      ],
      "source": [
        "class CaptionsPreprocessing:\n",
        "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
        "\n",
        "    Args:\n",
        "        captions_file_path (string): captions tsv file path\n",
        "    \"\"\"\n",
        "    def __init__(self, captions_file_path):\n",
        "        self.captions_file_path = captions_file_path\n",
        "        self.raw_captions_dict = self.read_raw_captions()\n",
        "        self.captions_dict = self.process_captions()\n",
        "        self.vocab = self.generate_vocabulary()\n",
        "    def read_raw_captions(self):\n",
        "        # Dictionary with raw captions list keyed by image ids (integers)\n",
        "        captions_dict = {}\n",
        "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
        "            for img_caption_line in f.readlines():\n",
        "                img_captions = img_caption_line.strip().split('\\t')\n",
        "                image_path = '/content/drive/MyDrive/data/train_data_main/' + img_captions[0]\n",
        "                \n",
        "                image_path = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/data/' + img_captions[0]\n",
        "                \n",
        "                if os.path.exists(image_path):\n",
        "                    captions_dict[img_captions[0]] = img_captions[1]\n",
        "                \n",
        "                if len(captions_dict) == 5000:\n",
        "                    break\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def process_captions(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        raw_captions_dict = self.raw_captions_dict\n",
        "\n",
        "        # Do the preprocessing here\n",
        "        # Can remove the stopwords and gibberish in the caption\n",
        "        stop_words = stopwords.words('english')\n",
        "        self.allowedLength = 7\n",
        "        punctuation = list(string.punctuation)\n",
        "\n",
        "        for key, value in raw_captions_dict.items():\n",
        "            cleaned_caption = re.sub('[^A-Za-z0-9]+', ' ', value) #Extra space removal\n",
        "            tokens = word_tokenize(cleaned_caption)\n",
        "            cleaned_tokens = [token for token in tokens if token not in stop_words and token not in punctuation] # Remove stopwords and punctuation\n",
        "            cleaned_caption = \"[START] \" + \" \".join(cleaned_tokens[:self.allowedLength]) + \" [END]\"\n",
        "            raw_captions_dict[key] = cleaned_caption        \n",
        "\n",
        "        captions_dict = raw_captions_dict\n",
        "\n",
        "        return captions_dict\n",
        "\n",
        "    def generate_vocabulary(self):\n",
        "        \"\"\"\n",
        "        Use this function to generate dictionary and other preprocessing on captions\n",
        "        \"\"\"\n",
        "\n",
        "        captions_dict = self.captions_dict\n",
        "        vocabulary = {}\n",
        "        max_caption = 0\n",
        "        idx = 1\n",
        "        index_to_word = {}\n",
        "        for key, value in captions_dict.items():\n",
        "            val = value.split()\n",
        "            max_caption = max(max_caption, len(val))\n",
        "\n",
        "            for i in val:\n",
        "                if i not in vocabulary.keys():\n",
        "                    vocabulary[i] = idx\n",
        "                    index_to_word[idx] = i\n",
        "                    idx+=1\n",
        "\n",
        "        self.max_caption = max_caption\n",
        "        self.max_caption = (self.allowedLength+2)\n",
        "\n",
        "        index_to_word[0] = \"NIL\"\n",
        "\n",
        "        self.index_to_word = index_to_word\n",
        "        \n",
        "        # Generate the vocabulary\n",
        "        print(\"Size of Vocabulary = {}\".format(len(vocabulary)))\n",
        "        return vocabulary\n",
        "\n",
        "\n",
        "    def get_captions(self, tensor_tokens):\n",
        "        caption = [self.index_to_word[int(x)] for x in tensor_tokens]\n",
        "        return \" \".join(caption)\n",
        "\n",
        "    def captions_transform(self, img_caption):\n",
        "        \"\"\"\n",
        "        Use this function to generate tensor tokens for the text captions\n",
        "        Args:\n",
        "            img_caption_list: List of captions for a particular image\n",
        "        \"\"\"\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = img_caption.split(\" \")\n",
        "        \n",
        "        \n",
        "        # print(img_caption, caption)\n",
        "\n",
        "        caption_mapped = np.zeros(self.max_caption)\n",
        "        for i in range(len(caption)):\n",
        "            try: caption_mapped[i] = self.vocab[caption[i]]\n",
        "            except: print(img_caption, caption, i)\n",
        "\n",
        "        # caption_mapped = np.zeros((self.max_caption, len(self.vocab)))\n",
        "        # for i in range(len(caption)):\n",
        "        #     val = np.zeros(len(self.vocab))\n",
        "        #     val[self.vocab[caption[i]]] = 1\n",
        "        #     caption_mapped[i,:] = val \n",
        "\n",
        "        #captions_mapped = np.argmax(captions_mapped, axis = 1)\n",
        "        \n",
        "        return torch.LongTensor(caption_mapped)\n",
        "\n",
        "# Set the captions tsv file path\n",
        "\n",
        "# CAPTIONS_FILE_PATH = '/content/drive/MyDrive/data/train_text.tsv'\n",
        "#CAPTIONS_FILE_PATH = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4/Train_text.tsv'\n",
        "\n",
        "BASE_DIR = '/home/prakank/IIT Delhi/3rd_year/Sem5/COL774_Machine_Learning/COL774-Machine-Learning-Assignments/Assignment-4/'\n",
        "CAPTIONS_FILE_PATH = os.path.join(BASE_DIR, 'data', 'train_text.tsv')\n",
        "\n",
        "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BZpbuRPXbis",
        "outputId": "374961a0-d26e-4c1e-f6ac-997abfc47af1"
      },
      "outputs": [],
      "source": [
        "# print(captions_preprocessing_obj.index_to_word)\n",
        "# print(captions_preprocessing_obj.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTjaF7F43tkc",
        "outputId": "78cc7b1b-7b94-461d-f953-bb9d00265b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2513\n"
          ]
        }
      ],
      "source": [
        "print(len(captions_preprocessing_obj.index_to_word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSGE4kORcRTm"
      },
      "source": [
        "## DataSet Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Hr-f4Q7lE9zy"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            captions_dict: Dictionary with captions list keyed by image paths (strings)\n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "\n",
        "            captions_transform: (callable, optional): Optional transform to be applied\n",
        "                on the caption sample (list).\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_dict = captions_dict\n",
        "        self.img_transform = img_transform\n",
        "        self.captions_transform = captions_transform\n",
        "\n",
        "        self.image_ids = list(captions_dict.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        captions = self.captions_dict[img_name]\n",
        "\n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "\n",
        "        if self.captions_transform:\n",
        "            captions = self.captions_transform(captions)\n",
        "\n",
        "        sample = {'image': image, 'captions': captions}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vAkn6005wm8d"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  res = {}\n",
        "#   print('Initial shape: ',np.asarray(batch).shape)\n",
        "\n",
        "  res['image'] = [sample['image'].unsqueeze(0) for sample in batch] \n",
        "\n",
        "#   print('Res image1:',(res['image']))\n",
        "\n",
        "  res['image'] = torch.cat((res['image']), dim=0)\n",
        "  \n",
        "#   print('Res image2:',res['image'].shape)\n",
        "\n",
        "  res['captions'] = [sample['captions'] for sample in batch]\n",
        "  \n",
        "#   print('Res caption1:',res['captions'].shape)\n",
        "\n",
        "  res['captions'] = torch.nn.utils.rnn.pad_sequence(res['captions'], batch_first=True)\n",
        "\n",
        "#   print(res)\n",
        "\n",
        "  return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nBo0O182cRTn"
      },
      "outputs": [],
      "source": [
        "class TestDatasetLoader(Dataset):\n",
        "    \n",
        "    def __init__(self, img_dir, img_transform):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the test images.            \n",
        "            img_transform (callable, optional): Optional transform to be applied\n",
        "                on the image sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.img_transform = img_transform\n",
        "        \n",
        "        self.image_ids = ['test_data/test' + str(i) + '.jpg' for i in range(1, 5001)]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_ids[idx]\n",
        "        image = io.imread(img_name)\n",
        "        \n",
        "        if self.img_transform:\n",
        "            image = self.img_transform(image)\n",
        "        angle_in_degrees = 45\n",
        "\n",
        "        #output = torch.from_numpy(ndimage.rotate(alpha, angle_in_degrees, reshape=False))\n",
        "        # sample = {\n",
        "        #     'top': image,\n",
        "        #     'left': torch.from_numpy(ndimage.rotate(image, 90, reshape=False)),\n",
        "        #     'bottom': torch.from_numpy(ndimage.rotate(image, 180, reshape=False)),\n",
        "        #     'right': torch.from_numpy(ndimage.rotate(image, 270, reshape=False))\n",
        "        #     }\n",
        "        sample['image'] = image\n",
        "        \n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJmX46Q3cRTn"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "95MJggLDwU9O"
      },
      "outputs": [],
      "source": [
        "# the VGG11 architecture\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.in_channels = 3\n",
        "        self.num_classes = embed_dim\n",
        "        # convolutional layers \n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        # fully connected linear layers\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(in_features=32768, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.Linear(in_features=4096, out_features=4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout2d(0.5),\n",
        "            nn.Linear(in_features=4096, out_features=embedding_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        # flatten to prepare for the fully connected layers\n",
        "        #rint(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        print('Encoder-Output:',x.shape)\n",
        "        return x\n",
        "        \n",
        "# class Decoder(nn.Module):\n",
        "#     '''\n",
        "#     This class represents the Decoder Module which consists of LSTM layers\n",
        "#     Parameters:\n",
        "#         embed_size : Embedding dimension of words and images\n",
        "#         hidden_size : hidden_state dimension of LSTM\n",
        "#         vocab_size : Length of vocabulary\n",
        "#         num_layers : Number of LSTM layers\n",
        "        \n",
        "#     Input :\n",
        "#         features : Encoded image features\n",
        "#         captions : Tokenized training captions\n",
        "#         lengths: Length of each sequence \n",
        "    \n",
        "#     Output :\n",
        "#         Outputs probability distribution over vocabulary ( dimension : 1 * vocab_size)\n",
        "#     '''\n",
        "#     def __init__(self,embed_dim, lstm_hidden_size, num_layers=1):\n",
        "#         \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "#         super(Decoder, self).__init__()\n",
        "#         self.embed = nn.Embedding(self.vocab_size, embed_size)\n",
        "#         self.lstm = nn.LSTM(embed_dim, lstm_hidden_size, num_layers, batch_first=True)\n",
        "#         self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "#         #self.relu = nn.ReLU(inplace = True)   # Performs worse since LSTM already has sigmoid activation\n",
        "#         self.dropout = nn.Dropout(p=0.5, inplace = False)\n",
        "# #        self.init_weights\n",
        "#         self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "        \n",
        "#     def init_weights(self):\n",
        "#         self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "#         self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "#         self.linear.bias.data.fill_(0)\n",
        "        \n",
        "#     def forward(self, features, captions, lengths):\n",
        "#         \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "#         embeddings = self.embed(captions)   #Embedd tokenized captions into latent space\n",
        "#         embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # Concatenate image enocded features with embedded captions\n",
        "#         #embeddings = self.dropout(embeddings)\n",
        "#         # Dropout after concatenation leads to better Bleu Score\n",
        "#         packed_seq = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted= False)\n",
        "#         hiddens , _ = self.lstm(packed_seq)\n",
        "#         #outputs = self.linear(hiddens_rasha[0])  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "#         outputs = self.linear(self.dropout(hiddens[0]))  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "#         return outputs\n",
        "\n",
        "\n",
        "#     def get_pred(self, features, hidden=None):\n",
        "#         '''Helper function for max_predictions'''\n",
        "#         output, hidden = self.lstm(features, hidden)\n",
        "#         output = self.linear(output.squeeze(1))\n",
        "#         return output, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_dim, lstm_hidden_size, lstm_layers=1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.lstm_hidden_size = lstm_hidden_size\n",
        "        self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "        # print(\"VOCAB SIZE = \", self.vocab_size)\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size = embed_dim, hidden_size = lstm_hidden_size,\n",
        "                            num_layers = lstm_layers, batch_first = True)\n",
        "        #self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "        self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "        #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "        self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "    def forward(self, image_features, image_captions):\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        embeddings = self.dropout(self.embed(image_captions))\n",
        "        print(\"Dimension:\",embeddings.shape, image_features.shape)\n",
        "        embeddings = torch.cat((image_features, embeddings[:,:-1]), dim = 1)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# class Decoder(nn.Module):\n",
        "#     def __init__(self, embed_dim, lstm_hidden_size,lstm_layers=1):\n",
        "#         super(Decoder, self).__init__()\n",
        "\n",
        "# #         self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "# #         self.linear.weight.data.uniform_(-0.1, 0.1)\n",
        "# #         self.linear.bias.data.fill_(0)\n",
        "\n",
        "#         self.lstm_hidden_size = lstm_hidden_size\n",
        "#         self.vocab_size = len(captions_preprocessing_obj.vocab)\n",
        "\n",
        "#         self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n",
        "        \n",
        "#         # self.attention = AttentionBlock(embed_dim, lstm_hidden_size, self.vocab_size)\n",
        "\n",
        "#         self.linear = nn.Linear(lstm_hidden_size, self.vocab_size)\n",
        "#         self.dropout = nn.Dropout(p=0.4, inplace = False)\n",
        "\n",
        "#         #self.embed = nn.Embedding.from_pretrained(init_weights)\n",
        "\n",
        "#         self.embed = nn.Embedding(self.vocab_size, embed_dim)\n",
        "#         self.init_weights()\n",
        "\n",
        "#     def init_weights(self):\n",
        "#         \"\"\"\n",
        "#         Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "#         \"\"\"\n",
        "#         # self.lstm.weight.data.uniform_(-1,1)\n",
        "#         self.embed.weight.data.uniform_(-0.1, 0.1)\n",
        "#         # self.fc.bias.data.fill_(0)\n",
        "#         # self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "\n",
        "#     def forward(self, image_features, image_captions):\n",
        "#         #print(\"DECODER INPUT\", image_features)\n",
        "#         # if phase == \"Train\":\n",
        "#         #     #print(image)\n",
        "#         #     image_features = torch.Tensor.repeat_interleave(image_features, repeats=5 , dim=0)\n",
        "\n",
        "# #         embeddings = torch.cat((features.unsqueeze(1), embeddings), 1) # Concatenate image enocded features with embedded captions\n",
        "# #         #embeddings = self.dropout(embeddings)\n",
        "# #         # Dropout after concatenation leads to better Bleu Score\n",
        "# #         packed_seq = pack_padded_sequence(embeddings, lengths, batch_first=True, enforce_sorted= False)\n",
        "# #         hiddens , _ = self.lstm(packed_seq)\n",
        "# #         #outputs = self.linear(hiddens_rasha[0])  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "# #         outputs = self.linear(self.dropout(hiddens[0]))  #Pass output of lstm through a linear layer to get prob. dist. over vocab\n",
        "# #         return outputs\n",
        "\n",
        "\n",
        "#         if not torch.cuda.is_available():\n",
        "#             image_features = torch.LongTensor(image_features)            \n",
        "\n",
        "#         image_features = image_features.unsqueeze(1)\n",
        "    \n",
        "#         embedded_captions = self.embed(image_captions)\n",
        "#         embedded_captions = self.dropout(embedded_captions)\n",
        "        \n",
        "#         input_lstm = torch.cat((image_features, embedded_captions[:,:-1]), dim = 1) # Teacher Forcing :)\n",
        "\n",
        "#         #input_lstm = pack_padded_sequence(input_lstm, lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "#         lstm_outputs, _ = self.lstm(input_lstm)\n",
        "#         #lstm_outputs = self.linear(lstm_outputs[0]) \n",
        "#         # print(\"lstm_outputs.shape\", lstm_outputs.shape)\n",
        "#         lstm_outputs = self.linear(lstm_outputs) \n",
        "        \n",
        "#         return lstm_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1HhVbh_iwsVM"
      },
      "outputs": [],
      "source": [
        "class ImageCaptionsNet(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, num_of_layers):\n",
        "        super(ImageCaptionsNet, self).__init__()              \n",
        "        self.Encoder = Encoder(embed_size)\n",
        "        self.Decoder = Decoder(embed_size, hidden_size, num_of_layers)\n",
        "\n",
        "    def forward(self, img_batch, cap_batch):\n",
        "        x = self.Encoder(img_batch)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            # x = torch.LongTensor(x)\n",
        "            x = x.cpu().long().numpy()\n",
        "            x = torch.LongTensor(x)\n",
        "            x = x.cuda()\n",
        "        else:\n",
        "            x = x.long().numpy()\n",
        "\n",
        "        try:\n",
        "            print('Working',x.shape, cap_batch.shape)\n",
        "            x = self.Decoder(x, cap_batch)\n",
        "        except:\n",
        "            print('Error',x.shape, cap_batch.shape)\n",
        "        return x\n",
        "    \n",
        "    # def predict(self, device, test_loader):\n",
        "    #     self.Encoder.eval()\n",
        "    #     self.Decoder.eval()\n",
        "\n",
        "    #     with torch.no_grad():\n",
        "    #         conc_out = []\n",
        "    #         captions = []\n",
        "    #         # conc_label = []\n",
        "\n",
        "    #         for batch_idx, sample in enumerate(test_loader):\n",
        "    #             # Move tensor to the proper device\n",
        "    #             image_batch = sample['image'].float()\n",
        "                \n",
        "    #             #image_batch = image_batch.to(device)\n",
        "    #             # Encode data\n",
        "    #             encoded_data = self.Encoder(image_batch)\n",
        "    #             # Decode data\n",
        "    #             decoded_data = self.Decoder(encoded_data)  # 32*11\n",
        "\n",
        "    #             conc_out = decoded_data.cpu().numpy()\n",
        "                                \n",
        "    #             for i in conc_out:\n",
        "    #                 captions.append(self.captions_preprocessing_obj.get_caption(i))\n",
        "\n",
        "    #             print(\"Batch:{}\".format(batch_idx))\n",
        "                \n",
        "    #             if batch_idx > 3:\n",
        "    #                 return captions\n",
        "\n",
        "    #     return captions\n",
        "        \n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "g9oL9H7e_kmW",
        "outputId": "cb47ca5f-3146-4541-bc4d-ff977837bd8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset loaded\n",
            "Optimizer loaded\n",
            "Train Loader loaded\n",
            "Epoch 1\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "Error--- torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n",
            "torch.Size([10, 200])\n",
            "Working (10, 200) torch.Size([10, 9])\n",
            "Error (10, 200) torch.Size([10, 9])\n",
            "Normal torch.Size([10, 9]) (10, 200)\n"
          ]
        }
      ],
      "source": [
        "# os.chdir('/content/drive/MyDrive/data/train_data_main/')\n",
        "# IMAGE_DIR = '/content/drive/MyDrive/data/train_data_main/'\n",
        "\n",
        "# IMAGE_DIR = '/Users/pratyushsaini/Documents/Semester 5/COL774/Assignment-4'\n",
        "\n",
        "os.chdir(os.path.join(BASE_DIR, 'data'))\n",
        "IMAGE_DIR = os.path.join(BASE_DIR, 'data')\n",
        "\n",
        "embedding_dim = 200\n",
        "units = 512\n",
        "lstm_layers = 4\n",
        "\n",
        "net = ImageCaptionsNet(embedding_dim, units, lstm_layers)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    net = net.to(torch.device(\"cuda:0\"))\n",
        "else:\n",
        "    net = net.to(torch.device(\"cpu\"))\n",
        "# Creating the Dataset\n",
        "train_dataset = ImageCaptionsDataset(\n",
        "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
        "    captions_transform=captions_preprocessing_obj.captions_transform\n",
        ")\n",
        "print(\"Train Dataset loaded\")\n",
        "# Define your hyperparameters\n",
        "NUMBER_OF_EPOCHS = 5\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 10\n",
        "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0).to(device)\n",
        "# optimizer = optim.SGD(list(net.Decoder.parameters()) + list(net.Encoder.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "optimizer_encoder = optim.SGD(list(net.Encoder.parameters()), lr=0.01, momentum=0.9, weight_decay=0.0005)\n",
        "optimizer_decoder = optim.Adam(list(net.Decoder.parameters()), lr=LEARNING_RATE)\n",
        "\n",
        "print(\"Optimizer loaded\")\n",
        "\n",
        "# Creating the DataLoader for batching purposes\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn = collate_fn)\n",
        "print(\"Train Loader loaded\")\n",
        "\n",
        "start = time()\n",
        "loss_list = []\n",
        "import os\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "for epoch in range(NUMBER_OF_EPOCHS):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    iteration = 0\n",
        "    for batch_idx, sample in enumerate(train_loader):\n",
        "        net.Encoder.zero_grad()\n",
        "        net.Decoder.zero_grad()\n",
        "        \n",
        "        # optimizer.zero_grad()\n",
        "        optimizer_encoder.zero_grad()\n",
        "        optimizer_decoder.zero_grad()\n",
        "\n",
        "        image_batch, captions_batch = sample['image'], sample['captions']\n",
        "\n",
        "        #If GPU training required\n",
        "        image_batch = image_batch.float()\n",
        "        #captions_batch = captions_batch.float()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
        "\n",
        "        try:\n",
        "            output_captions = net(image_batch, captions_batch) # Batch_size * max_caption_length * vocab_size\n",
        "            print('Normal',captions_batch.shape, output_captions.shape)    \n",
        "        except:\n",
        "            print('Error---',captions_batch.shape, output_captions.shape)    \n",
        "            \n",
        "        if len(output_captions.shape) < 3:\n",
        "            continue\n",
        "\n",
        "\n",
        "        # print(captions_batch.shape, output_captions.shape)\n",
        "\n",
        "        if batch_idx > 10 or epoch > 0:\n",
        "            for i in range(captions_batch.shape[0]):\n",
        "                if len(captions_batch.shape) > 2:\n",
        "                    val1 = captions_batch[i,::]\n",
        "                    val1 = np.argmax(val1, axis=1).numpy()\n",
        "                else:\n",
        "                    val1 = captions_batch[i,::]\n",
        "                \n",
        "                if torch.cuda.is_available():\n",
        "                    val1 = val1.cpu().detach().numpy()\n",
        "                    val2 = output_captions[i,::]\n",
        "                    val2 = np.argmax((val2.cpu().detach().numpy()), axis=1)\n",
        "                else:\n",
        "                    val1 = val1.detach().numpy()\n",
        "                    val2 = output_captions[i,::]\n",
        "                    val2 = np.argmax((val2.detach().numpy()), axis=1)\n",
        "\n",
        "                print(captions_preprocessing_obj.get_captions(val1))\n",
        "                print(captions_preprocessing_obj.get_captions(val2),'\\n')\n",
        "        \n",
        "        # Analyze the loss_function\n",
        "        # Pass the matrices with same dimension\n",
        "\n",
        "\n",
        "                # val2 = np.argmax((output_captions.detach().numpy())[i,:,:], axis=1)\n",
        "                # print(captions_preprocessing_obj.get_captions(val1.detach().numpy()))\n",
        "                # print(captions_preprocessing_obj.get_captions(val2),'\\n')\n",
        "            # print(list(map(lambda x : captions_preprocessing_obj.index_to_word[int(x)], val1)))\n",
        "            # print(list(map(lambda x : captions_preprocessing_obj.index_to_word[int(x)], val2)))\n",
        "\n",
        "        # for j in captions_batch[0][i].split():\n",
        "        #     print(captions_preprocessing_obj.get_captions(caption_batch[i]))\n",
        "        #     print(captions_preprocessing_obj.get_captions(output_batch[i]))\n",
        "        #     print((caption_batch[i]))\n",
        "        #     print((output_batch[i]))\n",
        "\n",
        "        #print(np.argmax(captions_batch, axis = 2).shape, np.argmax(output_captions.detach().numpy(), axis = 2).shape, image_batch.shape)\n",
        "\n",
        "        # loss = loss_function(output_captions.reshape(output_captions.shape[0], -1), captions_batch.reshape(captions_batch.shape[0], -1)) # 32 * max_caption_length * vocab_size\n",
        "        # loss = loss_function(output_captions.reshape(output_captions.shape[0], -1), captions_batch.reshape(captions_batch.shape[0], -1)) # 32 * max_caption_length * vocab_size\n",
        "        # loss = loss_function(np.argmax(torch.Tensor(np.argmax(output_captions.detach().numpy(), axis = 2))), torch.Tensor(np.argmax(captions_batch, axis = 2))) # 32 * max_caption_length * vocab_size\n",
        "\n",
        "        # Look into the loss function\n",
        "        # Output_captions - Batch_size * max_caption_length * vocab_size\n",
        "        # Captions_batch  - Batch_size * max_caption_length\n",
        "\n",
        "        # loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "        loss = loss_function(output_captions.reshape(-1, output_captions.shape[2]), captions_batch.reshape(-1))\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        loss.backward()\n",
        "\n",
        "        # optimizer.step()\n",
        "        optimizer_encoder.step()\n",
        "        optimizer_decoder.step()\n",
        "\n",
        "        print(\"Epoch:{}, Iteration: {}, Loss: {}, TimeElapsed: {}Min\".format(epoch, iteration+1, round(loss.item(), 2), round((time()-start)/60,2), ))\n",
        "        iteration+=1\n",
        "\n",
        "        if (iteration > 15):\n",
        "            break\n",
        "\n",
        "# Encoder Output - Batch_size * \n",
        "\n",
        "\n",
        "# Attention\n",
        "# OCR\n",
        "# YOLO \n",
        "\n",
        "# Bidirectional LSTM\n",
        "# Pretrained Localization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkBk7uOx4T8D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cad5b-tpraQC"
      },
      "outputs": [],
      "source": [
        "placeholder = np.zeros(15)\n",
        "for i in captions_preprocessing_obj.captions_dict.values():\n",
        "    placeholder[len(i.split(\" \"))] += 1\n",
        "\n",
        "for i in placeholder:\n",
        "    print(round(i,3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hJvLwS9cRTo"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ0AJsJZcRTo"
      },
      "outputs": [],
      "source": [
        "TEST_IMAGE_DIR = '/content/drive/MyDrive/data/test_data/'\n",
        "\n",
        "test_img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor(), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)) ]) # Applied sequentially\n",
        "\n",
        "# Creating the Dataset\n",
        "test_dataset = TestDatasetLoader(TEST_IMAGE_DIR, img_transform=test_img_transform)\n",
        "\n",
        "test_loader  = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "#output_caption = net.predict(device, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf6vQFnkcRTo"
      },
      "outputs": [],
      "source": [
        "def caption_image(image_feature, max_words=20):\n",
        "        x = image_feature.unsqueeze(0)\n",
        "        results = []\n",
        "        states = None\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_words):\n",
        "                \n",
        "                hiddens, states = net.Decoder.lstm(x, states)\n",
        "                decoder_op = net.Decoder.linear(hiddens.squeeze(1))\n",
        "                # predicted_word = decoder_op.argmax(1) # 1, 7356\n",
        "                predicted_word = np.argmax(decoder_op, axis=1) # 1, 7356\n",
        "                decoder_op = decoder_op[0].tolist()\n",
        "                print(max(decoder_op))\n",
        "                prob = max(decoder_op)\n",
        "                x = net.Decoder.embed(predicted_word)\n",
        "                x = x.unsqueeze(0)\n",
        "                word = np.argmax(predicted_word[0])\n",
        "                results.append(word)\n",
        "                if predicted_word == captions_preprocessing_obj.vocab[\"[END]\"]:\n",
        "                    break\n",
        "        print(results)\n",
        "        caption = [captions_preprocessing_obj.index_to_word[int(i)] for i in  results]\n",
        "        cap = ' '.join(caption)\n",
        "        cap = cap.replace(\"[START]\",\"\").replace(\"[END]\",\"\")\n",
        "        return cap\n",
        "def to_device(data, device):\n",
        "        if isinstance(data,(list,tuple)):\n",
        "            return [to_device(x,device) for x in data]\n",
        "        return data.to(device)\n",
        "\n",
        "def max_prediction(encoded_features, decoder_model, max_length=80):\n",
        "    #word_2_ix, ix_2_word = vocab_dict\n",
        "    start_token = captions_preprocessing_obj.vocab['[START]']\n",
        "    end_token = captions_preprocessing_obj.vocab['[END]']\n",
        "    hidden = None # In the beginning the hidden state is None\n",
        "    caption_word_id = []\n",
        "    for i in range(max_length):\n",
        "        encoded_features = encoded_features.unsqueeze(1)\n",
        "        if(hidden == None):\n",
        "            output, hidden = net.Decoder.get_pred(encoded_features)\n",
        "        else:\n",
        "            output, hidden = net.Decoder.get_pred(encoded_features, to_device(hidden, device))\n",
        "    \n",
        "        _ , predicted_id = output.max(1)\n",
        "        caption_word_id.append(predicted_id)\n",
        "        if (predicted_id == end_token):\n",
        "            break\n",
        "        encoded_features = decoder_model.embed(predicted_id)\n",
        "    caption_word_id = torch.stack(caption_word_id, 1)\n",
        "    return caption_word_id.cpu().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bj5T__BkcRTp"
      },
      "outputs": [],
      "source": [
        "pred_caps = {}\n",
        "for batch_idx, sample in enumerate(test_loader):\n",
        "        print(\"Image_idx\", batch_idx)\n",
        "        image = sample['image']\n",
        "        image = image.float()\n",
        "        img_features = net.Encoder(image)\n",
        "        print(img_features.shape)\n",
        "        pred_cap = caption_image(img_features, 11)\n",
        "        pred_caps[batch_idx] = pred_cap\n",
        "        print(\"Predicted\",batch_idx, pred_cap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE-2K64GcRTp"
      },
      "outputs": [],
      "source": [
        "def test_pred(embed_dim, hidden_dim,num_layer, vocab_len, vocab_dict, train_enc=False):\n",
        "    encoder_model = net.Encoder(embed_dim, train_enc)\n",
        "    decoder_model = net.Decoder(embed_dim, hidden_dim, num_layer)\n",
        "        \n",
        "    #encoder_model = encoder_model.cuda()\n",
        "    encoder_model.eval() # Makes the model ready to be used in evaluation by taking care of batch norm and dropout\n",
        "    #decoder_model = decoder_model.cuda()\n",
        "    decoder_model.eval()\n",
        "    \n",
        "    test_dl = get_test_data()\n",
        "    private_test_dl = get_private_test_data()\n",
        "    bleu_score=[]\n",
        "    hypo_complete = []\n",
        "    ref = [[],[],[],[],[]]\n",
        "    #for batchid, (images, img_id) in enumerate(private_test_dl):\n",
        "    for batch_idx, sample in enumerate(test_loader):\n",
        "        image = sample['image']\n",
        "        images = images.cuda()\n",
        "        encode_feat = encoder_model(images.float(), train_enc)\n",
        "        #output_bs_pred =  beam_search_pred(encode_feat_rasha, decoder_model, vocab_dict, bw)\n",
        "        output = max_prediction(encode_feat, decoder_model,  max_length=80):\n",
        "        output_converted = [captions_preprocessing_obj.index_to_word[int(x)] for x in output]\n",
        "        #hypo = convert_rasha_max_pred(output_bs_pred, ix_2_word)\n",
        "        #private.write(f'{(img_id[0])}\\t {\" \".join(hypo)}\\n') \n",
        "        print(output_converted)\n",
        "\n",
        "    # sacre_bleu = sacrebleu.corpus_bleu(hypo_complete, ref).score\n",
        "    # print(f\"BLEU SCORE on public test data by beam search : {np.mean(np.array(bleu_score))}\")\n",
        "    # print(f\"SACREBLEU SCORE by beam search : {sacre_bleu}\")\n",
        "    # return np.mean(np.array(bleu_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "a =  np.arange(100).reshape(5,20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "q1_new.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
